{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer_lm_prompt import TransformerLanguageModelPrompt\n",
    "import os\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = 'data/KD-DTI/raw/optimized_tokenization_test.x.json'\n",
    "with open(src_file) as reader:\n",
    "     src_inputs_ids = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[483,\n",
       " 677,\n",
       " 9,\n",
       " 20308,\n",
       " 1595,\n",
       " 2413,\n",
       " 21,\n",
       " 14,\n",
       " 1583,\n",
       " 4579,\n",
       " 14247,\n",
       " 3720,\n",
       " 29729,\n",
       " 9,\n",
       " 202,\n",
       " 1297,\n",
       " 2214,\n",
       " 4,\n",
       " 6,\n",
       " 108,\n",
       " 3720,\n",
       " 29729,\n",
       " 202,\n",
       " 12,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 11,\n",
       " 21,\n",
       " 32,\n",
       " 193,\n",
       " 7,\n",
       " 126,\n",
       " 667,\n",
       " 552,\n",
       " 16,\n",
       " 6,\n",
       " 172,\n",
       " 5,\n",
       " 3120,\n",
       " 13,\n",
       " 2049,\n",
       " 3079,\n",
       " 270,\n",
       " 4,\n",
       " 462,\n",
       " 12505,\n",
       " 588,\n",
       " 31,\n",
       " 382,\n",
       " 13,\n",
       " 16184,\n",
       " 22897,\n",
       " 6,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 7,\n",
       " 2788,\n",
       " 2087,\n",
       " 265,\n",
       " 5,\n",
       " 66,\n",
       " 752,\n",
       " 92,\n",
       " 4,\n",
       " 10,\n",
       " 476,\n",
       " 7,\n",
       " 6,\n",
       " 8505,\n",
       " 8,\n",
       " 182,\n",
       " 520,\n",
       " 2872,\n",
       " 5,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 31888,\n",
       " 21,\n",
       " 1638,\n",
       " 1913,\n",
       " 4,\n",
       " 60,\n",
       " 47,\n",
       " 69,\n",
       " 14,\n",
       " 49,\n",
       " 9,\n",
       " 117,\n",
       " 342,\n",
       " 576,\n",
       " 13,\n",
       " 464,\n",
       " 382,\n",
       " 6602,\n",
       " 8,\n",
       " 1328,\n",
       " 588,\n",
       " 16,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 2214,\n",
       " 79,\n",
       " 4,\n",
       " 218,\n",
       " 682,\n",
       " 342,\n",
       " 3686,\n",
       " 220,\n",
       " 142,\n",
       " 5768,\n",
       " 6172,\n",
       " 9,\n",
       " 117,\n",
       " 737,\n",
       " 11992,\n",
       " 14618,\n",
       " 2953,\n",
       " 28976,\n",
       " 9,\n",
       " 1286,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 2214,\n",
       " 79,\n",
       " 4,\n",
       " 483,\n",
       " 677,\n",
       " 9,\n",
       " 20308,\n",
       " 1595,\n",
       " 2413,\n",
       " 301,\n",
       " 1583,\n",
       " 2214,\n",
       " 79,\n",
       " 10,\n",
       " 376,\n",
       " 1325,\n",
       " 197,\n",
       " 202,\n",
       " 1203,\n",
       " 8,\n",
       " 3288,\n",
       " 1752,\n",
       " 7,\n",
       " 6289,\n",
       " 1381,\n",
       " 1182,\n",
       " 7,\n",
       " 8,\n",
       " 315,\n",
       " 5,\n",
       " 939,\n",
       " 189,\n",
       " 9,\n",
       " 36045,\n",
       " 787,\n",
       " 15,\n",
       " 9398,\n",
       " 297,\n",
       " 358,\n",
       " 5,\n",
       " 6697,\n",
       " 7,\n",
       " 6290,\n",
       " 7,\n",
       " 8,\n",
       " 6290,\n",
       " 7,\n",
       " 170,\n",
       " 7,\n",
       " 8,\n",
       " 15,\n",
       " 2533,\n",
       " 729,\n",
       " 5,\n",
       " 586,\n",
       " 4,\n",
       " 117,\n",
       " 25,\n",
       " 55,\n",
       " 1325,\n",
       " 7,\n",
       " 483,\n",
       " 677,\n",
       " 9,\n",
       " 20308,\n",
       " 1595,\n",
       " 2413,\n",
       " 21,\n",
       " 6,\n",
       " 130,\n",
       " 1583,\n",
       " 4579,\n",
       " 14247,\n",
       " 1297,\n",
       " 2214,\n",
       " 16,\n",
       " 6,\n",
       " 108,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 1420,\n",
       " 196,\n",
       " 4,\n",
       " 1257,\n",
       " 5,\n",
       " 6,\n",
       " 1372,\n",
       " 396,\n",
       " 10,\n",
       " 3720,\n",
       " 8,\n",
       " 20308,\n",
       " 1595,\n",
       " 2413,\n",
       " 201,\n",
       " 7,\n",
       " 59,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 1861,\n",
       " 8,\n",
       " 320,\n",
       " 2643,\n",
       " 7,\n",
       " 47,\n",
       " 291,\n",
       " 22,\n",
       " 905,\n",
       " 55,\n",
       " 2820,\n",
       " 31,\n",
       " 5795,\n",
       " 107,\n",
       " 7,\n",
       " 6,\n",
       " 20308,\n",
       " 1595,\n",
       " 2413,\n",
       " 9,\n",
       " 201,\n",
       " 8188,\n",
       " 10,\n",
       " 6,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 2077,\n",
       " 9178,\n",
       " 13,\n",
       " 6,\n",
       " 3720,\n",
       " 9,\n",
       " 201,\n",
       " 8188,\n",
       " 4,\n",
       " 6,\n",
       " 3219,\n",
       " 5,\n",
       " 14,\n",
       " 1583,\n",
       " 4579,\n",
       " 14247,\n",
       " 1297,\n",
       " 696,\n",
       " 926,\n",
       " 677,\n",
       " 2214,\n",
       " 841,\n",
       " 14,\n",
       " 2958,\n",
       " 1178,\n",
       " 13,\n",
       " 345,\n",
       " 2485,\n",
       " 35,\n",
       " 193,\n",
       " 520,\n",
       " 552,\n",
       " 10,\n",
       " 307,\n",
       " 4,\n",
       " 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_inputs_ids[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 01:27:20 | INFO | fairseq.file_utils | loading archive file checkpoints/RE-DTI-BioGPT\n",
      "2023-04-24 01:27:20 | INFO | fairseq.file_utils | loading archive file data/KD-DTI/relis-bin\n",
      "2023-04-24 01:27:21 | INFO | src.language_modeling_prompt | dictionary: 42384 types\n",
      "2023-04-24 01:27:25 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n"
     ]
    }
   ],
   "source": [
    "m = TransformerLanguageModelPrompt.from_pretrained(\n",
    "    'checkpoints/RE-DTI-BioGPT', \n",
    "    'checkpoint_avg.pt', \n",
    "    'data/KD-DTI/relis-bin',\n",
    "    max_len_b=1024,\n",
    "    max_tokens=12000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerLanguageModelPrompt(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(42393, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=42393, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m.cfg)\n",
    "m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_inputs = [torch.Tensor(id).long() for id in src_inputs_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = m.generate(src_inputs, beam=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [m.decode(hypos[0][\"tokens\"]) for hypos in batch_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{'examples/RE-DTI/generate_checkpoint_optimized_tokenizer.pt'}\", \"w\", encoding='utf8') as fw:\n",
    "    for i in range(len(outputs)):\n",
    "        fw.write(outputs[i] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sed -i \"s/@@ //g\" generate_checkpoint_optimized_tokenizer.pt\n",
    "\n",
    "perl ../../mosesdecoder/scripts/tokenizer/detokenizer.perl -l en -a < generate_checkpoint_optimized_tokenizer.pt > generate_checkpoint_optimized_tokenizer.pt.detok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'examples/RE-DTI/generate_checkpoint_optimized_tokenizer.pt.detok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [\n",
    "    '(learned[0-9]+ )+',\n",
    "    'we can conclude that',\n",
    "    'we have that',\n",
    "    'in conclusion,',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(line: str) -> str:\n",
    "    \"\"\"Removes the prefix from the line.\"\"\"\n",
    "    # search for the prefix in the line\n",
    "    for p in prefix:\n",
    "        res = re.search(p, line)\n",
    "        \n",
    "        if res is not None:\n",
    "            # remove the prefix from the line\n",
    "            line = re.split(p, line)[-1].strip()\n",
    "            break\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(line):\n",
    "    \"\"\"Splits the line into sentences by semicolon.\"\"\"\n",
    "    sentences = re.split(r\"; \", line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relis_sentence(sentence):\n",
    "    # match sentences of the form \"the interaction between X and Y is Z\"\n",
    "    # and return the tuple (X, Z, Y)\n",
    "    ans = None\n",
    "    segs = re.match(r\"the interaction between (.*) and (.*) is (.*)\", sentence)\n",
    "    if segs is not None:\n",
    "        segs = segs.groups()\n",
    "        ans = (segs[0].strip(), segs[2].strip(), segs[1].strip())\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(sample, head_idx=0, rel_idx=1, tail_idx=2):\n",
    "    ret = {\n",
    "        \"triple_list_gold\": [], \n",
    "        \"triple_list_pred\": [], \n",
    "        \"new\": [], \n",
    "        \"lack\": [], \n",
    "        \"id\": [0]\n",
    "    }\n",
    "    for item in sample:\n",
    "        ret[\"triple_list_pred\"].append(\n",
    "            {\"subject\": item[head_idx], \n",
    "            \"relation\": item[rel_idx], \n",
    "            \"object\": item[tail_idx]}\n",
    "        )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 1159\n"
     ]
    }
   ],
   "source": [
    "all_lines = []\n",
    "with open(out_file, \"r\", encoding=\"utf8\") as fr:\n",
    "    for line in fr:\n",
    "        e = line.strip()\n",
    "        if len(e) > 0 and e[-1] == \".\": # if the last character of the line is a period, remove it\n",
    "            all_lines.append(e[:-1])\n",
    "        else:\n",
    "            all_lines.append(e)\n",
    "\n",
    "print(f'Number of lines: {len(all_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inhibition of rat brain monoamine oxidase activities by psoralen and isopsoralen: implications for the treatment of affective disorders. psoralen and isopsoralen, furocoumarins isolated from the plant psoralea corylifolia l., were demonstrated to exhibit in vitro inhibitory actions on monoamine oxidase (mao) activities in rat brain mitochondria, preferentially inhibiting mao-a activity over mao-b activity. this inhibition of enzyme activities was found to be dose-dependent and reversible. for mao-a, the ic50 values are 15.2 + / - 1.3 microm psoralen and 9.0 + / - 0.6 microm isopsoralen. for mao-b, the ic50 values are 61.8 + / - 4.3 microm psoralen and 12.8 + / - 0.5 microm isopsoralen. lineweaver-burk transformation of the inhibition data indicates that inhibition by both psoralen and isopsoralen is non-competitive for mao-a. the ki values were calculated to be 14.0 microm for psoralen and 6.5 microm for isopsoralen. on the other hand, inhibition by both psoralen and isopsoralen is competitive for mao-b. the ki values were calculated to be 58.1 microm for psoralen and 10.8 microm for isopsoralen. these inhibitory actions of psoralen and isopsoralen on rat brain mitochondrial mao activities are discussed in relation to their toxicities and their potential applications to treat affective disorders. learned1 learned2 learned3 learned4 learned5 learned6 learned7 learned8 learned9 the interaction between isopsoralen and monoamine oxidase type b (mao-b) is inhibitor; the interaction between isopsoralen and monoamine oxidase type a (mao-a) is inhibitor'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = []\n",
    "cnt = 0\n",
    "fail_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed:id:765, line:function of the central domain of streptokinase in substrate plasminogen docking and processing revealed by site-directed mutagenesis. the possible role of the central beta-domain (residues 151-287) of streptokinase (sk) was probed by site-specifically altering two charged residues at a time to alanines in a region (residues 230-290) previously identified by peptide walking to play a key role in plasminogen (pg) activation. these mutants were then screened for altered ability to activate equimolar \"partner\" human pg, or altered interaction with substrate pg resulting in an overall compromised capability for substrate pg processing. of the eight initial alanine-linker mutants of sk, one mutant, viz. sk (kk256.257aa) (sk-d1), showed a roughly 20-fold reduction in pg activator activity in comparison to wild-type sk expressed in escherichia coli (nsk). five other mutants were as active as nsk, with two [sk (re248.249aa) and sk (ek281.282aa), referred to as sk (c) and sk (h), respectively] showing specific activities approximately one-half and two-thirds, respectively, that of nsk. unlike sk (c) and sk (h), however, sk (d1) showed an extended initial delay in the kinetics of pg activation. these features were drastically accentuated when the charges on the two lys residues at positions 256 and 257 of nsk were reversed, to obtain sk (kk256.257ee) [sk (d2)]. this mutant showed a pg activator activity approximately 10-fold less than that of sk (d1). remarkably, inclusion of small amounts of human plasmin (pn) in the pg activation reactions of sk (d2) resulted in a dramatic, pn dose-dependent rejuvenation of its pg activation capability, indicating that it required pre-existing pn to form a functional activator since it could not effect active site exposure in partner pg on its own, a conclusion further confirmed by its inability to show a \"burst\" of p-nitrophenol release in the presence of equimolar human pg and p-nitrophenyl guanidino benzoate. the steady-state kinetic parameters for hpg activation of its 1: 1 complex with human pn revealed that although it could form a highly functional activator once \"supplied\" with a mature active site, the km for pg was increased nearly eightfold in comparison to that of nsk-pn. sk mutants carrying simultaneous two- and three-site charge-cluster alterations, viz., sk (re24249aa: ek281.282aa) [sk (ch)], sk (ek272.273aa; ek281.282aa) [sk (fh)], and sk (re248.249aa; ek272.273aa: ek281.282aa + + +) [sk (cfh)], showed additive / synergistic influence of multiple charge-cluster mutations on hpg activation when compared to the respective \"single-site\" mutants, with the \"triple-site\" mutant [sk (cf... learned1 learned2 learned3 learned4 learned5 learned6 learned7 learned8 learned9 the interaction between streptokinase and plasminogen)\n"
     ]
    }
   ],
   "source": [
    "# i is the index of the line in the input file\n",
    "for i, line in enumerate(all_lines):\n",
    "    # cnt is the number of lines in the input file\n",
    "    cnt += 1\n",
    "    ret = []\n",
    "    # convert the \"& amp\" if it is in line to \"&\"\n",
    "    line = line.replace(\" & amp; \", \"&\")\n",
    "    strip_line = strip_prefix(line)\n",
    "    sentences = split_sentence(strip_line)\n",
    "    for sen in sentences:\n",
    "        ans = convert_relis_sentence(sen)\n",
    "        if ans is not None:\n",
    "            ret.append(ans)\n",
    "    if len(ret) > 0:\n",
    "        hypothesis.append(ret)\n",
    "    else:\n",
    "        hypothesis.append([(\"failed\", \"failed\", \"failed\")])\n",
    "        fail_cnt += 1\n",
    "        print(\"Failed:id:{}, line:{}\".format(i+1, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('failed', 'failed', 'failed')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis[764]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_formatted = []\n",
    "for i in range(len(hypothesis)):\n",
    "    ret_formatted.append(converter(hypothesis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triple_list_gold': [],\n",
       " 'triple_list_pred': [{'subject': 'failed',\n",
       "   'relation': 'failed',\n",
       "   'object': 'failed'}],\n",
       " 'new': [],\n",
       " 'lack': [],\n",
       " 'id': [0]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_formatted[764]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed = 1, total = 1159\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{out_file}.extracted.json\", \"w\", encoding=\"utf8\") as fw:\n",
    "    for eg in ret_formatted:\n",
    "        print(json.dumps(eg), file=fw)\n",
    "\n",
    "\n",
    "print(f\"failed = {fail_cnt}, total = {cnt}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'examples/RE-DTI/generate_checkpoint_optimized_tokenizer.pt.detok.extracted.json'\n",
    "gold_file = 'data/KD-DTI/raw/test.json'\n",
    "pmids_file = 'data/KD-DTI/raw/relis_test.pmid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(s: str):\n",
    "    s = s.strip()\n",
    "\n",
    "    # normalize roman type id at end of string\n",
    "    num2roman = {\"0\": \"0\", \"1\": \"I\", \"2\": \"II\", \"3\": \"III\", \"4\": \"IV\", \"5\": \"V\", \"6\": \"VI\", \"7\": \"VII\", \"8\": \"VIII\", \"9\": \"IX\"}\n",
    "    if len(s) > 2 and s[-1].isnumeric() and not s[-2].isnumeric() and s[-1] in num2roman:\n",
    "        tmps = list(s)\n",
    "        s = ''.join(tmps[:-1]) + num2roman[tmps[-1]]\n",
    "\n",
    "    # remove useless end string\n",
    "    s = s.replace(\"-type\", '')\n",
    "    \n",
    "    # remove non-alphanumeric characters\n",
    "    return re.sub('[^a-zA-Z0-9]+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace amine-associated receptor 1 (taar1)\n",
      "traceamineassociatedreceptor1taar1\n",
      "\n",
      "trace amine-associated receptor 1\n",
      "traceamineassociatedreceptorI\n",
      "\n",
      "trace amine-associated receptor 21\n",
      "traceamineassociatedreceptor21\n"
     ]
    }
   ],
   "source": [
    "# e.p. for normalize_name\n",
    "\n",
    "print(f\"trace amine-associated receptor 1 (taar1)\\n{normalize_name('trace amine-associated receptor 1 (taar1)')}\\n\\ntrace amine-associated receptor 1\\n{normalize_name('trace amine-associated receptor 1')}\\n\\ntrace amine-associated receptor 21\\n{normalize_name('trace amine-associated receptor 21')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_abbr(tgt_set):\n",
    "    \"\"\" remove abbreviation in the brackets of entity, eg: aaa (bb) -> aaa \"\"\"\n",
    "    def rm(s):\n",
    "        s = s.strip()\n",
    "        if \"(\" in s and s[-1] == ')':  # entity end with a bracketed short cut\n",
    "            return normalize_name(s[:s.rfind(\"(\")].strip())\n",
    "        else:\n",
    "            return normalize_name(s)\n",
    "\n",
    "    tgt_set = list(tgt_set)\n",
    "    if tgt_set and type(tgt_set[0]) in [tuple, list]:  # process triples\n",
    "        return set([(rm(tp[0]), rm(tp[1]), rm(tp[2])) for tp in tgt_set])\n",
    "    else:  # process entities\n",
    "        return set([rm(e) for e in tgt_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abbr(tgt_set):\n",
    "    \"\"\" extract abbreviation in the brackets of entity, eg: aaa (bb) -> bb \"\"\"\n",
    "    def rm(s):\n",
    "        s = s.strip()\n",
    "        if \"(\" in s and s[-1] == ')':\n",
    "            return normalize_name(s[s.rfind(\"(\")+1:-1].strip())\n",
    "        else:\n",
    "            return normalize_name(s)\n",
    "\n",
    "    tgt_set = list(tgt_set)\n",
    "    if tgt_set and type(tgt_set[0]) in [tuple, list]:  # process triples\n",
    "        return set([(rm(tp[0]), rm(tp[1]), rm(tp[2])) for tp in tgt_set])\n",
    "    else:  # process entities\n",
    "        return set([rm(e) for e in tgt_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style acc \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(gold_set) == 0 else 1.0 * tp_num / len(pred_set | gold_set)\n",
    "\n",
    "\n",
    "def precision(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style precision \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(pred_set) == 0 else 1.0 * tp_num / len(pred_set)\n",
    "\n",
    "\n",
    "def recall(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style recall \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(gold_set) == 0 else 1.0 * tp_num / len(gold_set)\n",
    "\n",
    "\n",
    "def normed_eval(pred_set, gold_set, metric):\n",
    "    \"\"\" Both body and abbreviation match are considered correct \"\"\"\n",
    "    abbr_pred_set, abbr_gold_set = get_abbr(pred_set), get_abbr(gold_set)\n",
    "    rm_pred_set, rm_gold_set = rm_abbr(pred_set), rm_abbr(gold_set)\n",
    "    return max(metric(abbr_pred_set, abbr_gold_set), metric(rm_pred_set, rm_gold_set))\n",
    "\n",
    "\n",
    "def get_f1(p, r):\n",
    "    return 0 if (p + r) == 0 else (2.0 * p * r / (p + r))\n",
    "\n",
    "\n",
    "def ave(scores):\n",
    "    return 1.0 * sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(preds, pmids, golden):\n",
    "    ret = []\n",
    "    num_pred, num_gold, num_missing = 0, 0, 0\n",
    "    all_f1, p, r, d_acc, t_acc, i_acc = [], [], [], [], [], []\n",
    "    all_pred_triple, all_pred_d, all_pred_t, all_pred_i, all_gold_triple, all_gold_d, all_gold_t, all_gold_i = [], [], [], [], [], [], [], [],\n",
    "\n",
    "    for pred, idx in zip(preds, pmids):\n",
    "        # initialize a set for each entity type (drug, target, interaction) for the prediction and gold standard\n",
    "        gold_d_set, gold_t_set, gold_i_set, gold_set = set(), set(), set(), set()\n",
    "        pred_d_set, pred_t_set, pred_i_set, pred_set = set(), set(), set(), set()\n",
    "        \n",
    "        # if the prediction is not empty and if the model did not fail to extract any triple\n",
    "        if pred[\"triple_list_pred\"] and pred[\"triple_list_pred\"][0][\"subject\"] != 'failed':\n",
    "            # loop over all triples and add the entities to the respective sets\n",
    "            for tp in pred[\"triple_list_pred\"]:\n",
    "                d = tp[\"subject\"].strip().lower().replace(' ', '')\n",
    "                t = tp[\"object\"].strip().lower().replace(' ', '')\n",
    "                i = tp[\"relation\"].strip().lower().replace(' ', '')\n",
    "\n",
    "                pred_d_set.add(d)\n",
    "                pred_t_set.add(t)\n",
    "                pred_i_set.add(i)\n",
    "                pred_set.add((d, t, i))\n",
    "        # if the paper is not in the golden set\n",
    "        if idx not in golden:\n",
    "            # increase the number of missing papers\n",
    "            num_missing += 1\n",
    "            # print a message\n",
    "            print(\"----Missing:\", idx)\n",
    "            # skip this paper\n",
    "            continue\n",
    "        # if there are triples in the golden set\n",
    "        if golden[idx][\"triples\"]:\n",
    "            # loop over all triples and add the entities to the respective sets\n",
    "            for tp in golden[idx][\"triples\"]:\n",
    "                d = tp[\"drug\"].strip().lower().replace(' ', '')\n",
    "                t = tp[\"target\"].strip().lower().replace(' ', '')\n",
    "                i = tp[\"interaction\"].strip().lower().replace(' ', '')\n",
    "                gold_d_set.add(d)\n",
    "                gold_t_set.add(t)\n",
    "                gold_i_set.add(i)\n",
    "                gold_set.add((d, t, i))\n",
    "\n",
    "        # sample level eval\n",
    "        p.append(normed_eval(pred_set, gold_set, metric=precision))\n",
    "        r.append(normed_eval(pred_set, gold_set, metric=recall))\n",
    "        all_f1.append(get_f1(p[-1], r[-1]))\n",
    "        d_acc.append(normed_eval(pred_d_set, gold_d_set, metric=acc))\n",
    "        t_acc.append(normed_eval(pred_t_set, gold_t_set, metric=acc))\n",
    "        i_acc.append(normed_eval(pred_i_set, gold_i_set, metric=acc))\n",
    "\n",
    "        # onto level eval\n",
    "        all_pred_d.extend(pred_d_set)\n",
    "        all_pred_t.extend(pred_t_set)\n",
    "        all_pred_i.extend(pred_i_set)\n",
    "        all_pred_triple.extend(pred_set)\n",
    "        all_gold_d.extend(gold_d_set)\n",
    "        all_gold_t.extend(gold_t_set)\n",
    "        all_gold_i.extend(gold_i_set)\n",
    "        all_gold_triple.extend(gold_set)\n",
    "        \n",
    "        # if len(gold_set) < len(golden[idx][\"triples\"]):\n",
    "            # print(\"Duplicate extists, ori\", golden[idx][\"triples\"], gold_set)\n",
    "\n",
    "        num_pred += len(pred_set)\n",
    "        num_gold += len(gold_set)\n",
    "\n",
    "        ret.append({\n",
    "            \"pmid\": idx,\n",
    "            \"title\": golden[idx][\"title\"] if \"title\" in golden[idx] else None,\n",
    "            \"abstract\": golden[idx][\"abstract\"],\n",
    "            \"d_pred_gold\": [d_acc[-1], list(pred_d_set), list(gold_d_set)],\n",
    "            \"t_pred_gold\": [t_acc[-1], list(pred_t_set), list(gold_t_set)],\n",
    "            \"i_pred_gold\": [i_acc[-1], list(pred_i_set), list(gold_i_set)],\n",
    "            \"all_pred_gold\": [all_f1[-1], list(pred_set), list(gold_set)],\n",
    "        })\n",
    "\n",
    "\n",
    "    print(\"num sample\", len(all_f1), \"missing\", len(preds) - len(all_f1), \"num_gold tp\", num_gold, \"num_pred\", num_pred)\n",
    "\n",
    "    # Note: we adopt multi-label metrics following: http://129.211.169.156/publication/tkde13rev.pdf\n",
    "    print(\"Sample: acc d: {:.4f}\\tt:{:.4f}\\ti: {:.4f}\\ntp p: {:.4f}\\ttp r: {:.4f}\\ttp micro f1: {:.4f}\\ttp macro f1: {:.4f} \".format(\n",
    "        ave(d_acc), ave(t_acc), ave(i_acc), ave(p), ave(r), ave(all_f1), get_f1(ave(p), ave(r))))\n",
    "\n",
    "    # Ontology evaluation_scripts\n",
    "    all_p, all_r = normed_eval(set(all_pred_triple), set(all_gold_triple), metric=precision), normed_eval(set(all_pred_triple), set(all_gold_triple), metric=recall)\n",
    "    d_p, d_r = normed_eval(set(all_pred_d), set(all_gold_d), metric=precision), normed_eval(set(all_pred_d), set(all_gold_d), metric=recall)\n",
    "    t_p, t_r = normed_eval(set(all_pred_t), set(all_gold_t), metric=precision), normed_eval(set(all_pred_t), set(all_gold_t), metric=recall)\n",
    "    i_p, i_r = normed_eval(set(all_pred_i), set(all_gold_i), metric=precision), normed_eval(set(all_pred_i), set(all_gold_i), metric=recall)\n",
    "\n",
    "    print(\"Ontology: f1 d: {:.4f}\\tt:{:.4f}\\ti: {:.4f}\\t \\nall p: {:.4f}\\tall r: {:.4f}\\tonto f1: {:.4f}\".format(\n",
    "        get_f1(d_p, d_r), get_f1(t_p, t_r), get_f1(i_p, i_r), all_p, all_r, get_f1(all_p, all_r)\n",
    "    ))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with open(pred_file) as reader:\n",
    "    for line in reader:\n",
    "        preds.append(json.loads(line))\n",
    "\n",
    "with open(gold_file) as reader:\n",
    "    golden = json.load(reader)\n",
    "\n",
    "with open(pmids_file) as reader:\n",
    "    if '.json' in pmids_file:\n",
    "        pmids = json.load(reader)\n",
    "    else:\n",
    "        pmids = []\n",
    "        for line in reader:\n",
    "            pmids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====File:  generate_checkpoint_optimized_tokenizer.pt.detok.extracted.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====File: \", os.path.basename(pred_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triple_list_gold': [],\n",
       " 'triple_list_pred': [{'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type b (mao-b)'},\n",
       "  {'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type a (mao-a)'}],\n",
       " 'new': [],\n",
       " 'lack': [],\n",
       " 'id': [0]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sample 1159 missing 0 num_gold tp 1567 num_pred 2250\n",
      "Sample: acc d: 0.6175\tt:0.6067\ti: 0.8667\n",
      "tp p: 0.4009\ttp r: 0.3980\ttp micro f1: 0.3851\ttp macro f1: 0.3994 \n",
      "Ontology: f1 d: 0.6093\tt:0.5122\ti: 0.7556\t \n",
      "all p: 0.2518\tall r: 0.3735\tonto f1: 0.3008\n"
     ]
    }
   ],
   "source": [
    "result = do_eval(preds, pmids, golden)\n",
    "\n",
    "last_pos = pred_file.rfind('.json')\n",
    "res_file_name = pred_file[:last_pos] + '.eval_res.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sample 1159 missing 0 num_gold tp 1567 num_pred 2167\n",
      "Sample: acc d: 0.6153\tt:0.5999\ti: 0.8660\n",
      "tp p: 0.3969\ttp r: 0.3934\ttp micro f1: 0.3805\ttp macro f1: 0.3952 \n",
      "Ontology: f1 d: 0.6043\tt:0.5060\ti: 0.7727\t \n",
      "all p: 0.2593\tall r: 0.3720\tonto f1: 0.3056\n"
     ]
    }
   ],
   "source": [
    "# new\n",
    "result = do_eval(preds, pmids, golden)\n",
    "\n",
    "last_pos = pred_file.rfind('.json')\n",
    "res_file_name = pred_file[:last_pos] + '.eval_res.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(res_file_name, 'w') as writer:\n",
    "    json.dump(result, writer, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the comparation of the outputs in:\n",
    "\n",
    "examples/RE-DTI/generate_checkpoint_avg.pt.detok.extracted.eval_res.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioGPT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'examples/RE-DTI/generate_checkpoint_avg.pt.detok.extracted.json'\n",
    "# gold_file = 'data/KD-DTI/raw/test.json'\n",
    "gold_file = 'data/KD-DTI/raw/train.json'\n",
    "# pmids_file = 'data/KD-DTI/raw/relis_test.pmid'\n",
    "pmids_file = 'data/KD-DTI/raw/relis_train.pmid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "preds = []\n",
    "with open(pred_file) as reader:\n",
    "    for line in reader:\n",
    "        preds.append(json.loads(line))\n",
    "\n",
    "with open(gold_file) as reader:\n",
    "    golden = json.load(reader)\n",
    "\n",
    "with open(pmids_file) as reader:\n",
    "    if '.json' in pmids_file:\n",
    "        pmids = json.load(reader)\n",
    "    else:\n",
    "        pmids = []\n",
    "        for line in reader:\n",
    "            pmids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Beta-subunits co-determine the sensitivity of rat neuronal nicotinic receptors to antagonists.',\n",
       " 'abstract': \"We have investigated the effect of 4 ganglionic cholinergic antagonists (hexamethonium, mecamylamine, pentolinium, trimetaphan) on rat alpha 3 beta 2 and alpha 3 beta 4 neuronal nicotinic acetylcholine receptors (nAChRs) expressed in Xenopus oocytes. Current responses were elicited by fast application of acetylcholine on voltage-clamped oocytes (holding potentialVh = -80mV). Concentration-inhibition curves were used to get estimates of IC50, the antagonist concentration yielding 50% reduction of the peak current. The KB's of the antagonists were calculated using estimates of the apparent KD of acetylcholine. The order of affinity of the antagonists was similar for both receptor subtypes: mecamylamine approximately pentolinium &gt; hexamethonium &gt; trimetaphan. However, alpha 3 beta 4 neuronal nAChRs were 9 to 22 times more sensitive to each of the 4 antagonists than alpha 3 beta 2 receptors. These results further underline the importance of the beta-subunit as co-determinant of the functional properties of neuronal nAChRs.\",\n",
       " 'triples': [{'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit beta-4',\n",
       "   'interaction': 'antagonist'},\n",
       "  {'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit alpha-10',\n",
       "   'interaction': 'antagonist'},\n",
       "  {'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit alpha-3',\n",
       "   'interaction': 'antagonist'}],\n",
       " 'pmid': '7761270'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden[pmids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triple_list_gold': [],\n",
       " 'triple_list_pred': [{'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type b (mao-b)'},\n",
       "  {'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type a (mao-a)'}],\n",
       " 'new': [],\n",
       " 'lack': [],\n",
       " 'id': [0]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred_D = 0\n",
    "num_pred_T = 0\n",
    "num_gold_D = 0\n",
    "num_gold_T = 0\n",
    "\n",
    "\n",
    "num_predD_in_original = 0\n",
    "num_predT_in_original = 0\n",
    "# # if there are more than 50% prefix tokens of a span is in the original text, count it\n",
    "# num_predD_over50_in_original = 0\n",
    "# num_predT_over50_in_original = 0\n",
    "# partial in the golden means the model didn't generate the whole span.\n",
    "num_goldD_in_original = 0\n",
    "\n",
    "predT_not_in_original_pmid_predTs = []\n",
    "\n",
    "for id, pred in zip(pmids, preds):\n",
    "    # only use lower() to nomalize the text\n",
    "    pred_Ds = list(set([triple['subject'].lower() for triple in pred['triple_list_pred']]))\n",
    "    pred_Ts = list(set([triple['object'].lower() for triple in pred['triple_list_pred']]))\n",
    "    gold_Ds = list(set([triple['drug'].lower() for triple in golden[id]['triples']]))\n",
    "    gold_Ts = list(set([triple['target'].lower() for triple in golden[id]['triples']]))\n",
    "    text = (golden[id]['title'] + \" \" + golden[id]['abstract']).lower()\n",
    "    \n",
    "    num_pred_D += len(pred_Ds)\n",
    "    num_pred_T += len(pred_Ts)\n",
    "    num_gold_D += len(gold_Ds)\n",
    "    num_gold_T += len(gold_Ts)\n",
    "\n",
    "    predT_not_in_original = []\n",
    "\n",
    "    # count for if pred_D in the original text\n",
    "    for pred_D in pred_Ds:\n",
    "        if pred_D is not None:\n",
    "            if pred_D in text:\n",
    "                num_predD_in_original += 1\n",
    "\n",
    "    # count for if pred_T in the original text\n",
    "    for pred_T in pred_Ts:\n",
    "        if pred_T is not None:\n",
    "            if pred_T in text:\n",
    "                num_predT_in_original += 1\n",
    "            else:\n",
    "                predT_not_in_original.append(pred_T)\n",
    "\n",
    "\n",
    "    # count for if god_D in the original text\n",
    "    for gold_D in gold_Ds:\n",
    "        if gold_D in text:\n",
    "            num_goldD_in_original += 1\n",
    "\n",
    "    # count for if god_T in the original text\n",
    "    for gold_T in gold_Ts:\n",
    "        if gold_T in text:\n",
    "            num_goldT_in_original += 1\n",
    "\n",
    "\n",
    "    if predT_not_in_original:\n",
    "        predT_not_in_original_pmid_predTs.append({\n",
    "            \"id\": id,\n",
    "            \"predT_not_in_original\": predT_not_in_original\n",
    "            })\n",
    "    # pred_D_ids = [tokenizer.encode(pred_d, add_special_tokens=False) for pred_d in pred_Ds]\n",
    "    # pred_T_ids = [tokenizer.encode(pred_t, add_special_tokens=False) for pred_t in pred_Ts]\n",
    "    # gold_D_ids = [tokenizer.encode(gold_d, add_special_tokens=False) for gold_d in gold_Ds]\n",
    "    # gold_T_ids = [tokenizer.encode(gold_t, add_special_tokens=False) for gold_t in gold_Ts]\n",
    "    # text_id = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # # count for if pred_D in the original text\n",
    "    # for pred_d_id in pred_D_ids:\n",
    "    #     length = len(pred_d_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == pred_d_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predD_in_original += 1\n",
    "    #             else:\n",
    "    #                 if j+1 > length/2:\n",
    "    #                     num_predD_over50_in_original += 1\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if pred_T in the original text\n",
    "    # for pred_t_id in pred_T_ids:\n",
    "    #     length = len(pred_t_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == pred_t_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predT_in_original += 1\n",
    "    #             else:\n",
    "    #                 if j+1 > length/2:\n",
    "    #                     print(tokenizer.decode(pred_t_id[:j+1]))\n",
    "    #                     num_predT_over50_in_original += 1\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if gold_T in the original text\n",
    "    # for gold_t_id in gold_T_ids:\n",
    "    #     length = len(gold_t_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == gold_t_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predT_in_golden += 1\n",
    "    #             else:\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if gold_D in the original text\n",
    "    # for gold_d_id in gold_D_ids:\n",
    "    #     length = len(gold_d_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == gold_d_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predD_in_golden += 1\n",
    "    #             else:\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(golden) == len(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gold_D = 0\n",
    "num_gold_T = 0\n",
    "\n",
    "\n",
    "num_predD_in_original = 0\n",
    "num_predT_in_original = 0\n",
    "# # if there are more than 50% prefix tokens of a span is in the original text, count it\n",
    "# num_predD_over50_in_original = 0\n",
    "# num_predT_over50_in_original = 0\n",
    "# partial in the golden means the model didn't generate the whole span.\n",
    "num_goldD_in_original = 0\n",
    "num_goldT_in_original = 0\n",
    "\n",
    "for id in pmids:\n",
    "    # only use lower() to nomalize the text\n",
    "    gold_Ds = list(set([triple['drug'].lower() for triple in golden[id]['triples']]))\n",
    "    gold_Ts = list(set([triple['target'].lower() for triple in golden[id]['triples']]))\n",
    "    text = (golden[id]['title'] + \" \" + golden[id]['abstract']).lower()\n",
    "    \n",
    "    num_gold_D += len(gold_Ds)\n",
    "    num_gold_T += len(gold_Ts)\n",
    "\n",
    "    predT_not_in_original = []\n",
    "\n",
    "    # count for if god_D in the original text\n",
    "    for gold_D in gold_Ds:\n",
    "        if gold_D in text:\n",
    "            num_goldD_in_original += 1\n",
    "\n",
    "    # count for if god_T in the original text\n",
    "    for gold_T in gold_Ts:\n",
    "        if gold_T in text:\n",
    "            num_goldT_in_original += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_goldD_in_original:  8369\n",
      "num_goldT_in_original:  1535\n",
      "num_gold_D:  19426\n",
      "num_gold_T:  17179\n"
     ]
    }
   ],
   "source": [
    "# print num_predD_in_original, num_predT_in_original, num_predD_over50_in_original, num_predT_over50_in_original, num_predD_in_golden = 0, num_predT_in_golden with their lables\n",
    "# print(\"num_predD_in_original: \", num_predD_in_original)\n",
    "# print(\"num_predT_in_original: \", num_predT_in_original)\n",
    "print(\"num_goldD_in_original: \", num_goldD_in_original)\n",
    "print(\"num_goldT_in_original: \", num_goldT_in_original)\n",
    "# print(\"num__pred_D: \", num_pred_D)\n",
    "# print(\"num_pred_T: \", num_pred_T)\n",
    "print(\"num_gold_D: \", num_gold_D)\n",
    "print(\"num_gold_T: \", num_gold_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gold_D:\n",
      "8369 / 19426 : 0.4308\n",
      "For gold_T:\n",
      "1535 / 17179 : 0.0894\n"
     ]
    }
   ],
   "source": [
    "# print(f\"For pred_D:\\n{num_predD_in_original} / {num_pred_D} : {round(num_predD_in_original/num_pred_D, 4)}\")\n",
    "# print(f\"For pred_T:\\n{num_predT_in_original} / {num_pred_T} : {round(num_predT_in_original/num_pred_T, 4)}\")\n",
    "print(f\"For gold_D:\\n{num_goldD_in_original} / {num_gold_D} : {round(num_goldD_in_original/num_gold_D, 4)}\")\n",
    "print(f\"For gold_T:\\n{num_goldT_in_original} / {num_gold_T} : {round(num_goldT_in_original/num_gold_T, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('analysis/predT_not_in_original_pmid_predTs.json', 'w') as writer:\n",
    "    json.dump(predT_not_in_original_pmid_predTs, writer, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
