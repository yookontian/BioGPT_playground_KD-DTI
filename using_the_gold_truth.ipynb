{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pred_T file and the gold file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'analysis/predT_not_in_original_pmid_predTs.json'\n",
    "gold_file = 'data/KD-DTI/raw/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823\n",
      "{'id': '11169165', 'predT_not_in_original': ['monoamine oxidase type b (mao-b)', 'monoamine oxidase type a (mao-a)']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load the pmids that the pred_Ts are not in the original articles\n",
    "with open (pred_file, 'r') as f:\n",
    "    pred_d_not_in_original = json.load(f)\n",
    "\n",
    "print(len(pred_d_not_in_original))\n",
    "print(pred_d_not_in_original[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "{'title': 'Inhibition of rat brain monoamine oxidase activities by psoralen and isopsoralen: implications for the treatment of affective disorders.', 'abstract': 'Psoralen and isopsoralen, furocoumarins isolated from the plant Psoralea corylifolia L., were demonstrated to exhibit in vitro inhibitory actions on monoamine oxidase (MAO) activities in rat brain mitochondria, preferentially inhibiting MAO-A activity over MAO-B activity. This inhibition of enzyme activities was found to be dose-dependent and reversible. For MAO-A, the IC50 values are 15.2 +/- 1.3 microM psoralen and 9.0 +/- 0.6 microM isopsoralen. For MAO-B, the IC50 values are 61.8 +/- 4.3 microM psoralen and 12.8 +/- 0.5 microM isopsoralen. Lineweaver-Burk transformation of the inhibition data indicates that inhibition by both psoralen and isopsoralen is non-competitive for MAO-A. The Ki values were calculated to be 14.0 microM for psoralen and 6.5 microM for isopsoralen. On the other hand, inhibition by both psoralen and isopsoralen is competitive for MAO-B. The Ki values were calculated to be 58.1 microM for psoralen and 10.8 microM for isopsoralen. These inhibitory actions of psoralen and isopsoralen on rat brain mitochondrial MAO activities are discussed in relation to their toxicities and their potential applications to treat affective disorders.', 'triples': [{'drug': 'Psoralen', 'target': 'Monoamine oxidase type A (MAO-A)', 'interaction': 'inhibitor'}, {'drug': 'Psoralen', 'target': 'Monoamine oxidase type B (MAO-B)', 'interaction': 'inhibitor'}], 'pmid': '11169165'}\n"
     ]
    }
   ],
   "source": [
    "# load the gold standard\n",
    "with open (gold_file, 'r') as f:\n",
    "    gold_d = json.load(f)\n",
    "\n",
    "print(len(gold_d))\n",
    "print(gold_d[pred_d_not_in_original[0]['id']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 02:41:26 | INFO | fairseq.file_utils | loading archive file checkpoints/RE-DTI-BioGPT\n",
      "2023-04-17 02:41:26 | INFO | fairseq.file_utils | loading archive file data/KD-DTI/relis-bin\n",
      "2023-04-17 02:41:28 | INFO | src.language_modeling_prompt | dictionary: 42384 types\n",
      "2023-04-17 02:41:31 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'fastbpe', 'bpe_codes': 'data/bpecodes'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'en', 'moses_no_dash_splits': False, 'moses_no_escape': False}}\n",
      "Loading codes from data/bpecodes ...\n",
      "Read 40000 codes from the codes file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerLanguageModelPrompt(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(42393, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=42393, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.transformer_lm_prompt import TransformerLanguageModelPrompt\n",
    "m = TransformerLanguageModelPrompt.from_pretrained(\n",
    "        \"checkpoints/RE-DTI-BioGPT\", \n",
    "        \"checkpoint_avg.pt\", \n",
    "        \"data/KD-DTI/relis-bin\",\n",
    "        tokenizer='moses', \n",
    "        bpe='fastbpe', \n",
    "        bpe_codes=\"data/bpecodes\",\n",
    "        max_len_b=1024,\n",
    "        beam=5)\n",
    "m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# because it's hard to use the moses tokenizer.decode() to show the different between the 4 (= 4#) and 4</w>\n",
    "# so here using the tokenizer from HF to decode each generated token, and it doesn't include the learn0 - learn9, which is not a problem\n",
    "from transformers import BioGptTokenizer\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test data: {\"pmid\": ,\n",
    "#               \"id\": ,\n",
    "#               \"title+abstract.lower()\": ,\n",
    "#               \"text_tokens\": ,\n",
    "#               \"pred_Ts\": ,\n",
    "#              \"pred_Ts_tokens\": ,}\n",
    "#               \"gold_triples\": ,}\n",
    "\n",
    "def get_test_data(id):\n",
    "    prefix = torch.arange(42384, 42393)\n",
    "    test_data = {}\n",
    "    test_data['pmid'] = pred_d_not_in_original[id]['id']\n",
    "    test_data['text'] = gold_d[test_data['pmid']]['title'].strip() + \" \" + gold_d[test_data['pmid']]['abstract']\n",
    "    test_data['text'] = test_data['text'].lower().strip().replace('  ', ' ')\n",
    "    test_data['text_tokens'] = m.encode(test_data['text'])\n",
    "    test_data['text_tokens_with_prefix'] = torch.cat([test_data['text_tokens'], prefix], dim=-1).unsqueeze(0).cuda()\n",
    "    try:\n",
    "        test_data['pred_Ts'] = pred_d_not_in_original[id]['predT_not_in_original']\n",
    "        test_data['pred_Ts_tokens'] = [m.encode(pred_T) for pred_T in test_data['pred_Ts']]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    test_data['gold_triples'] = gold_d[test_data['pmid']]['triples']\n",
    "    test_data['gold_drugs'] = [tokenizer.encode(gold_triple['drug'].lower(), add_special_tokens=False, return_tensors='pt')for gold_triple in test_data['gold_triples']]\n",
    "    test_data['gold_targets'] = [tokenizer.encode(gold_triple['target'].lower(), add_special_tokens=False, return_tensors='pt') for gold_triple in test_data['gold_triples']]\n",
    "    test_data['gold_interaction'] = [tokenizer.encode(gold_triple['interaction'].lower(), add_special_tokens=False, return_tensors='pt') for gold_triple in test_data['gold_triples']]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"pred_Ts\": \"sodium-dependent serotonin transporter\",\n",
      "\"gold\": {\n",
      "\"drug\": \"Paroxetine\",\n",
      "\"target\": \"Sodium-dependent serotonin transporter\",\n",
      "\"interaction\": \"inhibitor\",\n",
      "},\n",
      "\"text\": \"paroxetine : a review of its pharmacology and therapeutic potential in the management of panic disorder. synopsis: paroxetine is the first selective serotonin (5-hydroxytryptamine; 5-ht) reuptake inhibitor (ssri) to be approved for the treatment of patients with panic disorder with or without agoraphobia. it is a highly selective inhibitor of presynaptic serotonin reuptake and does not interact with adrenergic, dopaminergic, histaminergic or serotonergic receptors to any significant extent.         oral paroxetine 10 to 60 mg/day is significantly more effective than placebo in reducing the frequency of panic attacks and improving associated symptoms, as shown in short term trials in patients with panic disorder with or without agoraphobia. the efficacy of the drug was maintained during up to 6 months'; treatment, and continued therapy reduced the risk of relapse. oral paroxetine 10 to 60 mg/day was at least as effective as clomipramine 10 to 150 mg/day, but appeared to have a more rapid onset of effect, in a placebo-controlled trial.         the tolerability profile of paroxetine is similar to that established for other ssris and is characterised by adverse events such as nausea, headache, somnolence, dry mouth, tremor, insomnia, asthenia, sweating, constipation, dizziness and sexual dysfunction. paroxetine was better tolerated overall than clomipramine and was associated with a lower incidence of certain anticholinergic events (such as dry mouth and constipation) in a comparative trial. it is not associated with the type of dependence seen with benzodiazepines, and it appears to be safer in overdose than the tricyclic antidepressants. paroxetine 20 or 30mg does not significantly impair psychomotor function or interact with alcohol (ethanol).         in conclusion, the good tolerability profile of paroxetine, including lack of dependence potential and relative safety in overdose, makes it attractive for the treatment of patients with panic disorder. it appears to be at least as effective as clomipramine in reducing panic attacks and associated symptoms. although further trials to compare the efficacy and tolerability of paroxetine with that of other tricyclic agents (especially imipramine), high-potency benzodiazepines and monoamine oxidase inhibitors are needed, the drug appears to have the potential to become a first-line treatment for panic disorder. pharmacodynamic properties: paroxetine increases serotonergic neurotransmission by inhibiting presynaptic reuptake of serotonin (5-hydroxytryptamine; 5-ht) and thereby increasing the level of the neurotransmitter at the synaptic cleft. in vitro, it is a more potent inhibitor of serotonin uptake than the selective serotonin reuptake inhibitors (ssris) citalopram, fluvoxamine and fluoxetine. paroxetine was more potent than sertraline in one study that compared mean inhibition constants for serotonin uptake, but not in another study that compared the concentrations required to inhibit serotonin uptake by 50%. in contrast to the tricyclic antidepressants, paroxetine has little effect on the uptake of dopamine or noradrenaline (norepinephrine) in vitro. it has negligible affinity for alphar(1-), alphar(2-) and betar-adrenoceptors, dopamine d(1) and d(2) receptors, hi starnine h(1) receptors and serotonin 5-ht(1a), 5-ht(2a) and 5-ht(2c) receptors. however, paroxetine does have weak affinity for muscarinic cholinergic receptors.         as shown in rats, paroxetine appears to indirectly activate somatodendritic 5-ht(1a) autoreceptors when initially administered, thereby inhibiting firing of 5-ht neurons and release of serotonin. this may explain why the onset of therapeutic effect of paroxetine is delayed. however, repeated administration of paroxetine causes adaptive changes in synaptic serotonergic receptors, including a decrease in the responsiveness of somatodendritic and terminal serotonin autoreceptors. central betar-adrenoceptors are not down-regulated by administration of paroxetine to rats.         various studies in healthy volunteers without sleep disorders or volunteers reporting poor sleep have indicated that paroxetine disturbs normal sleep patterns by reducing rapid eye movement (rem) sleep time and lengthening rem latency. the effect of paroxetine on sleep in patients with panic disorder has not been determined, but in patients with depression the drug improves subjective quality of sleep. in electroencephalographic studies in healthy volunteers, administration of a single dose of paroxetine 30mg produced changes indicative of a sedative profile, whereas administration of 70mg produced changes indicative of activating properties.         no significant impairment of psychomotor function was observed after administration of single or multiple doses of paroxetine 20 or 30mg to healthy volunteers or patients with depression. the sedation and impairment of psychomotor function caused by haloperidol, amobarbital, oxazepam or alcohol (ethanol) were not potentiated by the administration of paroxetine 30mg.         in contrast to amitriptyline 150 mg/day or doxepin 150 mg/day, 2 to 6 weeks' treatment with paroxetine 20 or 30 mg/day did not produce clinically significant haemodynamic or electrophysiological effects on cardiac function in healthy volunteers or patients with depression. fewer adverse cardiac effects were reported by paroxetine than nortriptyline recipients in a study in patients with depression and ischaemic heart disease.         the anxiolytic activity of paroxetine has been demonstrated after 7 or 21 days' administration in several rodent models. pharmacokinetic properties: paroxetine is well absorbed after oral administration. it undergoes extensive first-pass metabolism and is rapidly distributed into tissue. only about 1% of the paroxetine dose remains in the systemic circulation. approximately 95% of paroxetine is protein bound in the plasma. steady-state concentrations are reached after 7 to 14 days of oral administration and the terminal elimination half-life (t1/2betar) is approximately 24 hours. however, there is a great deal of interindividual variation in the pharmacokinetics of paroxetine.         paroxetine is metabolised by at least 2 enzymes of the cytochrome p450 (cyp) system, one of which is cyp2d6. this enzyme is subject to genetic polymorphism, and thus the pharmacokinetics of paroxetine differ between individuals who have the enzyme (extensive metabolisers) and those who do not (poor metabolisers). the metabolites of paroxetine are essentially inactive.         metabolism of paroxetine by cyp2d6 is saturable. consequently, with repeated administration, bioavailability of paroxetine increases and pharmacokinetics may become nonlinear in some patients, especially when the dosage of paroxetine is increased.         approximately two-thirds of a paroxetine dose is eliminated in the urine and the remainder is excreted in faeces. almost all of the dose is eliminated as metabolites; lt3% is excreted as unchanged drug.         the plasma concentration and area under the plasma concentration-time curve of paroxetine are greater, and the t1/2betar prolonged, in elderly patients and those with hepatic or severe renal impairment compared with the general population. paroxetine distributes into breast milk to produce concentrations similar to those in plasma. therapeutic potential: as shown in 3 short term placebo-controlled trials in patients with panic disorder with or without agoraphobia, oral paroxetine 10 to 60 mg/day is significantly more effective than placebo for most variables measuring reduction in panic attack frequency. the drug also produced significantly greater improvements in various anxiety and depression scales than placebo.         an extension phase of one of the placebo-controlled studies showed that the efficacy of paroxetine in reducing panic attack frequency is maintained during up to 6 months' treatment and that the drug reduces the risk of relapse.         oral paroxetine 10 to 60 mg/day was at least as effective as clomipramine 10 to 150 mg/day in a comparative study. during weeks 7 to 9 of treatment, 51% of paroxetine recipients had no full panic attacks, compared with 37% of clomipramine recipients. the onset of action appeared to be more rapid for paroxetine than for clomipramine. the 2 drugs were equally effective in improving generalised anxiety, phobic avoidance and social, family and work interactions.         in patients who elected to continue treatment for a further 36 weeks in an extension phase of the above study, response rates increased further in all groups, including the placebo group. during weeks 34 to 36 of extended treatment, 85% of paroxetine recipients, 72% of clomipramine recipients and 59% of placebo recipients had no panic attacks. the difference between paroxetine and placebo was statistically significant at this time point; however, there was no significant difference between groups at the primary efficacy endpoint (weeks 22 to 24). tolerability: paroxetine is generally well tolerated by both younger and older individuals and its adverse event profile is consistent with that expected for an ssri. the tolerability profile of paroxetine in patients with panic disorder appears to resemble that in patients with depression. headache, nausea, somnolence, dry mouth and insomnia were the most common adverse events among 469 patients with panic disorder who received paroxetine 10 to 60 mg/day in short term clinical trials. the individual incidences for these events ranged from 18 to 25%; however, the incidence of headache in paroxetine-treated patients was the same as that in placebo recipients. (abstract truncated)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test_data = get_test_data(822)\n",
    "\n",
    "# print(f'{{\\n\"pred_Ts\": \"{test_data[\"pred_Ts\"][0]}\",')\n",
    "# print('\"gold\": {')\n",
    "# for key, value in test_data[\"gold_triples\"][0].items():\n",
    "#     print(f'\"{key}\": \"{value}\",')\n",
    "# print('},')\n",
    "# # print(f'\"gold\": {test_data[\"gold_triples\"][0]},')\n",
    "# print(f'\"text\": \"{test_data[\"text\"]}\"\\n}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11169165: (1/823) \n",
      "output_text: the interaction between psoralen and monoamine oxidase type a (mao-a) is inhibitor;\n",
      "\n",
      "11169165: (2/823) \n",
      "output_text: the interaction between psoralen and monoamine oxidase type a (mao-a) is inhibitor; the interaction between psoralen and monoamine oxidase type b (mao-b) is inhibitor.\n",
      "\n",
      "1 / 1159\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "for test_data_id in range(0, len(pred_d_not_in_original)):\n",
    "    # initialize\n",
    "    test_data = get_test_data(test_data_id)\n",
    "    test_input = test_data['text_tokens_with_prefix']\n",
    "\n",
    "    output_text = []\n",
    "    prob = []\n",
    "    ranking = []\n",
    "    step = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m.models[0].decoder.eval()\n",
    "        for new_triple in range(len(test_data['gold_triples'])):\n",
    "            # the interaction between\n",
    "            for i in range(3):\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                _, top_k_indices = torch.topk(out[0][0][-1], k=k)\n",
    "                top_k_tokens = [tokenizer.convert_ids_to_tokens([indice]) for indice in top_k_indices]\n",
    "                top_k_probs = torch.softmax(out[0][0][-1][top_k_indices], dim=-1)\n",
    "                top_k = [(token, prob.item()) for token, prob in zip(top_k_tokens, top_k_probs)]\n",
    "                # print(f'The top-{k} most possible tokens are:\\n{top_k}')\n",
    "                next_token_id = 1\n",
    "                test_input = torch.cat([test_input[0], top_k_indices[next_token_id-1].unsqueeze(0)], dim=-1).unsqueeze(0)\n",
    "                output_text.append(top_k_indices[next_token_id-1])\n",
    "\n",
    "                prob.append(softmax_out[top_k_indices[next_token_id-1]].item())\n",
    "                ranking.append(next_token_id)\n",
    "\n",
    "            # drug\n",
    "            for id in test_data['gold_drugs'][new_triple][0]:        \n",
    "                # print(f'output_text: {m.decode(output_text)}\\n')\n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                customized_string_tokens = id.unsqueeze(0).cuda()\n",
    "                test_input = torch.cat([test_input[0], customized_string_tokens], dim=-1).unsqueeze(0)\n",
    "                output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "                customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "                sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "                prob.append(softmax_out[customized_string_tokens].item())\n",
    "                ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "            \n",
    "            # and\n",
    "            step += 1\n",
    "\n",
    "            out = m.models[0].decoder(test_input)\n",
    "            softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "            customized_string_tokens = tokenizer.encode(\"and\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "            test_input = torch.cat([test_input[0], customized_string_tokens.cuda()], dim=-1).unsqueeze(0)\n",
    "            output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "            customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "            sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "            prob.append(softmax_out[customized_string_tokens].item())\n",
    "            ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "            # print(f'output_text: {m.decode(output_text)}\\n')\n",
    "\n",
    "            # target\n",
    "            for id in test_data['gold_targets'][new_triple][0]:        \n",
    "                # print(f'output_text: {m.decode(output_text)}\\n')\n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                customized_string_tokens = id.unsqueeze(0).cuda()\n",
    "                test_input = torch.cat([test_input[0], customized_string_tokens], dim=-1).unsqueeze(0)\n",
    "                output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "                customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "                sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "                prob.append(softmax_out[customized_string_tokens].item())\n",
    "                ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "            \n",
    "            # is\n",
    "            step += 1\n",
    "\n",
    "            out = m.models[0].decoder(test_input)\n",
    "            softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "            customized_string_tokens = tokenizer.encode(\"is\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "            test_input = torch.cat([test_input[0], customized_string_tokens.cuda()], dim=-1).unsqueeze(0)\n",
    "            output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "            customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "            sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "            prob.append(softmax_out[customized_string_tokens].item())\n",
    "            ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "            # print(f'{test_data[\"pmid\"]}: ({new_triple + 1}/{len(test_data[\"gold_triples\"])}) \\noutput_text: {m.decode(output_text)}\\n')\n",
    "\n",
    "            # interaction\n",
    "            for id in test_data['gold_interaction'][new_triple][0]:        \n",
    "                # print(f'output_text: {m.decode(output_text)}\\n')\n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                customized_string_tokens = id.unsqueeze(0).cuda()\n",
    "                test_input = torch.cat([test_input[0], customized_string_tokens], dim=-1).unsqueeze(0)\n",
    "                output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "                customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "                sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "                prob.append(softmax_out[customized_string_tokens].item())\n",
    "                ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "\n",
    "            # add . or ;\n",
    "            if new_triple + 1 == len(test_data['gold_triples']):\n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                customized_string_tokens = torch.tensor(4).unsqueeze(0).cuda()\n",
    "                test_input = torch.cat([test_input[0], customized_string_tokens.cuda()], dim=-1).unsqueeze(0)\n",
    "                output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "                customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "                sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "                prob.append(softmax_out[customized_string_tokens].item())\n",
    "                ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "                print(f'{test_data[\"pmid\"]}: ({new_triple + 1}/{len(pred_d_not_in_original)}) \\noutput_text: {m.decode(output_text)}\\n')\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                step += 1\n",
    "\n",
    "                out = m.models[0].decoder(test_input)\n",
    "                softmax_out = torch.softmax(out[0][0][-1], dim=-1)\n",
    "                customized_string_tokens = torch.tensor(44).unsqueeze(0).cuda()\n",
    "                test_input = torch.cat([test_input[0], customized_string_tokens.cuda()], dim=-1).unsqueeze(0)\n",
    "                output_text.append(customized_string_tokens.squeeze(0))\n",
    "\n",
    "                customized_string_prob = out[0][0][-1][customized_string_tokens].clone()\n",
    "                sorted_output, _ = torch.sort(out[0][0][-1], descending=True)\n",
    "                prob.append(softmax_out[customized_string_tokens].item())\n",
    "                ranking.append(torch.where(sorted_output == customized_string_prob)[0].item() + 1)\n",
    "                print(f'{test_data[\"pmid\"]}: ({new_triple + 1}/{len(pred_d_not_in_original)}) \\noutput_text: {m.decode(output_text)}\\n')\n",
    "\n",
    "\n",
    "    drugs_in_original = []\n",
    "    targets_in_original = []\n",
    "    for i in range(len(test_data['gold_triples'])):\n",
    "        if test_data['gold_triples'][i]['drug'].lower().strip().replace('  ', ' ') in test_data['text']:\n",
    "            drugs_in_original.append(1)\n",
    "        else:\n",
    "            drugs_in_original.append(0)\n",
    "        \n",
    "        if test_data['gold_triples'][i]['target'].lower().strip().replace('  ', ' ') in test_data['text']:\n",
    "            targets_in_original.append(1)\n",
    "        else:\n",
    "            targets_in_original.append(0)\n",
    "\n",
    "\n",
    "    # Create some fake data.\n",
    "    x = np.arange(step)\n",
    "    y1 = prob\n",
    "    y2 = ranking\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    fig.suptitle(f'Pmid: {test_data[\"pmid\"]}')\n",
    "\n",
    "    ax1.plot(x, y1, '.-')\n",
    "    ax1.set_ylabel('Probability')\n",
    "\n",
    "    ax2.plot(x, y2, '.-')\n",
    "    ax2.set_xlabel('step')\n",
    "    ax2.set_ylabel('Ranking')\n",
    "\n",
    "    marks = [0]* (step-1)\n",
    "    mark = False\n",
    "\n",
    "    # 0 for parttern tokens, 1 for drugs ,2 for targets, 3 for interaction\n",
    "    for i, token in enumerate(output_text):\n",
    "        if token != 6 and output_text[i-1] == 45:\n",
    "            marks[i] = 1\n",
    "            mark = True\n",
    "            continue\n",
    "        if token == 8 or token == 21 or token == 4:\n",
    "            continue\n",
    "        if token != 6 and output_text[i-1] == 8:\n",
    "            marks[i] = 2\n",
    "            mark = True\n",
    "            continue\n",
    "        if token != 6 and output_text[i-1] == 21:\n",
    "            marks[i] = 3\n",
    "            mark = True\n",
    "            continue\n",
    "        if token == 44:\n",
    "            mark = False\n",
    "            continue\n",
    "        if mark:\n",
    "            marks[i] = marks[i-1]\n",
    "            \n",
    "\n",
    "    # if marks[x] == 1, then using hollow circle for the plot, if marks[x] == 2, then using hollow triangle for the plot, if marks[x] == 3, then using star for the plot.\n",
    "    for i in range(step-1):\n",
    "        if marks[i] == 1:\n",
    "            ax1.plot(x[i], y1[i], marker='o', color='white', markeredgecolor='blue')\n",
    "            ax2.plot(x[i], y2[i], marker='o', color='white', markeredgecolor='blue')\n",
    "            if y2[i] > 5:\n",
    "                ax1.plot(x[i], y1[i], marker='o', color='white', markeredgecolor='red')\n",
    "                ax2.plot(x[i], y2[i], marker='o', color='white', markeredgecolor='red')\n",
    "\n",
    "        if marks[i] == 2:\n",
    "            ax1.plot(x[i], y1[i], marker='^', color='white', markeredgecolor='blue')\n",
    "            ax2.plot(x[i], y2[i], marker='^', color='white', markeredgecolor='blue')\n",
    "            if y2[i] > 5:\n",
    "                ax1.plot(x[i], y1[i], marker='^', color='red')\n",
    "                ax2.plot(x[i], y2[i], marker='^', color='red')\n",
    "\n",
    "        if marks[i] == 3:\n",
    "            ax1.plot(x[i], y1[i], marker='x', color='white', markeredgecolor='blue')\n",
    "            ax2.plot(x[i], y2[i], marker='x', color='white', markeredgecolor='blue')\n",
    "            if y2[i] > 5:\n",
    "                ax1.plot(x[i], y1[i], marker='x', color='red')\n",
    "                ax2.plot(x[i], y2[i], marker='x', color='red')\n",
    "    plt.text(-0.05, 2.45, f\"1: exist,    0: no\", transform=plt.gca().transAxes)\n",
    "    plt.text(-0.05, 2.35, f\"drugs: {drugs_in_original}\", transform=plt.gca().transAxes)\n",
    "    plt.text(-0.05, 2.25, f\"targets: {targets_in_original}\", transform=plt.gca().transAxes)\n",
    "\n",
    "    if 1 in targets_in_original:\n",
    "        plt.savefig(f'analysis/img/goden_truth_forcing/target_in_original/{test_data[\"pmid\"]}-{1 in drugs_in_original}-{1 in targets_in_original}.png')\n",
    "    else:\n",
    "        plt.savefig(f'analysis/img/goden_truth_forcing/{test_data[\"pmid\"]}-{1 in drugs_in_original}-{1 in targets_in_original}.png')\n",
    "\n",
    "    # plt.savefig(f'analysis/img/goden_truth_forcing/{test_data[\"pmid\"]}-{1 in targets_in_original}.png')\n",
    "\n",
    "    plt.close()\n",
    "    print (f'{test_data_id + 1} / {len(gold_d)}')\n",
    "    # break\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
