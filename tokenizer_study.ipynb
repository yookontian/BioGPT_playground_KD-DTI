{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.</w>': 4,\n",
       " 'of</w>': 5,\n",
       " 'the</w>': 6,\n",
       " ',</w>': 7,\n",
       " 'and</w>': 8,\n",
       " '@-@</w>': 9,\n",
       " 'in</w>': 10,\n",
       " ')</w>': 11,\n",
       " '(</w>': 12,\n",
       " 'to</w>': 13,\n",
       " 'a</w>': 14,\n",
       " 'with</w>': 15,\n",
       " 'for</w>': 16,\n",
       " 'was</w>': 17,\n",
       " 'The</w>': 18,\n",
       " 'were</w>': 19,\n",
       " ':</w>': 20,\n",
       " 'is</w>': 21,\n",
       " 'that</w>': 22,\n",
       " 'by</w>': 23,\n",
       " '%</w>': 24,\n",
       " 'on</w>': 25,\n",
       " '/</w>': 26,\n",
       " 'as</w>': 27,\n",
       " 'patients</w>': 28,\n",
       " 'from</w>': 29,\n",
       " 'or</w>': 30,\n",
       " 'are</w>': 31,\n",
       " 'an</w>': 32,\n",
       " 'be</w>': 33,\n",
       " 'at</w>': 34,\n",
       " 'this</w>': 35,\n",
       " '1</w>': 36,\n",
       " 'In</w>': 37,\n",
       " 'study</w>': 38,\n",
       " 'A</w>': 39,\n",
       " '2</w>': 40,\n",
       " 'not</w>': 41,\n",
       " 'cells</w>': 42,\n",
       " '=</w>': 43,\n",
       " ';</w>': 44,\n",
       " 'between</w>': 45,\n",
       " 'which</w>': 46,\n",
       " 'have</w>': 47,\n",
       " 'than</w>': 48,\n",
       " 'cell</w>': 49,\n",
       " 'after</w>': 50,\n",
       " '+</w>': 51,\n",
       " 'We</w>': 52,\n",
       " 'treatment</w>': 53,\n",
       " '3</w>': 54,\n",
       " 'these</w>': 55,\n",
       " 'This</w>': 56,\n",
       " 'has</w>': 57,\n",
       " 'been</w>': 58,\n",
       " 'using</w>': 59,\n",
       " 'we</w>': 60,\n",
       " 'but</w>': 61,\n",
       " 'results</w>': 62,\n",
       " 'may</w>': 63,\n",
       " 'group</w>': 64,\n",
       " 'two</w>': 65,\n",
       " 'their</w>': 66,\n",
       " 'had</w>': 67,\n",
       " 'P</w>': 68,\n",
       " 'used</w>': 69,\n",
       " 'during</w>': 70,\n",
       " 'can</w>': 71,\n",
       " 'also</w>': 72,\n",
       " 'associated</w>': 73,\n",
       " 'both</w>': 74,\n",
       " 'high</w>': 75,\n",
       " 'disease</w>': 76,\n",
       " 'more</w>': 77,\n",
       " 'analysis</w>': 78,\n",
       " 'activity</w>': 79,\n",
       " 'protein</w>': 80,\n",
       " '-</w>': 81,\n",
       " '5</w>': 82,\n",
       " 'clinical</w>': 83,\n",
       " 'data</w>': 84,\n",
       " '4</w>': 85,\n",
       " 'one</w>': 86,\n",
       " 'levels</w>': 87,\n",
       " 'increased</w>': 88,\n",
       " 'expression</w>': 89,\n",
       " 'significantly</w>': 90,\n",
       " '10</w>': 91,\n",
       " 'effects</w>': 92,\n",
       " 'all</w>': 93,\n",
       " 'time</w>': 94,\n",
       " 'found</w>': 95,\n",
       " 'effect</w>': 96,\n",
       " 'compared</w>': 97,\n",
       " 'other</w>': 98,\n",
       " 'significant</w>': 99,\n",
       " 'studies</w>': 100,\n",
       " 'cancer</w>': 101,\n",
       " 'no</w>': 102,\n",
       " 'C</w>': 103,\n",
       " '&apos;</w>': 104,\n",
       " 'risk</w>': 105,\n",
       " 'its</w>': 106,\n",
       " 'different</w>': 107,\n",
       " 'human</w>': 108,\n",
       " '&lt;</w>': 109,\n",
       " 'p</w>': 110,\n",
       " 'showed</w>': 111,\n",
       " '&quot;</w>': 112,\n",
       " 'years</w>': 113,\n",
       " 'it</w>': 114,\n",
       " 'control</w>': 115,\n",
       " '6</w>': 116,\n",
       " 'based</w>': 117,\n",
       " 'age</w>': 118,\n",
       " 'use</w>': 119,\n",
       " 'induced</w>': 120,\n",
       " '&apos;s</w>': 121,\n",
       " 'into</w>': 122,\n",
       " 'RESULTS</w>': 123,\n",
       " 'These</w>': 124,\n",
       " 'patient</w>': 125,\n",
       " 'well</w>': 126,\n",
       " 'cases</w>': 127,\n",
       " 'higher</w>': 128,\n",
       " 'only</w>': 129,\n",
       " 'most</w>': 130,\n",
       " 'gene</w>': 131,\n",
       " 'related</w>': 132,\n",
       " 'response</w>': 133,\n",
       " 'rate</w>': 134,\n",
       " '&#91;</w>': 135,\n",
       " 'blood</w>': 136,\n",
       " 'groups</w>': 137,\n",
       " 'observed</w>': 138,\n",
       " '&#93;</w>': 139,\n",
       " 'specific</w>': 140,\n",
       " 'low</w>': 141,\n",
       " 'three</w>': 142,\n",
       " 'present</w>': 143,\n",
       " 'model</w>': 144,\n",
       " 'type</w>': 145,\n",
       " 'METHODS</w>': 146,\n",
       " 'such</w>': 147,\n",
       " 'system</w>': 148,\n",
       " 'who</w>': 149,\n",
       " 'health</w>': 150,\n",
       " 'role</w>': 151,\n",
       " 'B</w>': 152,\n",
       " 'factors</w>': 153,\n",
       " 'when</w>': 154,\n",
       " 'non</w>': 155,\n",
       " 'acid</w>': 156,\n",
       " 'T</w>': 157,\n",
       " 'However</w>': 158,\n",
       " 'first</w>': 159,\n",
       " 'up</w>': 160,\n",
       " 'those</w>': 161,\n",
       " 'therapy</w>': 162,\n",
       " 'changes</w>': 163,\n",
       " 'function</w>': 164,\n",
       " 'increase</w>': 165,\n",
       " 'level</w>': 166,\n",
       " 'To</w>': 167,\n",
       " 'total</w>': 168,\n",
       " 'n</w>': 169,\n",
       " 'respectively</w>': 170,\n",
       " 'method</w>': 171,\n",
       " 'development</w>': 172,\n",
       " 'DNA</w>': 173,\n",
       " 'I</w>': 174,\n",
       " 'normal</w>': 175,\n",
       " 'new</w>': 176,\n",
       " 'care</w>': 177,\n",
       " 'performed</w>': 178,\n",
       " 'growth</w>': 179,\n",
       " 'among</w>': 180,\n",
       " 'could</w>': 181,\n",
       " 'potential</w>': 182,\n",
       " 'treated</w>': 183,\n",
       " 'children</w>': 184,\n",
       " '-': 185,\n",
       " 'tumor</w>': 186,\n",
       " 'mean</w>': 187,\n",
       " 'less</w>': 188,\n",
       " 'factor</w>': 189,\n",
       " 'N</w>': 190,\n",
       " 'number</w>': 191,\n",
       " 'within</w>': 192,\n",
       " 'important</w>': 193,\n",
       " 'mg</w>': 194,\n",
       " 'case</w>': 195,\n",
       " 'reported</w>': 196,\n",
       " 'including</w>': 197,\n",
       " '12</w>': 198,\n",
       " 'through</w>': 199,\n",
       " 'lower</w>': 200,\n",
       " 'binding</w>': 201,\n",
       " 'receptor</w>': 202,\n",
       " '95</w>': 203,\n",
       " 'women</w>': 204,\n",
       " 'without</w>': 205,\n",
       " 'year</w>': 206,\n",
       " 'tissue</w>': 207,\n",
       " '8</w>': 208,\n",
       " 'rats</w>': 209,\n",
       " 'months</w>': 210,\n",
       " 'positive</w>': 211,\n",
       " 'It</w>': 212,\n",
       " 'L</w>': 213,\n",
       " 'did</w>': 214,\n",
       " '7</w>': 215,\n",
       " 'mice</w>': 216,\n",
       " 'days</w>': 217,\n",
       " 'our</w>': 218,\n",
       " '20</w>': 219,\n",
       " 'identified</w>': 220,\n",
       " 'each</w>': 221,\n",
       " 'over</w>': 222,\n",
       " 'there</w>': 223,\n",
       " 'suggest</w>': 224,\n",
       " 'test</w>': 225,\n",
       " 'CONCLUSIONS</w>': 226,\n",
       " 'diagnosis</w>': 227,\n",
       " 'dose</w>': 228,\n",
       " 'findings</w>': 229,\n",
       " 'B': 230,\n",
       " 'proteins</w>': 231,\n",
       " 'presence</w>': 232,\n",
       " 'alpha</w>': 233,\n",
       " 'infection</w>': 234,\n",
       " 'primary</w>': 235,\n",
       " 'single</w>': 236,\n",
       " 'L': 237,\n",
       " 'early</w>': 238,\n",
       " 'reduced</w>': 239,\n",
       " 'surgery</w>': 240,\n",
       " 'differences</w>': 241,\n",
       " 'similar</w>': 242,\n",
       " 'beta</w>': 243,\n",
       " 'due</w>': 244,\n",
       " 'D</w>': 245,\n",
       " 'serum</w>': 246,\n",
       " 'while</w>': 247,\n",
       " 'evidence</w>': 248,\n",
       " '30</w>': 249,\n",
       " 'S': 250,\n",
       " 'brain</w>': 251,\n",
       " 'population</w>': 252,\n",
       " 'obtained</w>': 253,\n",
       " 'drug</w>': 254,\n",
       " 'about</w>': 255,\n",
       " 'review</w>': 256,\n",
       " 'genes</w>': 257,\n",
       " 'studied</w>': 258,\n",
       " 'h</w>': 259,\n",
       " 'species</w>': 260,\n",
       " 'D': 261,\n",
       " 'day</w>': 262,\n",
       " 'decreased</w>': 263,\n",
       " 'under</w>': 264,\n",
       " 'some</w>': 265,\n",
       " 'CI</w>': 266,\n",
       " 'dependent</w>': 267,\n",
       " 'measured</w>': 268,\n",
       " 'R': 269,\n",
       " 'conditions</w>': 270,\n",
       " 'There</w>': 271,\n",
       " 'II</w>': 272,\n",
       " 'G': 273,\n",
       " '15</w>': 274,\n",
       " 'plasma</w>': 275,\n",
       " 'concentration</w>': 276,\n",
       " 'following</w>': 277,\n",
       " 'F': 278,\n",
       " 's</w>': 279,\n",
       " 'BACKGROUND</w>': 280,\n",
       " 'long</w>': 281,\n",
       " 'should</w>': 282,\n",
       " 'S</w>': 283,\n",
       " 'liver</w>': 284,\n",
       " 'concentrations</w>': 285,\n",
       " 'M': 286,\n",
       " 'T': 287,\n",
       " 'addition</w>': 288,\n",
       " 'developed</w>': 289,\n",
       " 'report</w>': 290,\n",
       " 'revealed</w>': 291,\n",
       " 'range</w>': 292,\n",
       " 'acute</w>': 293,\n",
       " 'survival</w>': 294,\n",
       " 'subjects</w>': 295,\n",
       " 'period</w>': 296,\n",
       " '50</w>': 297,\n",
       " 'they</w>': 298,\n",
       " 'N': 299,\n",
       " 'investigated</w>': 300,\n",
       " 'demonstrated</w>': 301,\n",
       " 'show</w>': 302,\n",
       " 'Â±</w>': 303,\n",
       " 'small</w>': 304,\n",
       " 'C': 305,\n",
       " 'bone</w>': 306,\n",
       " 'vitro</w>': 307,\n",
       " 'chronic</w>': 308,\n",
       " 'four</w>': 309,\n",
       " 'common</w>': 310,\n",
       " 'included</w>': 311,\n",
       " 'research</w>': 312,\n",
       " 'H': 313,\n",
       " 'pressure</w>': 314,\n",
       " 'activation</w>': 315,\n",
       " 'E</w>': 316,\n",
       " 'samples</w>': 317,\n",
       " 'large</w>': 318,\n",
       " 'CONCLUSION</w>': 319,\n",
       " 'molecular</w>': 320,\n",
       " 'P': 321,\n",
       " 'major</w>': 322,\n",
       " 'term</w>': 323,\n",
       " 'complex</w>': 324,\n",
       " 'examined</w>': 325,\n",
       " 'M</w>': 326,\n",
       " 'effective</w>': 327,\n",
       " 'kg</w>': 328,\n",
       " 'surface</w>': 329,\n",
       " 'evaluated</w>': 330,\n",
       " 'methods</w>': 331,\n",
       " 'approach</w>': 332,\n",
       " 'against</w>': 333,\n",
       " '24</w>': 334,\n",
       " 'rates</w>': 335,\n",
       " 'per</w>': 336,\n",
       " 'body</w>': 337,\n",
       " 'either</w>': 338,\n",
       " 'Our</w>': 339,\n",
       " 'determine</w>': 340,\n",
       " 'p': 341,\n",
       " 'functional</w>': 342,\n",
       " 'E': 343,\n",
       " 'anti</w>': 344,\n",
       " 'further</w>': 345,\n",
       " 'determined</w>': 346,\n",
       " 'structure</w>': 347,\n",
       " '9</w>': 348,\n",
       " 'phase</w>': 349,\n",
       " 'virus</w>': 350,\n",
       " '5': 351,\n",
       " 'quality</w>': 352,\n",
       " 'pain</w>': 353,\n",
       " 'H</w>': 354,\n",
       " 'IL</w>': 355,\n",
       " 'before</w>': 356,\n",
       " 'membrane</w>': 357,\n",
       " 'values</w>': 358,\n",
       " '100</w>': 359,\n",
       " 'muscle</w>': 360,\n",
       " '&gt;</w>': 361,\n",
       " 'non': 362,\n",
       " 'information</w>': 363,\n",
       " 'same</w>': 364,\n",
       " 'vs</w>': 365,\n",
       " 'rat</w>': 366,\n",
       " 'symptoms</w>': 367,\n",
       " 'greater</w>': 368,\n",
       " 'stress</w>': 369,\n",
       " 'process</w>': 370,\n",
       " 'production</w>': 371,\n",
       " 'free</w>': 372,\n",
       " 'whether</w>': 373,\n",
       " 'will</w>': 374,\n",
       " 'weight</w>': 375,\n",
       " 'various</w>': 376,\n",
       " 'min</w>': 377,\n",
       " 'provide</w>': 378,\n",
       " 'life</w>': 379,\n",
       " 'shown</w>': 380,\n",
       " 'formation</w>': 381,\n",
       " 'known</w>': 382,\n",
       " 'negative</w>': 383,\n",
       " 'responses</w>': 384,\n",
       " 'management</w>': 385,\n",
       " 'exposure</w>': 386,\n",
       " 'out</w>': 387,\n",
       " 'A': 388,\n",
       " 'follow</w>': 389,\n",
       " 'i</w>': 390,\n",
       " 'ratio</w>': 391,\n",
       " 'water</w>': 392,\n",
       " '6': 393,\n",
       " '0.05</w>': 394,\n",
       " 'detected</w>': 395,\n",
       " 'involved</w>': 396,\n",
       " 'mechanisms</w>': 397,\n",
       " 'syndrome</w>': 398,\n",
       " 'o</w>': 399,\n",
       " '14</w>': 400,\n",
       " 'R</w>': 401,\n",
       " '11</w>': 402,\n",
       " 'O': 403,\n",
       " 'current</w>': 404,\n",
       " 'weeks</w>': 405,\n",
       " 'multiple</w>': 406,\n",
       " 're': 407,\n",
       " 'several</w>': 408,\n",
       " 'novel</w>': 409,\n",
       " 'change</w>': 410,\n",
       " 'mortality</w>': 411,\n",
       " 'properties</w>': 412,\n",
       " 'G</w>': 413,\n",
       " '18</w>': 414,\n",
       " 'outcomes</w>': 415,\n",
       " 'possible</w>': 416,\n",
       " 'lung</w>': 417,\n",
       " 'performance</w>': 418,\n",
       " 'difference</w>': 419,\n",
       " 'any</w>': 420,\n",
       " 'old</w>': 421,\n",
       " 'resistance</w>': 422,\n",
       " 'like</w>': 423,\n",
       " 'assessed</w>': 424,\n",
       " 'medical</w>': 425,\n",
       " 'active</w>': 426,\n",
       " 'region</w>': 427,\n",
       " 'surgical</w>': 428,\n",
       " 'heart</w>': 429,\n",
       " '4': 430,\n",
       " 'association</w>': 431,\n",
       " 'reduction</w>': 432,\n",
       " 'controls</w>': 433,\n",
       " '25</w>': 434,\n",
       " 'loss</w>': 435,\n",
       " 'size</w>': 436,\n",
       " 'renal</w>': 437,\n",
       " 'models</w>': 438,\n",
       " 'enzyme</w>': 439,\n",
       " '7': 440,\n",
       " 'vivo</w>': 441,\n",
       " 'mechanism</w>': 442,\n",
       " 'flow</w>': 443,\n",
       " 'whereas</w>': 444,\n",
       " 'ability</w>': 445,\n",
       " 'un': 446,\n",
       " 'X</w>': 447,\n",
       " 'injury</w>': 448,\n",
       " 'V': 449,\n",
       " 'imaging</w>': 450,\n",
       " 'isolated</w>': 451,\n",
       " 'O</w>': 452,\n",
       " 'I': 453,\n",
       " 'g</w>': 454,\n",
       " 'site</w>': 455,\n",
       " 'severe</w>': 456,\n",
       " 'c</w>': 457,\n",
       " 'K': 458,\n",
       " 'mass</w>': 459,\n",
       " 'analyzed</w>': 460,\n",
       " 'mediated</w>': 461,\n",
       " 'many</w>': 462,\n",
       " 'HIV</w>': 463,\n",
       " 'evaluate</w>': 464,\n",
       " '13</w>': 465,\n",
       " '40</w>': 466,\n",
       " 'technique</w>': 467,\n",
       " 'inhibition</w>': 468,\n",
       " 'stage</w>': 469,\n",
       " 'frequency</w>': 470,\n",
       " 'presented</w>': 471,\n",
       " 'support</w>': 472,\n",
       " 'K</w>': 473,\n",
       " 'value</w>': 474,\n",
       " 'being</w>': 475,\n",
       " 'contrast</w>': 476,\n",
       " 'area</w>': 477,\n",
       " 'characteristics</w>': 478,\n",
       " '16</w>': 479,\n",
       " 'An</w>': 480,\n",
       " 'diseases</w>': 481,\n",
       " '9': 482,\n",
       " '8': 483,\n",
       " 'genetic</w>': 484,\n",
       " 'relationship</w>': 485,\n",
       " 'via</w>': 486,\n",
       " 'All</w>': 487,\n",
       " 'work</w>': 488,\n",
       " 'state</w>': 489,\n",
       " 'self</w>': 490,\n",
       " 'left</w>': 491,\n",
       " 'order</w>': 492,\n",
       " 'parameters</w>': 493,\n",
       " 'decrease</w>': 494,\n",
       " 'form</w>': 495,\n",
       " 'lesions</w>': 496,\n",
       " 'outcome</w>': 497,\n",
       " 'detection</w>': 498,\n",
       " 'Although</w>': 499,\n",
       " 'reaction</w>': 500,\n",
       " 'in': 501,\n",
       " 'breast</w>': 502,\n",
       " 'hospital</w>': 503,\n",
       " '60</w>': 504,\n",
       " 'very</w>': 505,\n",
       " '0.001</w>': 506,\n",
       " 'us</w>': 507,\n",
       " 'five</w>': 508,\n",
       " 'glucose</w>': 509,\n",
       " 'indicate</w>': 510,\n",
       " 'administration</w>': 511,\n",
       " 'incidence</w>': 512,\n",
       " 'male</w>': 513,\n",
       " 'healthy</w>': 514,\n",
       " 'systems</w>': 515,\n",
       " 'post</w>': 516,\n",
       " 'status</w>': 517,\n",
       " 'caused</w>': 518,\n",
       " 'containing</w>': 519,\n",
       " 'therapeutic</w>': 520,\n",
       " 'As</w>': 521,\n",
       " 'sensitivity</w>': 522,\n",
       " 'however</w>': 523,\n",
       " 'required</w>': 524,\n",
       " 'family</w>': 525,\n",
       " 'F</w>': 526,\n",
       " 't</w>': 527,\n",
       " 'sequence</w>': 528,\n",
       " 'release</w>': 529,\n",
       " 'derived</w>': 530,\n",
       " 'considered</w>': 531,\n",
       " 'described</w>': 532,\n",
       " 'cause</w>': 533,\n",
       " 'tested</w>': 534,\n",
       " 'cardiac</w>': 535,\n",
       " 'evaluation</w>': 536,\n",
       " 'followed</w>': 537,\n",
       " 'tumors</w>': 538,\n",
       " 'types</w>': 539,\n",
       " 'previously</w>': 540,\n",
       " 'highly</w>': 541,\n",
       " 'sites</w>': 542,\n",
       " 'x</w>': 543,\n",
       " 'RNA</w>': 544,\n",
       " 'second</w>': 545,\n",
       " 'animals</w>': 546,\n",
       " 'synthesis</w>': 547,\n",
       " 'inflammatory</w>': 548,\n",
       " 'carcinoma</w>': 549,\n",
       " 'result</w>': 550,\n",
       " 'For</w>': 551,\n",
       " 'target</w>': 552,\n",
       " 'receptors</w>': 553,\n",
       " 'identify</w>': 554,\n",
       " 'conducted</w>': 555,\n",
       " 'Here</w>': 556,\n",
       " 'correlation</w>': 557,\n",
       " 'AND</w>': 558,\n",
       " 'investigate</w>': 559,\n",
       " 'mRNA</w>': 560,\n",
       " 'OBJECTIVE</w>': 561,\n",
       " 'available</w>': 562,\n",
       " 'complications</w>': 563,\n",
       " 'immune</w>': 564,\n",
       " 'Patients</w>': 565,\n",
       " 'received</w>': 566,\n",
       " 'experimental</w>': 567,\n",
       " 'amino</w>': 568,\n",
       " 'distribution</w>': 569,\n",
       " 'insulin</w>': 570,\n",
       " 'because</w>': 571,\n",
       " 'After</w>': 572,\n",
       " 'ml</w>': 573,\n",
       " 'assess</w>': 574,\n",
       " 'influence</w>': 575,\n",
       " 'assay</w>': 576,\n",
       " 'produced</w>': 577,\n",
       " 's': 578,\n",
       " 'oral</w>': 579,\n",
       " '17</w>': 580,\n",
       " 'o': 581,\n",
       " 'regulation</w>': 582,\n",
       " '0</w>': 583,\n",
       " 'impact</w>': 584,\n",
       " 'features</w>': 585,\n",
       " 'efficacy</w>': 586,\n",
       " 'expressed</w>': 587,\n",
       " 'drugs</w>': 588,\n",
       " 'death</w>': 589,\n",
       " 'improved</w>': 590,\n",
       " 'better</w>': 591,\n",
       " 'aim</w>': 592,\n",
       " 'general</w>': 593,\n",
       " 'assessment</w>': 594,\n",
       " 'standard</w>': 595,\n",
       " 'combination</w>': 596,\n",
       " 'how</w>': 597,\n",
       " 'if</w>': 598,\n",
       " 'diabetes</w>': 599,\n",
       " 'prevalence</w>': 600,\n",
       " 'adult</w>': 601,\n",
       " 'individuals</w>': 602,\n",
       " 'stimulation</w>': 603,\n",
       " 'volume</w>': 604,\n",
       " 'Results</w>': 605,\n",
       " 'able</w>': 606,\n",
       " 'mm</w>': 607,\n",
       " 'failure</w>': 608,\n",
       " 'pathway</w>': 609,\n",
       " 'field</w>': 610,\n",
       " 'skin</w>': 611,\n",
       " 'artery</w>': 612,\n",
       " 'di': 613,\n",
       " 'physical</w>': 614,\n",
       " 'V</w>': 615,\n",
       " 'content</w>': 616,\n",
       " 'kinase</w>': 617,\n",
       " 'literature</w>': 618,\n",
       " 'activities</w>': 619,\n",
       " 'sample</w>': 620,\n",
       " 'CT</w>': 621,\n",
       " 'cellular</w>': 622,\n",
       " 'increasing</w>': 623,\n",
       " 'light</w>': 624,\n",
       " 'tissues</w>': 625,\n",
       " 'overall</w>': 626,\n",
       " '21</w>': 627,\n",
       " 'indicated</w>': 628,\n",
       " 'W': 629,\n",
       " 'pulmonary</w>': 630,\n",
       " 'energy</w>': 631,\n",
       " 'disorders</w>': 632,\n",
       " 'antibodies</w>': 633,\n",
       " 'cross</w>': 634,\n",
       " 'relative</w>': 635,\n",
       " 'd</w>': 636,\n",
       " 'men</w>': 637,\n",
       " 'line</w>': 638,\n",
       " 'interaction</w>': 639,\n",
       " 'intervention</w>': 640,\n",
       " 'antibody</w>': 641,\n",
       " 'analyses</w>': 642,\n",
       " 'six</w>': 643,\n",
       " 'index</w>': 644,\n",
       " 'end</w>': 645,\n",
       " 'independent</w>': 646,\n",
       " 'vascular</w>': 647,\n",
       " 'then</w>': 648,\n",
       " 'female</w>': 649,\n",
       " 'correlated</w>': 650,\n",
       " 'temperature</w>': 651,\n",
       " 'often</w>': 652,\n",
       " 'mouse</w>': 653,\n",
       " 'density</w>': 654,\n",
       " 'individual</w>': 655,\n",
       " '19</w>': 656,\n",
       " '2': 657,\n",
       " 'need</w>': 658,\n",
       " 'm</w>': 659,\n",
       " 'least</w>': 660,\n",
       " 'made</w>': 661,\n",
       " 'procedure</w>': 662,\n",
       " 'likely</w>': 663,\n",
       " 'improve</w>': 664,\n",
       " 'underwent</w>': 665,\n",
       " 'versus</w>': 666,\n",
       " 'characterized</w>': 667,\n",
       " 'No</w>': 668,\n",
       " 'average</w>': 669,\n",
       " 'where</w>': 670,\n",
       " 'participants</w>': 671,\n",
       " 'part</w>': 672,\n",
       " 'pattern</w>': 673,\n",
       " 'neurons</w>': 674,\n",
       " 'diagnostic</w>': 675,\n",
       " 'm': 676,\n",
       " 'r</w>': 677,\n",
       " '3': 678,\n",
       " 'strains</w>': 679,\n",
       " 'patterns</w>': 680,\n",
       " 'demonstrate</w>': 681,\n",
       " 'initial</w>': 682,\n",
       " 'd': 683,\n",
       " 'score</w>': 684,\n",
       " 'enhanced</w>': 685,\n",
       " 'culture</w>': 686,\n",
       " 'techniques</w>': 687,\n",
       " 'l</w>': 688,\n",
       " 'occurred</w>': 689,\n",
       " 'given</w>': 690,\n",
       " 'controlled</w>': 691,\n",
       " 'tests</w>': 692,\n",
       " 'might</w>': 693,\n",
       " 'applied</w>': 694,\n",
       " 'regions</w>': 695,\n",
       " 'h': 696,\n",
       " 'b': 697,\n",
       " 'central</w>': 698,\n",
       " 'combined</w>': 699,\n",
       " 'coronary</w>': 700,\n",
       " 'At</w>': 701,\n",
       " 'right</w>': 702,\n",
       " 'adults</w>': 703,\n",
       " 'c': 704,\n",
       " 'action</w>': 705,\n",
       " 'sensitive</w>': 706,\n",
       " 'e</w>': 707,\n",
       " 'areas</w>': 708,\n",
       " 'events</w>': 709,\n",
       " 'affected</w>': 710,\n",
       " 'times</w>': 711,\n",
       " 'paper</w>': 712,\n",
       " 'inhibitor</w>': 713,\n",
       " 'length</w>': 714,\n",
       " 'U': 715,\n",
       " 'local</w>': 716,\n",
       " 'behavior</w>': 717,\n",
       " 'signaling</w>': 718,\n",
       " 'inhibited</w>': 719,\n",
       " 'knowledge</w>': 720,\n",
       " 'processes</w>': 721,\n",
       " 'Two</w>': 722,\n",
       " 'pH</w>': 723,\n",
       " 'seen</w>': 724,\n",
       " 'social</w>': 725,\n",
       " 'collected</w>': 726,\n",
       " 'limited</w>': 727,\n",
       " 'discussed</w>': 728,\n",
       " 'degrees</w>': 729,\n",
       " 'proliferation</w>': 730,\n",
       " 'Furthermore</w>': 731,\n",
       " 'them</w>': 732,\n",
       " 'al</w>': 733,\n",
       " 'sub': 734,\n",
       " 'short</w>': 735,\n",
       " 'recent</w>': 736,\n",
       " 'compounds</w>': 737,\n",
       " 'direct</w>': 738,\n",
       " 'would</w>': 739,\n",
       " 'comparison</w>': 740,\n",
       " 'chain</w>': 741,\n",
       " 'antigen</w>': 742,\n",
       " 'resulted</w>': 743,\n",
       " 'es</w>': 744,\n",
       " 'useful</w>': 745,\n",
       " 'good</w>': 746,\n",
       " 'interactions</w>': 747,\n",
       " 'micro': 748,\n",
       " 'biological</w>': 749,\n",
       " 'regression</w>': 750,\n",
       " 'practice</w>': 751,\n",
       " 'side</w>': 752,\n",
       " 'Using</w>': 753,\n",
       " 'training</w>': 754,\n",
       " 'confirmed</w>': 755,\n",
       " 'suggested</w>': 756,\n",
       " 'history</w>': 757,\n",
       " 'duration</w>': 758,\n",
       " 'trial</w>': 759,\n",
       " 'increases</w>': 760,\n",
       " 'even</w>': 761,\n",
       " 'trials</w>': 762,\n",
       " '0.01</w>': 763,\n",
       " 'agents</w>': 764,\n",
       " 't': 765,\n",
       " 'nerve</w>': 766,\n",
       " 'complete</w>': 767,\n",
       " 'e': 768,\n",
       " 'measures</w>': 769,\n",
       " 'median</w>': 770,\n",
       " 'across</w>': 771,\n",
       " 'i': 772,\n",
       " 'application</w>': 773,\n",
       " 'understanding</w>': 774,\n",
       " 'baseline</w>': 775,\n",
       " 'an': 776,\n",
       " '22</w>': 777,\n",
       " 'suggests</w>': 778,\n",
       " 'screening</w>': 779,\n",
       " 'OR</w>': 780,\n",
       " 'calcium</w>': 781,\n",
       " 'experiments</w>': 782,\n",
       " 'er</w>': 783,\n",
       " 'III</w>': 784,\n",
       " 'aged</w>': 785,\n",
       " 'ate</w>': 786,\n",
       " 'b</w>': 787,\n",
       " 'structures</w>': 788,\n",
       " 'established</w>': 789,\n",
       " 'n': 790,\n",
       " '90</w>': 791,\n",
       " 'approximately</w>': 792,\n",
       " 'injection</w>': 793,\n",
       " 'proposed</w>': 794,\n",
       " 'metabolism</w>': 795,\n",
       " 'scores</w>': 796,\n",
       " 'damage</w>': 797,\n",
       " 'infected</w>': 798,\n",
       " 'previous</w>': 799,\n",
       " 'suggesting</w>': 800,\n",
       " 'Thus</w>': 801,\n",
       " 'defined</w>': 802,\n",
       " 'k': 803,\n",
       " 'strain</w>': 804,\n",
       " 'peptide</w>': 805,\n",
       " 'a': 806,\n",
       " 'selected</w>': 807,\n",
       " 'PCR</w>': 808,\n",
       " 'secondary</w>': 809,\n",
       " 'One</w>': 810,\n",
       " 'exercise</w>': 811,\n",
       " 'mutations</w>': 812,\n",
       " 'main</w>': 813,\n",
       " 'peripheral</w>': 814,\n",
       " 'ed</w>': 815,\n",
       " 'disorder</w>': 816,\n",
       " 'measurements</w>': 817,\n",
       " 'examination</w>': 818,\n",
       " 'apoptosis</w>': 819,\n",
       " 'ing</w>': 820,\n",
       " 'differentiation</w>': 821,\n",
       " 'key</w>': 822,\n",
       " 'activated</w>': 823,\n",
       " 'pregnancy</w>': 824,\n",
       " 'r': 825,\n",
       " 'do</w>': 826,\n",
       " '80</w>': 827,\n",
       " 'postoperative</w>': 828,\n",
       " 'lines</w>': 829,\n",
       " 'visual</w>': 830,\n",
       " 'ation</w>': 831,\n",
       " 'metabolic</w>': 832,\n",
       " 'acids</w>': 833,\n",
       " 'domain</w>': 834,\n",
       " 'functions</w>': 835,\n",
       " 'essential</w>': 836,\n",
       " 'especially</w>': 837,\n",
       " 'alone</w>': 838,\n",
       " 'rare</w>': 839,\n",
       " 'components</w>': 840,\n",
       " 'provides</w>': 841,\n",
       " 'critical</w>': 842,\n",
       " 'infections</w>': 843,\n",
       " 'does</w>': 844,\n",
       " 'When</w>': 845,\n",
       " 'al': 846,\n",
       " 'design</w>': 847,\n",
       " 'testing</w>': 848,\n",
       " 'lead</w>': 849,\n",
       " 'sex</w>': 850,\n",
       " 'MS</w>': 851,\n",
       " 'basis</w>': 852,\n",
       " 'oxygen</w>': 853,\n",
       " 'experience</w>': 854,\n",
       " 'scale</w>': 855,\n",
       " 'lipid</w>': 856,\n",
       " 'transcription</w>': 857,\n",
       " 'fold</w>': 858,\n",
       " 'still</w>': 859,\n",
       " 'ventricular</w>': 860,\n",
       " 'according</w>': 861,\n",
       " 'structural</w>': 862,\n",
       " 'criteria</w>': 863,\n",
       " 'diagnosed</w>': 864,\n",
       " 'progression</w>': 865,\n",
       " 'hours</w>': 866,\n",
       " 'resistant</w>': 867,\n",
       " 'affect</w>': 868,\n",
       " 'signal</w>': 869,\n",
       " 'series</w>': 870,\n",
       " 'f': 871,\n",
       " 'strategies</w>': 872,\n",
       " 'y</w>': 873,\n",
       " 'onset</w>': 874,\n",
       " 'improvement</w>': 875,\n",
       " 'week</w>': 876,\n",
       " 'point</w>': 877,\n",
       " 'rapid</w>': 878,\n",
       " 'thus</w>': 879,\n",
       " '28</w>': 880,\n",
       " 'additional</w>': 881,\n",
       " 'play</w>': 882,\n",
       " 'g': 883,\n",
       " 'cycle</w>': 884,\n",
       " 'pathways</w>': 885,\n",
       " 'daily</w>': 886,\n",
       " '70</w>': 887,\n",
       " 'remains</w>': 888,\n",
       " 'kidney</w>': 889,\n",
       " 'cohort</w>': 890,\n",
       " 'inhibitors</w>': 891,\n",
       " '35</w>': 892,\n",
       " 'describe</w>': 893,\n",
       " 'future</w>': 894,\n",
       " 'populations</w>': 895,\n",
       " 'on': 896,\n",
       " 'reduce</w>': 897,\n",
       " 'cognitive</w>': 898,\n",
       " 'absence</w>': 899,\n",
       " 'chemotherapy</w>': 900,\n",
       " 'delivery</w>': 901,\n",
       " 'fluid</w>': 902,\n",
       " 'community</w>': 903,\n",
       " 'food</w>': 904,\n",
       " 'although</w>': 905,\n",
       " 'electron</w>': 906,\n",
       " 'products</w>': 907,\n",
       " '23</w>': 908,\n",
       " 'molecules</w>': 909,\n",
       " 'respiratory</w>': 910,\n",
       " 'doses</w>': 911,\n",
       " 'course</w>': 912,\n",
       " 'cost</w>': 913,\n",
       " 'gamma</w>': 914,\n",
       " 'l': 915,\n",
       " 'Of</w>': 916,\n",
       " 'provided</w>': 917,\n",
       " 'induction</w>': 918,\n",
       " 'chemical</w>': 919,\n",
       " 'With</w>': 920,\n",
       " 'poor</w>': 921,\n",
       " 'older</w>': 922,\n",
       " 'importance</w>': 923,\n",
       " 'medium</w>': 924,\n",
       " 'article</w>': 925,\n",
       " '1': 926,\n",
       " '?</w>': 927,\n",
       " 'interval</w>': 928,\n",
       " 'solution</w>': 929,\n",
       " 'procedures</w>': 930,\n",
       " 'degree</w>': 931,\n",
       " 'uptake</w>': 932,\n",
       " 'pre</w>': 933,\n",
       " 'depression</w>': 934,\n",
       " 'administered</w>': 935,\n",
       " 'hypertension</w>': 936,\n",
       " 'young</w>': 937,\n",
       " 'linked</w>': 938,\n",
       " 'nuclear</w>': 939,\n",
       " 'diet</w>': 940,\n",
       " '.': 941,\n",
       " 'randomized</w>': 942,\n",
       " 'On</w>': 943,\n",
       " 'hormone</w>': 944,\n",
       " 'bacterial</w>': 945,\n",
       " 'resulting</w>': 946,\n",
       " 'prior</w>': 947,\n",
       " 'exposed</w>': 948,\n",
       " 'Both</w>': 949,\n",
       " 'capacity</w>': 950,\n",
       " 'elevated</w>': 951,\n",
       " 'condition</w>': 952,\n",
       " '48</w>': 953,\n",
       " 'sequences</w>': 954,\n",
       " 'problems</w>': 955,\n",
       " 'markers</w>': 956,\n",
       " 'recovery</w>': 957,\n",
       " 'natural</w>': 958,\n",
       " 'Health</w>': 959,\n",
       " 'Î²</w>': 960,\n",
       " 'transport</w>': 961,\n",
       " 'people</w>': 962,\n",
       " 'stem</w>': 963,\n",
       " 'J': 964,\n",
       " 'achieved</w>': 965,\n",
       " 'host</w>': 966,\n",
       " 'Moreover</w>': 967,\n",
       " 'pre': 968,\n",
       " 'component</w>': 969,\n",
       " 'particularly</w>': 970,\n",
       " 'mutation</w>': 971,\n",
       " 'month</w>': 972,\n",
       " 'so</w>': 973,\n",
       " 'safety</w>': 974,\n",
       " 'open</w>': 975,\n",
       " 'purpose</w>': 976,\n",
       " 'ated</w>': 977,\n",
       " 'de': 978,\n",
       " 'endothelial</w>': 979,\n",
       " 'secretion</w>': 980,\n",
       " 'anti': 981,\n",
       " 'gastric</w>': 982,\n",
       " 'cardiovascular</w>': 983,\n",
       " 'systemic</w>': 984,\n",
       " 'variables</w>': 985,\n",
       " 'inflammation</w>': 986,\n",
       " 'en': 987,\n",
       " 'stable</w>': 988,\n",
       " 'myocardial</w>': 989,\n",
       " 'stroke</w>': 990,\n",
       " '26</w>': 991,\n",
       " '45</w>': 992,\n",
       " 'arterial</w>': 993,\n",
       " 'set</w>': 994,\n",
       " 'adverse</w>': 995,\n",
       " 'radiation</w>': 996,\n",
       " 'consistent</w>': 997,\n",
       " 'viral</w>': 998,\n",
       " 'cerebral</w>': 999,\n",
       " 'examine</w>': 1000,\n",
       " 'position</w>': 1001,\n",
       " 'U</w>': 1002,\n",
       " 'needed</w>': 1003,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&#91;</w>\n",
      "&#93;</w>\n",
      "95</w>\n",
      "9</w>\n",
      "9\n",
      "19</w>\n",
      "90</w>\n",
      "29</w>\n",
      "39</w>\n",
      "19\n",
      "49</w>\n",
      "59</w>\n",
      "96</w>\n",
      "69</w>\n",
      "92</w>\n",
      "79</w>\n",
      "89</w>\n",
      "99</w>\n",
      "98</w>\n",
      "93</w>\n",
      "94</w>\n",
      "91</w>\n",
      "97</w>\n",
      "0.9</w>\n",
      "0.9\n",
      "1.9</w>\n",
      "2009</w>\n",
      "9.\n",
      "2.9</w>\n",
      "29\n",
      "1999</w>\n",
      "1990</w>\n",
      "90\n",
      "1998</w>\n",
      "1995</w>\n",
      "1997</w>\n",
      "3.9</w>\n",
      "1996</w>\n",
      "9.5</w>\n",
      "1994</w>\n",
      "1992</w>\n",
      "1993</w>\n",
      "5.9</w>\n",
      "6.9</w>\n",
      "1991</w>\n",
      "4.9</w>\n",
      "2019</w>\n",
      "7.9</w>\n",
      "9.4</w>\n",
      "9.6</w>\n",
      "1988</w>\n",
      "1989</w>\n",
      "9.2</w>\n",
      "9.7</w>\n",
      "9.8</w>\n",
      "9.3</w>\n",
      "1987</w>\n",
      "9.1</w>\n",
      "1980</w>\n",
      "1985</w>\n",
      "9.0</w>\n",
      "8.9</w>\n",
      "1986</w>\n",
      "109</w>\n",
      "99\n",
      "0.99</w>\n",
      "9.9</w>\n",
      "0.09</w>\n",
      "39\n",
      "1984</w>\n",
      "0.98</w>\n",
      "09</w>\n",
      "129</w>\n",
      "0.95</w>\n",
      "0.96</w>\n",
      "0.97</w>\n",
      "9,\n",
      "49\n",
      "1982</w>\n",
      "0.94</w>\n",
      "1983</w>\n",
      "0.93</w>\n",
      "99m</w>\n",
      "0.92</w>\n",
      "0.89</w>\n",
      "119</w>\n",
      "0.91</w>\n",
      "99mTc</w>\n",
      "-9</w>\n",
      "190</w>\n",
      "0.90</w>\n",
      "1981</w>\n",
      "A549</w>\n",
      "139</w>\n",
      "0.79</w>\n",
      "0.009</w>\n",
      "0.19</w>\n",
      "1979</w>\n",
      "900</w>\n",
      "79\n",
      "0.69</w>\n",
      "59\n",
      "195</w>\n",
      "149</w>\n",
      "192</w>\n",
      "1975</w>\n",
      "1978</w>\n",
      "0.59</w>\n",
      "0.29</w>\n",
      "0.39</w>\n",
      "0.49</w>\n",
      "Cas9</w>\n",
      "1.09</w>\n",
      "19.\n",
      "159</w>\n",
      "199</w>\n",
      "1977</w>\n",
      "293</w>\n",
      "1976</w>\n",
      "1970</w>\n",
      "169</w>\n",
      "69\n",
      "196</w>\n",
      "1974</w>\n",
      "209</w>\n",
      "10.9</w>\n",
      "198</w>\n",
      "197</w>\n",
      "179</w>\n",
      "194</w>\n",
      "189</w>\n",
      "191</w>\n",
      "193</w>\n",
      "1.19</w>\n",
      "29.\n",
      "98\n",
      "1973</w>\n",
      "11.9</w>\n",
      "12.9</w>\n",
      "89\n",
      "1.29</w>\n",
      "1990s</w>\n",
      "1980s</w>\n",
      "390</w>\n",
      "13.9</w>\n",
      "1972</w>\n",
      "290</w>\n",
      "0.019</w>\n",
      "95\n",
      "1.39</w>\n",
      "239</w>\n",
      "U937</w>\n",
      "14.9</w>\n",
      "CD19</w>\n",
      "19.5</w>\n",
      "1971</w>\n",
      "219</w>\n",
      "1966</w>\n",
      "229</w>\n",
      "1.49</w>\n",
      "15.9</w>\n",
      "96\n",
      "97\n",
      "298</w>\n",
      "Hsp90</w>\n",
      "1970s</w>\n",
      "19.4</w>\n",
      "17.9</w>\n",
      "16.9</w>\n",
      "A9</w>\n",
      "19.2</w>\n",
      "H9\n",
      "99.\n",
      "19.6</w>\n",
      "309</w>\n",
      "0.029</w>\n",
      "249</w>\n",
      "199\n",
      "259</w>\n",
      "295</w>\n",
      "9-</w>\n",
      "393</w>\n",
      "1960</w>\n",
      "18.9</w>\n",
      "1969</w>\n",
      "1.59</w>\n",
      "B19</w>\n",
      "19.3</w>\n",
      "19.8</w>\n",
      "1968</w>\n",
      "269</w>\n",
      "398</w>\n",
      "19.7</w>\n",
      "296</w>\n",
      "1.69</w>\n",
      "93\n",
      "19.1</w>\n",
      "S9</w>\n",
      "299</w>\n",
      "1.95</w>\n",
      "0.039</w>\n",
      "1,9\n",
      "292</w>\n",
      "1965</w>\n",
      "279</w>\n",
      "C9</w>\n",
      "19.9</w>\n",
      "291</w>\n",
      "289</w>\n",
      "1.96</w>\n",
      "294</w>\n",
      "297</w>\n",
      "490</w>\n",
      "0.049</w>\n",
      "CD95</w>\n",
      "129\n",
      "1.79</w>\n",
      "19.0</w>\n",
      "-9\n",
      "1950</w>\n",
      "39.\n",
      "19,\n",
      "1.89</w>\n",
      "CYP2C19</w>\n",
      "1.92</w>\n",
      "21.9</w>\n",
      "409</w>\n",
      "1960s</w>\n",
      "20.9</w>\n",
      "HEK293</w>\n",
      "94\n",
      "42.9</w>\n",
      "29.4</w>\n",
      "319</w>\n",
      "22.9</w>\n",
      "1.98</w>\n",
      "H19</w>\n",
      "1.93</w>\n",
      "1.94</w>\n",
      "PCSK9</w>\n",
      "29.5</w>\n",
      "1.97</w>\n",
      "1967</w>\n",
      "1.90</w>\n",
      "1.91</w>\n",
      "329</w>\n",
      "1.9\n",
      "19th</w>\n",
      "98.\n",
      "CYP2C9</w>\n",
      "109\n",
      "339</w>\n",
      "9,10</w>\n",
      "23.9</w>\n",
      "29.6</w>\n",
      "1.99</w>\n",
      "25.9</w>\n",
      "39.5</w>\n",
      "26.9</w>\n",
      "29.2</w>\n",
      "0.99\n",
      "29.9</w>\n",
      "609</w>\n",
      "97.\n",
      "LY294002</w>\n",
      "24.9</w>\n",
      "28.9</w>\n",
      "2.09</w>\n",
      "396</w>\n",
      "HSP90</w>\n",
      "9th</w>\n",
      "F9</w>\n",
      "97.5</w>\n",
      "49.\n",
      "29.3</w>\n",
      "MMP9</w>\n",
      "N9</w>\n",
      "198\n",
      "395</w>\n",
      "389</w>\n",
      "95.\n",
      "93.3</w>\n",
      "88.9</w>\n",
      "96.\n",
      "509</w>\n",
      "397</w>\n",
      "429</w>\n",
      "392</w>\n",
      "46619</w>\n",
      "29.7</w>\n",
      "HT29</w>\n",
      "1964</w>\n",
      "27.9</w>\n",
      "29.1</w>\n",
      "29.8</w>\n",
      "CA19</w>\n",
      "349</w>\n",
      "TLR9</w>\n",
      "95th</w>\n",
      "PD98059</w>\n",
      "359</w>\n",
      "391</w>\n",
      "9a</w>\n",
      "369</w>\n",
      "38.9</w>\n",
      "379</w>\n",
      "91.7</w>\n",
      "497</w>\n",
      "394</w>\n",
      "59.\n",
      "P9</w>\n",
      "2.19</w>\n",
      "92.3</w>\n",
      "590</w>\n",
      "549</w>\n",
      "09\n",
      "495</w>\n",
      "2,9\n",
      "V79</w>\n",
      "94.\n",
      "399</w>\n",
      "499</w>\n",
      "90.9</w>\n",
      "1950s</w>\n",
      "2.29</w>\n",
      "89.\n",
      "494</w>\n",
      "30.9</w>\n",
      "498</w>\n",
      "99.9</w>\n",
      "69.\n",
      "449</w>\n",
      "L929</w>\n",
      "H9</w>\n",
      "34.9</w>\n",
      "31.9</w>\n",
      "1963</w>\n",
      "69.2</w>\n",
      "39.1</w>\n",
      "439</w>\n",
      "39.3</w>\n",
      "937</w>\n",
      "619</w>\n",
      "929</w>\n",
      "98059</w>\n",
      "294\n",
      "99mT\n",
      "980\n",
      "294002</w>\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key,_ in tokenizer.get_vocab().items():\n",
    "    # if key includes signs and length > 1, but not match the parttern: alphabeet+</w>, print it\n",
    "    if \"9\" in key:\n",
    "        print(key)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yl</w>\n",
      "phosphorylation</w>\n",
      "yl\n",
      "methyl</w>\n",
      "methyl\n",
      "methylation</w>\n",
      "pylori</w>\n",
      "amyloid</w>\n",
      "ethyl</w>\n",
      "prophylaxis</w>\n",
      "Staphylococcus</w>\n",
      "phosphorylated</w>\n",
      "phenyl\n",
      "phenyl</w>\n",
      "ylation</w>\n",
      "phylogenetic</w>\n",
      "acetylcholine</w>\n",
      "acetyl</w>\n",
      "lifestyle</w>\n",
      "ethylene</w>\n",
      "acetyl\n",
      "dimethyl\n",
      "ethyl\n",
      "acyl</w>\n",
      "hydroxylase</w>\n",
      "prophylactic</w>\n",
      "polyacrylamide</w>\n",
      "ylated</w>\n",
      "glycosylation</w>\n",
      "amylase</w>\n",
      "alkyl</w>\n",
      "adenylate</w>\n",
      "phenylalanine</w>\n",
      "dimethyl</w>\n",
      "hydroxyl</w>\n",
      "methylated</w>\n",
      "polyethylene</w>\n",
      "phosphatidylinositol</w>\n",
      "phosphatidylcholine</w>\n",
      "aryl</w>\n",
      "osyl</w>\n",
      "carboxyl</w>\n",
      "style</w>\n",
      "carbonyl</w>\n",
      "dodecyl</w>\n",
      "phyl\n",
      "fentanyl</w>\n",
      "ylo\n",
      "acetylation</w>\n",
      "xyl\n",
      "glycosylated</w>\n",
      "decarboxylase</w>\n",
      "methylene</w>\n",
      "theophylline</w>\n",
      "butyl</w>\n",
      "alkyl\n",
      "aryl\n",
      "propyl</w>\n",
      "methyltransferase</w>\n",
      "amyl\n",
      "benzyl</w>\n",
      "oyl</w>\n",
      "carboxylic</w>\n",
      "amyloidosis</w>\n",
      "chlorophyll</w>\n",
      "inyl</w>\n",
      "diethyl\n",
      "methacrylate</w>\n",
      "hydroxylation</w>\n",
      "acetyltransferase</w>\n",
      "Phylogenetic</w>\n",
      "acetylcholinesterase</w>\n",
      "Campylobacter</w>\n",
      "phenylephrine</w>\n",
      "acrylic</w>\n",
      "vinyl</w>\n",
      "yltransferase</w>\n",
      "osyl\n",
      "propyl\n",
      "osyltransferase</w>\n",
      "ylene</w>\n",
      "onyl</w>\n",
      "phylogeny</w>\n",
      "staphylococcal</w>\n",
      "diphenyl\n",
      "phosphorylase</w>\n",
      "acrylamide</w>\n",
      "enyl</w>\n",
      "benzyl\n",
      "ylic</w>\n",
      "cylindrical</w>\n",
      "ylate</w>\n",
      "trimethyl\n",
      "Methyl\n",
      "acyl\n",
      "diphenyl</w>\n",
      "ylamine</w>\n",
      "styles</w>\n",
      "methylprednisolone</w>\n",
      "anaphylaxis</w>\n",
      "acetylated</w>\n",
      "butyl\n",
      "carboxylate</w>\n",
      "condylar</w>\n",
      "xylose</w>\n",
      "dephosphorylation</w>\n",
      "spondylo\n",
      "syl\n",
      "deacetylase</w>\n",
      "staphylococci</w>\n",
      "alkylation</w>\n",
      "carboxylase</w>\n",
      "adenylyl</w>\n",
      "Phosphorylation</w>\n",
      "sulfhydryl</w>\n",
      "diacylglycerol</w>\n",
      "spondylitis</w>\n",
      "acylation</w>\n",
      "anyl</w>\n",
      "phosphatidylserine</w>\n",
      "glutamyl</w>\n",
      "phosphatidylethanolamine</w>\n",
      "condyle</w>\n",
      "ylase</w>\n",
      "hematoxylin</w>\n",
      "cholesteryl</w>\n",
      "alkylating</w>\n",
      "isopropyl\n",
      "cylinder</w>\n",
      "demethylation</w>\n",
      "salicylate</w>\n",
      "syl</w>\n",
      "syll\n",
      "oyl\n",
      "hydroxyl\n",
      "biphenyl</w>\n",
      "yle\n",
      "dyl\n",
      "vinyl\n",
      "biotinylated</w>\n",
      "pyridyl</w>\n",
      "isopropyl</w>\n",
      "benzoyl</w>\n",
      "triacylglycerol</w>\n",
      "salicylic</w>\n",
      "allyl</w>\n",
      "ankylosing</w>\n",
      "pyloric</w>\n",
      "Methyl</w>\n",
      "guanylate</w>\n",
      "ethylenedi\n",
      "palmitoyl</w>\n",
      "hexyl</w>\n",
      "hypermethylation</w>\n",
      "hydroxylated</w>\n",
      "phylogenetically</w>\n",
      "acetylcysteine</w>\n",
      "ophyll\n",
      "condyl\n",
      "acyltransferase</w>\n",
      "phosphoryl\n",
      "nitrophenyl</w>\n",
      "glycosyl\n",
      "Methylation</w>\n",
      "Amyloid</w>\n",
      "yloxy</w>\n",
      "propylene</w>\n",
      "formyl</w>\n",
      "phenyleth\n",
      "acrylate</w>\n",
      "yle</w>\n",
      "autophosphorylation</w>\n",
      "biphenyls</w>\n",
      "diethyl</w>\n",
      "dibutyryl</w>\n",
      "yls</w>\n",
      "ophylline</w>\n",
      "hydroxyethyl</w>\n",
      "acryl\n",
      "silyl</w>\n",
      "polypropylene</w>\n",
      "methylphenidate</w>\n",
      "Prophylactic</w>\n",
      "xylem</w>\n",
      "prolyl</w>\n",
      "amitriptyline</w>\n",
      "polyadenylation</w>\n",
      "acetylglucosamine</w>\n",
      "nylon</w>\n",
      "yloxy\n",
      "enyl\n",
      "amyloid\n",
      "ylates</w>\n",
      "oxyl</w>\n",
      "acylated</w>\n",
      "ophenyl</w>\n",
      "ylcholine</w>\n",
      "phosphorylates</w>\n",
      "sulfonyl</w>\n",
      "polyl\n",
      "polyvinyl</w>\n",
      "anaphylactic</w>\n",
      "dactyly</w>\n",
      "palmitoyl\n",
      "onyl\n",
      "ribosylation</w>\n",
      "Xyl\n",
      "ynyl</w>\n",
      "phosphorylate</w>\n",
      "lifestyles</w>\n",
      "tetramethyl\n",
      "arboxylate</w>\n",
      "chylomic\n",
      "Lifestyle</w>\n",
      "oxyl\n",
      "deacetyl\n",
      "omethyl</w>\n",
      "ethylmaleimide</w>\n",
      "Phyl\n",
      "osylceramide</w>\n",
      "osylated</w>\n",
      "phosphatidylglycerol</w>\n",
      "phylum</w>\n",
      "thylakoid</w>\n",
      "methylcellulose</w>\n",
      "acetylsalicylic</w>\n",
      "demethylase</w>\n",
      "hydroxymethyl</w>\n",
      "Pennsylvania</w>\n",
      "ethylene\n",
      "tetradecanoylphorbol</w>\n",
      "glycosyl</w>\n",
      "xylanase</w>\n",
      "amyl</w>\n",
      "phosphatidyl</w>\n",
      "xylo\n",
      "phyla</w>\n",
      "spondylolisthesis</w>\n",
      "triphenyl\n",
      "ethylamine</w>\n",
      "renyl</w>\n",
      "carboxymethyl</w>\n",
      "ophyll</w>\n",
      "alkylated</w>\n",
      "cyl</w>\n",
      "methylglutaryl</w>\n",
      "dyl</w>\n",
      "dipeptidyl</w>\n",
      "deoxynucleotidyl</w>\n",
      "phyllo\n",
      "aminoethyl</w>\n",
      "pentoxifylline</w>\n",
      "mesylate</w>\n",
      "ophyl\n",
      "alanyl</w>\n",
      "osyl-</w>\n",
      "monophyletic</w>\n",
      "Maryland</w>\n",
      "acetylene</w>\n",
      "Acetyl\n",
      "dicarboxylic</w>\n",
      "pegylated</w>\n",
      "oleoyl</w>\n",
      "octyl</w>\n",
      "allylic</w>\n",
      "osylation</w>\n",
      "omethyl\n",
      "polytetrafluoroethylene</w>\n",
      "atidylcholine</w>\n",
      "xylene</w>\n",
      "thymidylate</w>\n",
      "methylcholanthrene</w>\n",
      "hydroxypropyl</w>\n",
      "lysyl</w>\n",
      "arboxylic</w>\n",
      "cylinders</w>\n",
      "hydroxylamine</w>\n",
      "omethylation</w>\n",
      "decarboxylation</w>\n",
      "strongylus</w>\n",
      "glycosylase</w>\n",
      "ylor</w>\n",
      "aminoacyl</w>\n",
      "actyl\n",
      "dimethylsiloxane</w>\n",
      "malonyl</w>\n",
      "ondylar</w>\n",
      "peptidyl</w>\n",
      "Phenyl\n",
      "benzoyl\n",
      "rophyl\n",
      "hydroxyphenyl</w>\n",
      "Staphyloc\n",
      "yl-</w>\n",
      "leucyl</w>\n",
      "methylmercury</w>\n",
      "methoxyphenyl</w>\n",
      "adenosylmethionine</w>\n",
      "PEGylated</w>\n",
      "phenylacetic</w>\n",
      "methylamine</w>\n",
      "polyvinyl\n",
      "yline</w>\n",
      "monomethyl</w>\n",
      "pentyl\n",
      "chlorophenyl</w>\n",
      "phosphatidyl\n",
      "deacetylation</w>\n",
      "pylorus</w>\n",
      "formyl\n",
      "nobyl</w>\n",
      "methionyl</w>\n",
      "cotyledons</w>\n",
      "trifluoromethyl</w>\n",
      "tricarboxylic</w>\n",
      "oxylin</w>\n",
      "guanylyl</w>\n",
      "spondyl\n",
      "triacylglycerols</w>\n",
      "xylitol</w>\n",
      "ryl</w>\n",
      "ylosis</w>\n",
      "allyl\n",
      "sulfonylurea</w>\n",
      "ethylen\n",
      "methyltransferases</w>\n",
      "Acetylcholine</w>\n",
      "cyanoacrylate</w>\n",
      "isobutyl</w>\n",
      "ylin\n",
      "atidyl\n",
      "sialyl</w>\n",
      "idyl</w>\n",
      "phosphorylating</w>\n",
      "trimethylammonium</w>\n",
      "ylmethyl</w>\n",
      "demethyl\n",
      "tetraethylammonium</w>\n",
      "chemoprophylaxis</w>\n",
      "methylenedi\n",
      "Prophyl\n",
      "ylus</w>\n",
      "sulfonyl\n",
      "prophyl\n",
      "rophyll</w>\n",
      "amylo\n",
      "ylitis</w>\n",
      "tyline</w>\n",
      "cotyled\n",
      "tyly</w>\n",
      "acylglycerol</w>\n",
      "chyl\n",
      "thylak\n",
      "staphyloc\n",
      "yled\n",
      "acylglycerols</w>\n",
      "ylosing</w>\n",
      "adenosyl\n",
      "ylline</w>\n",
      "pyl\n",
      "ylon</w>\n",
      "yles</w>\n",
      "butyryl</w>\n",
      "cylindr\n",
      "acylglycer\n",
      "cylin\n",
      "ophosphorylation</w>\n",
      "oxifylline</w>\n",
      "idylate</w>\n",
      "ondyl\n",
      "thyl\n",
      "glutaryl</w>\n",
      "ethylammonium</w>\n",
      "ylyl</w>\n",
      "yland</w>\n",
      "estyle</w>\n",
      "ylobacter</w>\n",
      "riptyline</w>\n",
      "sylvan\n",
      "tetrafluoroethylene</w>\n",
      "ylob\n",
      "phenyls</w>\n",
      "estyles</w>\n",
      "ylphorbol</w>\n",
      "dyle</w>\n",
      "orylation</w>\n",
      "Pennsylvan\n",
      "aphyloc\n"
     ]
    }
   ],
   "source": [
    "for key,_ in tokenizer.get_vocab().items():\n",
    "    # if key includes \"farnes\", print it\n",
    "    if re.search(r'yl', key):\n",
    "        print(key)\n",
    "        count += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer for moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 03:09:31 | INFO | fairseq.file_utils | loading archive file checkpoints/RE-DTI-BioGPT\n",
      "2023-04-24 03:09:31 | INFO | fairseq.file_utils | loading archive file data/KD-DTI/relis-bin\n",
      "2023-04-24 03:09:32 | INFO | src.language_modeling_prompt | dictionary: 42384 types\n",
      "2023-04-24 03:09:35 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'fastbpe', 'bpe_codes': 'data/bpecodes'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'en', 'moses_no_dash_splits': False, 'moses_no_escape': False}}\n",
      "Loading codes from data/bpecodes ...\n",
      "Read 40000 codes from the codes file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerLanguageModelPrompt(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(42393, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=42393, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.transformer_lm_prompt import TransformerLanguageModelPrompt\n",
    "m = TransformerLanguageModelPrompt.from_pretrained(\n",
    "        \"checkpoints/RE-DTI-BioGPT\", \n",
    "        \"checkpoint_avg.pt\", \n",
    "        \"data/KD-DTI/relis-bin\",\n",
    "        tokenizer='moses', \n",
    "        bpe='fastbpe', \n",
    "        bpe_codes=\"data/bpecodes\",\n",
    "        max_len_b=1024,\n",
    "        beam=5)\n",
    "m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def encode(self, sentence: str) -> torch.LongTensor:\n",
      "        sentence = self.tokenize(sentence)\n",
      "        sentence = self.apply_bpe(sentence)\n",
      "        return self.binarize(sentence)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(m.encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def encode(self, x: str) -> str:\n",
      "        return self.tok.tokenize(\n",
      "            x,\n",
      "            aggressive_dash_splits=(not self.cfg.moses_no_dash_splits),\n",
      "            return_str=True,\n",
      "            escape=(not self.cfg.moses_no_escape),\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(m.tokenizer.encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def tokenize(\n",
      "        self,\n",
      "        text,\n",
      "        aggressive_dash_splits=False,\n",
      "        return_str=False,\n",
      "        escape=True,\n",
      "        protected_patterns=None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Python port of the Moses tokenizer.\n",
      "\n",
      "            :param tokens: A single string, i.e. sentence text.\n",
      "            :type tokens: str\n",
      "            :param aggressive_dash_splits: Option to trigger dash split rules .\n",
      "            :type aggressive_dash_splits: bool\n",
      "        \"\"\"\n",
      "        # Converts input string into unicode.\n",
      "        text = text_type(text)\n",
      "        # De-duplicate spaces and clean ASCII junk\n",
      "        for regexp, substitution in [self.DEDUPLICATE_SPACE, self.ASCII_JUNK]:\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "\n",
      "        if protected_patterns:\n",
      "            # Find the tokens that needs to be protected.\n",
      "            protected_tokens = [\n",
      "                match.group()\n",
      "                for protected_pattern in protected_patterns\n",
      "                for match in re.finditer(protected_pattern, text, re.IGNORECASE)\n",
      "            ]\n",
      "            # Apply the protected_patterns.\n",
      "            for i, token in enumerate(protected_tokens):\n",
      "                substituition = \"THISISPROTECTED\" + str(i).zfill(3)\n",
      "                text = text.replace(token, substituition)\n",
      "\n",
      "        # Strips heading and trailing spaces.\n",
      "        text = text.strip()\n",
      "        # FIXME!!!\n",
      "        \"\"\"\n",
      "        # For Finnish and Swedish, seperate out all \"other\" special characters.\n",
      "        if self.lang in [\"fi\", \"sv\"]:\n",
      "            # In Finnish and Swedish, the colon can be used inside words\n",
      "            # as an apostrophe-like character:\n",
      "            # USA:n, 20:een, EU:ssa, USA:s, S:t\n",
      "            regexp, substitution = self.FI_SV_COLON_APOSTROPHE\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "            # If a colon is not immediately followed by lower-case characters,\n",
      "            # separate it out anyway.\n",
      "            regexp, substitution = self.FI_SV_COLON_NO_LOWER_FOLLOW\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "        else:\n",
      "        \"\"\"\n",
      "        # Separate special characters outside of IsAlnum character set.\n",
      "        regexp, substitution = self.PAD_NOT_ISALNUM\n",
      "        text = re.sub(regexp, substitution, text)\n",
      "        # Aggressively splits dashes\n",
      "        if aggressive_dash_splits:\n",
      "            regexp, substitution = self.AGGRESSIVE_HYPHEN_SPLIT\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "\n",
      "        # Replaces multidots with \"DOTDOTMULTI\" literal strings.\n",
      "        text = self.replace_multidots(text)\n",
      "\n",
      "        # Separate out \",\" except if within numbers e.g. 5,300\n",
      "        for regexp, substitution in [\n",
      "            self.COMMA_SEPARATE_1,\n",
      "            self.COMMA_SEPARATE_2,\n",
      "            self.COMMA_SEPARATE_3,\n",
      "        ]:\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "\n",
      "        # (Language-specific) apostrophe tokenization.\n",
      "        if self.lang == \"en\":\n",
      "            for regexp, substitution in self.ENGLISH_SPECIFIC_APOSTROPHE:\n",
      "                text = re.sub(regexp, substitution, text)\n",
      "        elif self.lang in [\"fr\", \"it\"]:\n",
      "            for regexp, substitution in self.FR_IT_SPECIFIC_APOSTROPHE:\n",
      "                text = re.sub(regexp, substitution, text)\n",
      "        # FIXME!!!\n",
      "        ##elif self.lang == \"so\":\n",
      "        ##    for regexp, substitution in self.SO_SPECIFIC_APOSTROPHE:\n",
      "        ##        text = re.sub(regexp, substitution, text)\n",
      "        else:\n",
      "            regexp, substitution = self.NON_SPECIFIC_APOSTROPHE\n",
      "            text = re.sub(regexp, substitution, text)\n",
      "\n",
      "        # Handles nonbreaking prefixes.\n",
      "        text = self.handles_nonbreaking_prefixes(text)\n",
      "        # Cleans up extraneous spaces.\n",
      "        regexp, substitution = self.DEDUPLICATE_SPACE\n",
      "        text = re.sub(regexp, substitution, text).strip()\n",
      "        # Split trailing \".'\".\n",
      "        regexp, substituition = self.TRAILING_DOT_APOSTROPHE\n",
      "        text = re.sub(regexp, substituition, text)\n",
      "\n",
      "        # Restore the protected tokens.\n",
      "        if protected_patterns:\n",
      "            for i, token in enumerate(protected_tokens):\n",
      "                substituition = \"THISISPROTECTED\" + str(i).zfill(3)\n",
      "                text = text.replace(substituition, token)\n",
      "\n",
      "        # Restore multidots.\n",
      "        text = self.restore_multidots(text)\n",
      "        if escape:\n",
      "            # Escape XML symbols.\n",
      "            text = self.escape_xml(text)\n",
      "\n",
      "        return text if return_str else text.split()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(m.tokenizer.tok.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d @-@ alpha @-@ tocopheron 7894 @-@ huy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.tokenizer.tok.tokenize(\"d-alpha-tocopheron 7894-huy\",\n",
    "                         aggressive_dash_splits=(not m.tokenizer.cfg.moses_no_dash_splits),\n",
    "                         return_str=True,\n",
    "                         escape=(not m.tokenizer.cfg.moses_no_escape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for pmid: 9003517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"farnesyltransferase\"\n",
    "b = \"farnesylation\"\n",
    "c = \"farnesyltransferase (ftase)\"\n",
    "a_context = \"that inhibit ras farnesyltransferase. mutational\"\n",
    "b_context = \"is the farnesylation of a cysteine residue\"\n",
    "c_context = \"by the enzyme farnesyltransferase (ftase). inhibitors of\"\n",
    "text = \"structure-activity relationships of cysteine-lacking pentapeptide derivatives that inhibit ras farnesyltransferase. mutational activation of ras has been found in many types of human cancers, including a greater than 50% incidence in colon and about 90% in pancreatic carcinomas. the activity of both native and oncogenic ras proteins requires a series of post-translational processing steps. the first event in this process is the farnesylation of a cysteine residue located in the fourth position from the carboxyl terminus of the ras protein, catalyzed by the enzyme farnesyltransferase (ftase). inhibitors of ftase are potential candidates for development as antitumor agents. through a high-volume screening program, the pentapeptide derivative pd083176 (1), cbz-his-tyr(obn)-ser(obn)-trp-dala-nh2, was identified as an inhibitor of rat brain ftase, with an ic50 of 20 nm. structure-activity relationships were carried out to determine the importance of the side chain and chirality of each residue. this investigation led to a series of potent ftase inhibitors which lack a cysteine residue as found in the ras peptide substrate. the parent compound (1) inhibited the insulin-induced maturation of xenopus oocytes (concentration: 5 pmol/oocyte), a process which is dependent on the activation of the ras pathway.\"\n",
    "\n",
    "\n",
    "t = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19962, 12341]\n",
      "[19962, 12341]\n",
      "\n",
      "['farnes', 'yltransferase</w>']\n",
      "['farnes', 'yltransferase</w>']\n"
     ]
    }
   ],
   "source": [
    "print(m.encode(t)[:-1].tolist())\n",
    "print(tokenizer.encode(t)[1:])\n",
    "\n",
    "print()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(m.encode(t))[:-1])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(t))[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19962, 12341]\n",
      "[19962, 12341]\n",
      "\n",
      "['farnes', 'yltransferase</w>']\n",
      "['farnes', 'yltransferase</w>']\n"
     ]
    }
   ],
   "source": [
    "print(m.encode(t)[:-1].tolist())\n",
    "print(tokenizer.encode(t)[1:])\n",
    "\n",
    "print()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(m.encode(t))[:-1])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(t))[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19962, 12341]\n",
      "[19962, 12341]\n",
      "\n",
      "['farnes', 'yltransferase</w>']\n",
      "['farnes', 'yltransferase</w>']\n"
     ]
    }
   ],
   "source": [
    "print(m.encode(t)[:-1].tolist())\n",
    "print(tokenizer.encode(t)[1:])\n",
    "\n",
    "print()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(m.encode(t))[:-1])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(t))[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the similarity of words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestalt pattern matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# using Jaccard similarity to calculate the similarity between two sets\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection/union\n",
    "\n",
    "print(jaccard_similarity(set(a), set(b)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestalt (difflib, SequenceMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def gestalt(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    distance = lev(s1, s2)\n",
    "    return 1 - (distance / max(len(s1), len(s2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test word:  farnesyltransferase\n"
     ]
    }
   ],
   "source": [
    "# for content in a string, if there is \". \", then subplace it with \" . \"\n",
    "def add_space(text):\n",
    "    text = text.strip()\n",
    "    text = text + \" \"\n",
    "    return (text.replace(\". \", \" . \"))\n",
    "\n",
    "processed_text  = add_space(text)\n",
    "words = processed_text.split()\n",
    "words_set = set(words)\n",
    "\n",
    "test_word = a\n",
    "print(\"test word: \",test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farnesylation', 'farnesyltransferase']\n"
     ]
    }
   ],
   "source": [
    "similar_words_gestalt = []\n",
    "for w in words_set:\n",
    "    if gestalt(test_word, w) > 0.5 and len(w) > 2:\n",
    "        similar_words_gestalt.append(w)\n",
    "\n",
    "print(similar_words_gestalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['farnesyltransferase', 'farnesylation', 'relationships']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "\n",
    "get_close_matches(test_word, words_set, n=3, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farnesylation', 'farnesyltransferase']\n"
     ]
    }
   ],
   "source": [
    "similar_words_jaccard = []\n",
    "test_word_set = set(test_word)\n",
    "for w in words_set:\n",
    "    w_set = set(w)\n",
    "    if jaccard_similarity(test_word_set, w_set) > 0.6 and len(w) > 2:\n",
    "        similar_words_jaccard.append(w)\n",
    "\n",
    "print(similar_words_jaccard )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farnesylation', 'farnesyltransferase']\n"
     ]
    }
   ],
   "source": [
    "similar_words_lev = []\n",
    "for w in words_set:\n",
    "    if levenshtein_distance(test_word, w) > 0.4 and len(w) > 2:\n",
    "        similar_words_lev.append(w)\n",
    "\n",
    "print(similar_words_lev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the same parts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "longest common sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(S,T):\n",
    "    m = len(S)\n",
    "    n = len(T)\n",
    "    counter = [[0]*(n+1) for x in range(m+1)]\n",
    "    longest = 0\n",
    "    lcs_set = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if S[i] == T[j]:\n",
    "                c = counter[i][j] + 1\n",
    "                counter[i+1][j+1] = c\n",
    "                if c > longest:\n",
    "                    lcs_set = set()\n",
    "                    longest = c\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "                elif c == longest:\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "\n",
    "    return list(lcs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'farnes'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs(test_word, \"farnes\")[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "longest common subsequence (find all subseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longest common subsequence in Python\n",
    "# Function to find lcs_algo\n",
    "def lcs_algo(S1, S2, m, n):\n",
    "    L = [[0 for x in range(n+1)] for x in range(m+1)]\n",
    "\n",
    "    # Building the mtrix in bottom-up way\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0 or j == 0:\n",
    "                L[i][j] = 0\n",
    "            elif S1[i-1] == S2[j-1]:\n",
    "                L[i][j] = L[i-1][j-1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    "\n",
    "    index = L[m][n]\n",
    "\n",
    "    lcs_algo = [\"\"] * (index+1)\n",
    "    lcs_algo[index] = \"\"\n",
    "\n",
    "    i = m\n",
    "    j = n\n",
    "    while i > 0 and j > 0:\n",
    "\n",
    "        if S1[i-1] == S2[j-1]:\n",
    "            lcs_algo[index-1] = S1[i-1]\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            index -= 1\n",
    "\n",
    "        elif L[i-1][j] > L[i][j-1]:\n",
    "            i -= 1\n",
    "        else:\n",
    "            j -= 1\n",
    "            \n",
    "    # Printing the sub sequences\n",
    "    print(\"S1 : \" + S1 + \"\\nS2 : \" + S2)\n",
    "    print(\"LCS: \" + \"\".join(lcs_algo))\n",
    "    return lcs_algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 : farnesylation\n",
      "S2 : relationships\n",
      "LCS: relation\n",
      "['r', 'e', 'l', 'a', 't', 'i', 'o', 'n', '']\n"
     ]
    }
   ],
   "source": [
    "m = len('farnesylation')\n",
    "n = len('relationships')\n",
    "print(lcs_algo('farnesylation', 'relationships', m, n))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic algorithm (difflib.get_close_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create similar tuple list\n",
    "# create a list of tuples, for a word w in word_set, find the similar_words_gestalt for w, then create a tuple (similar_words_gestalt, w), a word won't be included in multiple tuples.\n",
    "def create_similar_tuples(test_data):\n",
    "    similar_tuples = []\n",
    "    dynamic_words_set = [w for w in test_data['words'] if len(w) > 2 and w.isalpha() and len(tokenizer.encode(w, add_special_tokens=False)) > 1]\n",
    "    dynamic_words_set = set(dynamic_words_set)\n",
    "    # print(\"the words set: \",dynamic_words_set)\n",
    "    while dynamic_words_set:\n",
    "        similar_words = []\n",
    "        w = dynamic_words_set.pop()\n",
    "        # print(w)\n",
    "        for w1 in dynamic_words_set:\n",
    "            if gestalt(w, w1) > 0.3 and len(w1) > 2 and len(lcs(w, w1)[0]) > 3:\n",
    "                # if w == \"farnesylation\":\n",
    "                #     print(\"w1: \",w1)\n",
    "                similar_words.append(w1)\n",
    "        if len(similar_words) != 0:\n",
    "            similar_words.append(w)\n",
    "            similar_tuples.append(similar_words)\n",
    "            dynamic_words_set.difference_update(similar_words[:-1])\n",
    "    # print(\"similar tuples: \", similar_tuples)\n",
    "    return similar_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the common part in the similar words, only consider the first longest part\n",
    "# input: similar_tuples\n",
    "# output: common_str or None\n",
    "import difflib\n",
    "def find_common(similar_tuples):\n",
    "    words = similar_tuples[0]\n",
    "\n",
    "    common_parts = []\n",
    "\n",
    "    for similar_tuple in similar_tuples:\n",
    "        common_str = similar_tuple[0]\n",
    "        skip = 0\n",
    "        for word in similar_tuple[1:]:\n",
    "            common = difflib.SequenceMatcher(None, common_str, word).find_longest_match(0, len(common_str), 0, len(word))\n",
    "            if common.size == 0:\n",
    "                # print(\"None.\")\n",
    "                skip = 1\n",
    "                break\n",
    "            else:\n",
    "                common_str = common_str[common.a:common.a+common.size]\n",
    "        \n",
    "        if skip:\n",
    "            common_parts.append(\"<none>\")\n",
    "            continue\n",
    "        common_parts.append(common_str)\n",
    "\n",
    "    if len(common_parts) > 0:\n",
    "        # print(common_parts)\n",
    "        return common_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize (truncate) the words in the same tuple, make a dictoinary\n",
    "# input: similar_tuples, common_parts\n",
    "# output: similar_tuple_dict\n",
    "\n",
    "def tokenize_tuple(similar_tuples, common_parts):\n",
    "    similar_tuple_dict = {}\n",
    "\n",
    "    for common, tuple in zip(common_parts, similar_tuples):\n",
    "        if common == \"<none>\":\n",
    "            continue\n",
    "        else:\n",
    "            for word in tuple:\n",
    "                # print(w)\n",
    "                separated_w  = [word.split(common)[0], common, word.split(common)[1]]\n",
    "                # remove empty string\n",
    "                separated_w = [w for w in separated_w if w != '']\n",
    "\n",
    "                token = []\n",
    "\n",
    "                for w in separated_w[:-1]:\n",
    "                    token.extend(tokenizer.tokenize(w + \"9\")[:-1])\n",
    "                token.extend(tokenizer.tokenize(separated_w[-1]))\n",
    "                # print(token)\n",
    "                # print(word)\n",
    "                similar_tuple_dict[word] = token\n",
    "    \n",
    "    return similar_tuple_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization for the text\n",
    "# input: title, text, similar_tuple_dict, add_subfix\n",
    "# output: tokenized_text (pt, cuda)\n",
    "\n",
    "def add_space(text):\n",
    "    text = text.strip()\n",
    "    text = text + \" \"\n",
    "    return (text.replace(\". \", \" . \"))\n",
    "\n",
    "def tokenization(test_data, similar_tuple_dict, add_subfix= False):\n",
    "    text_w = test_data['words']\n",
    "    tokenized_text = []\n",
    "    for w in text_w:\n",
    "        if w in similar_tuple_dict:\n",
    "            tokenized_text.extend(tokenizer.convert_tokens_to_ids(similar_tuple_dict[w]))\n",
    "        else:\n",
    "            tokenized_text.extend(tokenizer.encode(w, add_special_tokens=False))\n",
    "    if add_subfix:\n",
    "        tokenized_text.extend([2, 42384, 42385, 42386, 42387, 42388, 42389, 42390, 42391, 42392])\n",
    "    else:\n",
    "        tokenized_text.extend([2])\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenized all test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(id, test_file):\n",
    "    # prefix = torch.arange(42384, 42393)\n",
    "    test_data = {}\n",
    "    test_data['pmid'] = id\n",
    "    test_data['text'] = test_file[test_data['pmid']]['title'].strip() + \" \" + test_file[test_data['pmid']]['abstract']\n",
    "    test_data['text'] = test_data['text'].lower().strip().replace('  ', ' ')\n",
    "    test_data['text'] = add_space(test_data['text'])\n",
    "    test_data['words'] = test_data['text'].split()\n",
    "    # test_data['text_tokens'] = m.encode(test_data['text'])\n",
    "    # test_data['text_tokens_with_prefix'] = torch.cat([test_data['text_tokens'], prefix], dim=-1).unsqueeze(0).cuda()\n",
    "    # test_data['gold_triples'] = gold_d[test_data['pmid']]['triples']\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selective inhibitor of janus tyrosine kinase 3, pnu156804, prolongs allograft survival and acts synergistically with cyclosporine but additively with rapamycin . janus kinase 3 (jak3) is a cytoplasmic tyrosine (tyr) kinase associated with the interleukin-2 (il-2) receptor common gamma chain (gamma(c)) that is activated by multiple t-cell growth factors (tcgfs) such as il-2, -4, and -7 . using human t cells, it was found that a recently discovered variant of the undecylprodigiosin family of antibiotics, pnu156804, previously shown to inhibit il-2-induced cell proliferation, also blocks il-2-mediated jak3 auto-tyrosine phosphorylation, activation of jak3 substrates signal transducers and activators of transcription (stat) 5a and stat5b, and extracellular regulated kinase 1 (erk1) and erk2 (p44/p42) . although pnu156804 displayed similar efficacy in blocking jak3-dependent t-cell proliferation by il-2, -4, -7, or -15, it was more than 2-fold less effective in blocking jak2-mediated cell growth, its most homologous jak family member . a 14-day alternate-day oral gavage with 40 to 120 mg/kg pnu156804 extended the survival of heart allografts in a dose-dependent fashion . in vivo, pnu156804 acted synergistically with the signal 1 inhibitor cyclosporine a (csa) and additively with the signal 3 inhibitor rapamycin to block allograft rejection . it is concluded that inhibition of signal 3 alone by targeting jak3 in combination with a signal 1 inhibitor provides a unique strategy to achieve potent immunosuppression . \n",
      "1159\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file = 'data/KD-DTI/raw/test.json'\n",
    "\n",
    "# load the gold standard\n",
    "with open (file, 'r') as f:\n",
    "     test_file = json.load(f)\n",
    "\n",
    "test_file_keys = list(test_file.keys())\n",
    "\n",
    "test_data = get_test_data(test_file_keys[1], test_file)\n",
    "print(test_data['text'])\n",
    "print(len(test_file_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the words set:  {'xenopus', 'farnesylation', 'farnesyltransferase', 'ftase', 'pentapeptide'}\n"
     ]
    }
   ],
   "source": [
    "test_data = get_test_data(\"9003517\", test_file)\n",
    "similar_tuples = create_similar_tuples(test_data)\n",
    "common_parts = find_common(similar_tuples)\n",
    "similar_tuple_dict = tokenize_tuple(similar_tuples, common_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['farnesyltransferase', 'farnesylation']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['farnesyl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'farnesyltransferase': ['farnes', 'yl', 'transferase</w>'],\n",
       " 'farnesylation': ['farnes', 'yl', 'ation</w>']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tuple_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 / 1159\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(test_file_keys)):\n",
    "    test_data = get_test_data(test_file_keys[i], test_file)\n",
    "    similar_tuples = create_similar_tuples(test_data)\n",
    "    if len(similar_tuples) > 0:\n",
    "        count += 1\n",
    "\n",
    "print(f\"{count} / {len(test_file_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ids = []\n",
    "for i in range(len(test_file_keys)):\n",
    "    test_data = get_test_data(test_file_keys[i], test_file)\n",
    "    similar_tuples = create_similar_tuples(test_data)\n",
    "    if len(similar_tuples) == 0:\n",
    "        tokenized_id = tokenizer.encode(test_data['text'], add_special_tokens=False)\n",
    "        tokenized_id.extend([2])\n",
    "    else:\n",
    "        common_parts = find_common(similar_tuples)\n",
    "\n",
    "        similar_tuple_dict = tokenize_tuple(similar_tuples, common_parts)\n",
    "        tokenized_id = tokenization(test_data, similar_tuple_dict, add_subfix=False)\n",
    "\n",
    "    tokenized_ids.append(tokenized_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenized_ids line by line\n",
    "with open('data/KD-DTI/raw/optimized_tokenization_test.x.json', 'w') as f:\n",
    "    json.dump(tokenized_ids, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
