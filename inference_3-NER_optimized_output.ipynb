{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.transformer_lm_prompt import TransformerLanguageModelPrompt\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from only_tokens_in_original import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_file = 'data/KD-DTI/raw/optimized_tokenization_test.x.json'\n",
    "# with open(src_file) as reader:\n",
    "#      src_inputs_ids = json.load(reader)\n",
    "\n",
    "src_file = 'data/KD-DTI/raw/relis_test.tok.bpe.x'\n",
    "src_inputs_ids = []\n",
    "with open(src_file) as reader:\n",
    "    for line in reader:\n",
    "        src_inputs_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8@@ r @-@ lis@@ ur@@ ide is a potent stere@@ ospecific histamine h1 @-@ receptor partial agon@@ ist@@ . the human histamine h1 receptor ( h@@ 1@@ r ) is an important , well characterized target for the development of antagonists to treat allergic condi@@ tion@@ s. many neuropsychiatric drugs are known to potently antagonize the h@@ 1@@ r , thereby producing some of their side effec@@ ts@@ . in contrast , the tolerability and potential therapeutic utility of h@@ 1@@ r agonism is currently un@@ cl@@ ear@@ . we have used a cell @-@ based functional assay to evaluate known therapeutics and reference drugs for h@@ 1@@ r agonist activ@@ it@@ y@@ . our initial functional screen identified three erg@@ ot @-@ based compounds possessing here@@ to@@ fore @-@ unknown h@@ 1@@ r agonist activity . 8@@ r @-@ lis@@ ur@@ ide demonstrated potent agonist activity in various assays including receptor selection and amplification technology , inositol phosphate accumulation , and activation of nuclear factor @-@ kappa@@ b with pec@@ 50 values of 8.1 , 7.9 , and 7.9 , respectively , and with varying degrees of efficac@@ y@@ . based on these assays , 8@@ r @-@ lis@@ ur@@ ide is the most potent stere@@ ospecific partial agonist for the human h@@ 1@@ r yet repor@@ te@@ d. investigation of the residues involved in histamine and lis@@ ur@@ ide binding , using h@@ 1@@ r mutants and molecular modeling , have revealed that although these ligands are structurally different , the lis@@ ur@@ ide @-@ binding pocket in the h@@ 1@@ r closely corresponds to the histamine @-@ binding p@@ ock@@ et@@ . the discovery of a potent stere@@ ospecific partial h@@ 1@@ r agonist provides a valuable tool to further characterize this important therapeutic target in vitro .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_inputs_ids[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 03:37:44 | INFO | fairseq.file_utils | loading archive file checkpoints/RE-DTI-BioGPT\n",
      "2023-04-26 03:37:44 | INFO | fairseq.file_utils | loading archive file data/KD-DTI/relis-bin\n",
      "2023-04-26 03:37:46 | INFO | src.language_modeling_prompt | dictionary: 42384 types\n",
      "2023-04-26 03:37:49 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n"
     ]
    }
   ],
   "source": [
    "m = TransformerLanguageModelPrompt.from_pretrained(\n",
    "    'checkpoints/RE-DTI-BioGPT', \n",
    "    'checkpoint_avg.pt', \n",
    "    'data/KD-DTI/relis-bin',\n",
    "    max_len_b=1024,\n",
    "    max_tokens=12000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '../../src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/RE-DTI-BioGPT', 'restore_file': '../../checkpoints/Pre-trained-BioGPT/checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 1024, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_prompt_biogpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 24, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': 1024, 'tpu': False, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False}, 'task': {'_name': 'language_modeling_prompt', 'data': 'data/KD-DTI/relis-bin', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 1024, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'source_lang': None, 'target_lang': None, 'max_source_positions': 640, 'manual_prompt': None, 'learned_prompt': 9, 'learned_prompt_pattern': 'learned', 'prefix': False, 'sep_token': '<seqsep>'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': 1e-07, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerLanguageModelPrompt(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(42393, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=42393, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m.cfg)\n",
    "m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1159\n",
      "\n",
      "2 / 1159\n",
      "\n",
      "3 / 1159\n",
      "\n",
      "4 / 1159\n",
      "\n",
      "5 / 1159\n",
      "\n",
      "6 / 1159\n",
      "\n",
      "7 / 1159\n",
      "\n",
      "8 / 1159\n",
      "\n",
      "9 / 1159\n",
      "\n",
      "10 / 1159\n",
      "\n",
      "11 / 1159\n",
      "\n",
      "12 / 1159\n",
      "\n",
      "13 / 1159\n",
      "\n",
      "14 / 1159\n",
      "\n",
      "15 / 1159\n",
      "\n",
      "16 / 1159\n",
      "\n",
      "17 / 1159\n",
      "\n",
      "18 / 1159\n",
      "\n",
      "19 / 1159\n",
      "\n",
      "20 / 1159\n",
      "\n",
      "21 / 1159\n",
      "\n",
      "22 / 1159\n",
      "\n",
      "23 / 1159\n",
      "\n",
      "24 / 1159\n",
      "\n",
      "25 / 1159\n",
      "\n",
      "26 / 1159\n",
      "\n",
      "27 / 1159\n",
      "\n",
      "28 / 1159\n",
      "\n",
      "29 / 1159\n",
      "\n",
      "30 / 1159\n",
      "\n",
      "31 / 1159\n",
      "\n",
      "32 / 1159\n",
      "\n",
      "33 / 1159\n",
      "\n",
      "34 / 1159\n",
      "\n",
      "35 / 1159\n",
      "\n",
      "36 / 1159\n",
      "\n",
      "37 / 1159\n",
      "\n",
      "38 / 1159\n",
      "\n",
      "39 / 1159\n",
      "\n",
      "40 / 1159\n",
      "\n",
      "41 / 1159\n",
      "\n",
      "42 / 1159\n",
      "\n",
      "43 / 1159\n",
      "\n",
      "44 / 1159\n",
      "\n",
      "45 / 1159\n",
      "\n",
      "46 / 1159\n",
      "\n",
      "47 / 1159\n",
      "\n",
      "48 / 1159\n",
      "\n",
      "49 / 1159\n",
      "\n",
      "50 / 1159\n",
      "\n",
      "51 / 1159\n",
      "\n",
      "52 / 1159\n",
      "\n",
      "53 / 1159\n",
      "\n",
      "54 / 1159\n",
      "\n",
      "55 / 1159\n",
      "\n",
      "56 / 1159\n",
      "\n",
      "57 / 1159\n",
      "\n",
      "58 / 1159\n",
      "\n",
      "59 / 1159\n",
      "\n",
      "60 / 1159\n",
      "\n",
      "61 / 1159\n",
      "\n",
      "62 / 1159\n",
      "\n",
      "63 / 1159\n",
      "\n",
      "64 / 1159\n",
      "\n",
      "65 / 1159\n",
      "\n",
      "66 / 1159\n",
      "\n",
      "67 / 1159\n",
      "\n",
      "68 / 1159\n",
      "\n",
      "69 / 1159\n",
      "\n",
      "70 / 1159\n",
      "\n",
      "71 / 1159\n",
      "\n",
      "72 / 1159\n",
      "\n",
      "73 / 1159\n",
      "\n",
      "74 / 1159\n",
      "\n",
      "75 / 1159\n",
      "\n",
      "76 / 1159\n",
      "\n",
      "77 / 1159\n",
      "\n",
      "78 / 1159\n",
      "\n",
      "79 / 1159\n",
      "\n",
      "80 / 1159\n",
      "\n",
      "81 / 1159\n",
      "\n",
      "82 / 1159\n",
      "\n",
      "83 / 1159\n",
      "\n",
      "84 / 1159\n",
      "\n",
      "85 / 1159\n",
      "\n",
      "86 / 1159\n",
      "\n",
      "87 / 1159\n",
      "\n",
      "88 / 1159\n",
      "\n",
      "89 / 1159\n",
      "\n",
      "90 / 1159\n",
      "\n",
      "91 / 1159\n",
      "\n",
      "92 / 1159\n",
      "\n",
      "93 / 1159\n",
      "\n",
      "94 / 1159\n",
      "\n",
      "95 / 1159\n",
      "\n",
      "96 / 1159\n",
      "\n",
      "97 / 1159\n",
      "\n",
      "98 / 1159\n",
      "\n",
      "99 / 1159\n",
      "\n",
      "100 / 1159\n",
      "\n",
      "101 / 1159\n",
      "\n",
      "102 / 1159\n",
      "\n",
      "103 / 1159\n",
      "\n",
      "104 / 1159\n",
      "\n",
      "105 / 1159\n",
      "\n",
      "106 / 1159\n",
      "\n",
      "107 / 1159\n",
      "\n",
      "108 / 1159\n",
      "\n",
      "109 / 1159\n",
      "\n",
      "110 / 1159\n",
      "\n",
      "111 / 1159\n",
      "\n",
      "112 / 1159\n",
      "\n",
      "113 / 1159\n",
      "\n",
      "114 / 1159\n",
      "\n",
      "115 / 1159\n",
      "\n",
      "116 / 1159\n",
      "\n",
      "117 / 1159\n",
      "\n",
      "118 / 1159\n",
      "\n",
      "119 / 1159\n",
      "\n",
      "120 / 1159\n",
      "\n",
      "121 / 1159\n",
      "\n",
      "122 / 1159\n",
      "\n",
      "123 / 1159\n",
      "\n",
      "124 / 1159\n",
      "\n",
      "125 / 1159\n",
      "\n",
      "126 / 1159\n",
      "\n",
      "127 / 1159\n",
      "\n",
      "128 / 1159\n",
      "\n",
      "129 / 1159\n",
      "\n",
      "130 / 1159\n",
      "\n",
      "131 / 1159\n",
      "\n",
      "132 / 1159\n",
      "\n",
      "133 / 1159\n",
      "\n",
      "134 / 1159\n",
      "\n",
      "135 / 1159\n",
      "\n",
      "136 / 1159\n",
      "\n",
      "137 / 1159\n",
      "\n",
      "138 / 1159\n",
      "\n",
      "139 / 1159\n",
      "\n",
      "140 / 1159\n",
      "\n",
      "141 / 1159\n",
      "\n",
      "142 / 1159\n",
      "\n",
      "143 / 1159\n",
      "\n",
      "144 / 1159\n",
      "\n",
      "145 / 1159\n",
      "\n",
      "146 / 1159\n",
      "\n",
      "147 / 1159\n",
      "\n",
      "148 / 1159\n",
      "\n",
      "149 / 1159\n",
      "\n",
      "150 / 1159\n",
      "\n",
      "151 / 1159\n",
      "\n",
      "152 / 1159\n",
      "\n",
      "153 / 1159\n",
      "\n",
      "154 / 1159\n",
      "\n",
      "155 / 1159\n",
      "\n",
      "156 / 1159\n",
      "\n",
      "157 / 1159\n",
      "\n",
      "158 / 1159\n",
      "\n",
      "159 / 1159\n",
      "\n",
      "160 / 1159\n",
      "\n",
      "161 / 1159\n",
      "\n",
      "162 / 1159\n",
      "\n",
      "163 / 1159\n",
      "\n",
      "164 / 1159\n",
      "\n",
      "165 / 1159\n",
      "\n",
      "166 / 1159\n",
      "\n",
      "167 / 1159\n",
      "\n",
      "168 / 1159\n",
      "\n",
      "169 / 1159\n",
      "\n",
      "170 / 1159\n",
      "\n",
      "171 / 1159\n",
      "\n",
      "172 / 1159\n",
      "\n",
      "173 / 1159\n",
      "\n",
      "174 / 1159\n",
      "\n",
      "175 / 1159\n",
      "\n",
      "176 / 1159\n",
      "\n",
      "177 / 1159\n",
      "\n",
      "178 / 1159\n",
      "\n",
      "179 / 1159\n",
      "\n",
      "180 / 1159\n",
      "\n",
      "181 / 1159\n",
      "\n",
      "182 / 1159\n",
      "\n",
      "183 / 1159\n",
      "\n",
      "184 / 1159\n",
      "\n",
      "185 / 1159\n",
      "\n",
      "186 / 1159\n",
      "\n",
      "187 / 1159\n",
      "\n",
      "188 / 1159\n",
      "\n",
      "189 / 1159\n",
      "\n",
      "190 / 1159\n",
      "\n",
      "191 / 1159\n",
      "\n",
      "192 / 1159\n",
      "\n",
      "193 / 1159\n",
      "\n",
      "194 / 1159\n",
      "\n",
      "195 / 1159\n",
      "\n",
      "196 / 1159\n",
      "\n",
      "197 / 1159\n",
      "\n",
      "198 / 1159\n",
      "\n",
      "199 / 1159\n",
      "\n",
      "200 / 1159\n",
      "\n",
      "201 / 1159\n",
      "\n",
      "202 / 1159\n",
      "\n",
      "203 / 1159\n",
      "\n",
      "204 / 1159\n",
      "\n",
      "205 / 1159\n",
      "\n",
      "206 / 1159\n",
      "\n",
      "207 / 1159\n",
      "\n",
      "208 / 1159\n",
      "\n",
      "209 / 1159\n",
      "\n",
      "210 / 1159\n",
      "\n",
      "211 / 1159\n",
      "\n",
      "212 / 1159\n",
      "\n",
      "213 / 1159\n",
      "\n",
      "214 / 1159\n",
      "\n",
      "215 / 1159\n",
      "\n",
      "216 / 1159\n",
      "\n",
      "217 / 1159\n",
      "\n",
      "218 / 1159\n",
      "\n",
      "219 / 1159\n",
      "\n",
      "220 / 1159\n",
      "\n",
      "221 / 1159\n",
      "\n",
      "222 / 1159\n",
      "\n",
      "223 / 1159\n",
      "\n",
      "224 / 1159\n",
      "\n",
      "225 / 1159\n",
      "\n",
      "226 / 1159\n",
      "\n",
      "227 / 1159\n",
      "\n",
      "228 / 1159\n",
      "\n",
      "229 / 1159\n",
      "\n",
      "230 / 1159\n",
      "\n",
      "231 / 1159\n",
      "\n",
      "232 / 1159\n",
      "\n",
      "233 / 1159\n",
      "\n",
      "234 / 1159\n",
      "\n",
      "235 / 1159\n",
      "\n",
      "236 / 1159\n",
      "\n",
      "237 / 1159\n",
      "\n",
      "238 / 1159\n",
      "\n",
      "239 / 1159\n",
      "\n",
      "240 / 1159\n",
      "\n",
      "241 / 1159\n",
      "\n",
      "242 / 1159\n",
      "\n",
      "243 / 1159\n",
      "\n",
      "244 / 1159\n",
      "\n",
      "245 / 1159\n",
      "\n",
      "246 / 1159\n",
      "\n",
      "247 / 1159\n",
      "\n",
      "248 / 1159\n",
      "\n",
      "249 / 1159\n",
      "\n",
      "250 / 1159\n",
      "\n",
      "251 / 1159\n",
      "\n",
      "252 / 1159\n",
      "\n",
      "253 / 1159\n",
      "\n",
      "254 / 1159\n",
      "\n",
      "255 / 1159\n",
      "\n",
      "256 / 1159\n",
      "\n",
      "257 / 1159\n",
      "\n",
      "258 / 1159\n",
      "\n",
      "259 / 1159\n",
      "\n",
      "260 / 1159\n",
      "\n",
      "261 / 1159\n",
      "\n",
      "262 / 1159\n",
      "\n",
      "263 / 1159\n",
      "\n",
      "264 / 1159\n",
      "\n",
      "265 / 1159\n",
      "\n",
      "266 / 1159\n",
      "\n",
      "267 / 1159\n",
      "\n",
      "268 / 1159\n",
      "\n",
      "269 / 1159\n",
      "\n",
      "270 / 1159\n",
      "\n",
      "271 / 1159\n",
      "\n",
      "272 / 1159\n",
      "\n",
      "273 / 1159\n",
      "\n",
      "274 / 1159\n",
      "\n",
      "275 / 1159\n",
      "\n",
      "276 / 1159\n",
      "\n",
      "277 / 1159\n",
      "\n",
      "278 / 1159\n",
      "\n",
      "279 / 1159\n",
      "\n",
      "280 / 1159\n",
      "\n",
      "281 / 1159\n",
      "\n",
      "282 / 1159\n",
      "\n",
      "283 / 1159\n",
      "\n",
      "284 / 1159\n",
      "\n",
      "285 / 1159\n",
      "\n",
      "286 / 1159\n",
      "\n",
      "287 / 1159\n",
      "\n",
      "288 / 1159\n",
      "\n",
      "289 / 1159\n",
      "\n",
      "290 / 1159\n",
      "\n",
      "291 / 1159\n",
      "\n",
      "292 / 1159\n",
      "\n",
      "293 / 1159\n",
      "\n",
      "294 / 1159\n",
      "\n",
      "295 / 1159\n",
      "\n",
      "296 / 1159\n",
      "\n",
      "297 / 1159\n",
      "\n",
      "298 / 1159\n",
      "\n",
      "299 / 1159\n",
      "\n",
      "300 / 1159\n",
      "\n",
      "301 / 1159\n",
      "\n",
      "302 / 1159\n",
      "\n",
      "303 / 1159\n",
      "\n",
      "304 / 1159\n",
      "\n",
      "305 / 1159\n",
      "\n",
      "306 / 1159\n",
      "\n",
      "307 / 1159\n",
      "\n",
      "308 / 1159\n",
      "\n",
      "309 / 1159\n",
      "\n",
      "310 / 1159\n",
      "\n",
      "311 / 1159\n",
      "\n",
      "312 / 1159\n",
      "\n",
      "313 / 1159\n",
      "\n",
      "314 / 1159\n",
      "\n",
      "315 / 1159\n",
      "\n",
      "316 / 1159\n",
      "\n",
      "317 / 1159\n",
      "\n",
      "318 / 1159\n",
      "\n",
      "319 / 1159\n",
      "\n",
      "320 / 1159\n",
      "\n",
      "321 / 1159\n",
      "\n",
      "322 / 1159\n",
      "\n",
      "323 / 1159\n",
      "\n",
      "324 / 1159\n",
      "\n",
      "325 / 1159\n",
      "\n",
      "326 / 1159\n",
      "\n",
      "327 / 1159\n",
      "\n",
      "328 / 1159\n",
      "\n",
      "329 / 1159\n",
      "\n",
      "330 / 1159\n",
      "\n",
      "331 / 1159\n",
      "\n",
      "332 / 1159\n",
      "\n",
      "333 / 1159\n",
      "\n",
      "334 / 1159\n",
      "\n",
      "335 / 1159\n",
      "\n",
      "336 / 1159\n",
      "\n",
      "337 / 1159\n",
      "\n",
      "338 / 1159\n",
      "\n",
      "339 / 1159\n",
      "\n",
      "340 / 1159\n",
      "\n",
      "341 / 1159\n",
      "\n",
      "342 / 1159\n",
      "\n",
      "343 / 1159\n",
      "\n",
      "344 / 1159\n",
      "\n",
      "345 / 1159\n",
      "\n",
      "346 / 1159\n",
      "\n",
      "347 / 1159\n",
      "\n",
      "348 / 1159\n",
      "\n",
      "349 / 1159\n",
      "\n",
      "350 / 1159\n",
      "\n",
      "351 / 1159\n",
      "\n",
      "352 / 1159\n",
      "\n",
      "353 / 1159\n",
      "\n",
      "354 / 1159\n",
      "\n",
      "355 / 1159\n",
      "\n",
      "356 / 1159\n",
      "\n",
      "357 / 1159\n",
      "\n",
      "358 / 1159\n",
      "\n",
      "359 / 1159\n",
      "\n",
      "360 / 1159\n",
      "\n",
      "361 / 1159\n",
      "\n",
      "362 / 1159\n",
      "\n",
      "363 / 1159\n",
      "\n",
      "364 / 1159\n",
      "\n",
      "365 / 1159\n",
      "\n",
      "366 / 1159\n",
      "\n",
      "367 / 1159\n",
      "\n",
      "368 / 1159\n",
      "\n",
      "369 / 1159\n",
      "\n",
      "370 / 1159\n",
      "\n",
      "371 / 1159\n",
      "\n",
      "372 / 1159\n",
      "\n",
      "373 / 1159\n",
      "\n",
      "374 / 1159\n",
      "\n",
      "375 / 1159\n",
      "\n",
      "376 / 1159\n",
      "\n",
      "377 / 1159\n",
      "\n",
      "378 / 1159\n",
      "\n",
      "379 / 1159\n",
      "\n",
      "380 / 1159\n",
      "\n",
      "381 / 1159\n",
      "\n",
      "382 / 1159\n",
      "\n",
      "383 / 1159\n",
      "\n",
      "384 / 1159\n",
      "\n",
      "385 / 1159\n",
      "\n",
      "386 / 1159\n",
      "\n",
      "387 / 1159\n",
      "\n",
      "388 / 1159\n",
      "\n",
      "389 / 1159\n",
      "\n",
      "390 / 1159\n",
      "\n",
      "391 / 1159\n",
      "\n",
      "392 / 1159\n",
      "\n",
      "393 / 1159\n",
      "\n",
      "394 / 1159\n",
      "\n",
      "395 / 1159\n",
      "\n",
      "396 / 1159\n",
      "\n",
      "397 / 1159\n",
      "\n",
      "398 / 1159\n",
      "\n",
      "399 / 1159\n",
      "\n",
      "400 / 1159\n",
      "\n",
      "401 / 1159\n",
      "\n",
      "402 / 1159\n",
      "\n",
      "403 / 1159\n",
      "\n",
      "404 / 1159\n",
      "\n",
      "405 / 1159\n",
      "\n",
      "406 / 1159\n",
      "\n",
      "407 / 1159\n",
      "\n",
      "408 / 1159\n",
      "\n",
      "409 / 1159\n",
      "\n",
      "410 / 1159\n",
      "\n",
      "411 / 1159\n",
      "\n",
      "412 / 1159\n",
      "\n",
      "413 / 1159\n",
      "\n",
      "414 / 1159\n",
      "\n",
      "415 / 1159\n",
      "\n",
      "416 / 1159\n",
      "\n",
      "417 / 1159\n",
      "\n",
      "418 / 1159\n",
      "\n",
      "419 / 1159\n",
      "\n",
      "420 / 1159\n",
      "\n",
      "421 / 1159\n",
      "\n",
      "422 / 1159\n",
      "\n",
      "423 / 1159\n",
      "\n",
      "424 / 1159\n",
      "\n",
      "425 / 1159\n",
      "\n",
      "426 / 1159\n",
      "\n",
      "427 / 1159\n",
      "\n",
      "428 / 1159\n",
      "\n",
      "429 / 1159\n",
      "\n",
      "430 / 1159\n",
      "\n",
      "431 / 1159\n",
      "\n",
      "432 / 1159\n",
      "\n",
      "433 / 1159\n",
      "\n",
      "434 / 1159\n",
      "\n",
      "435 / 1159\n",
      "\n",
      "436 / 1159\n",
      "\n",
      "437 / 1159\n",
      "\n",
      "438 / 1159\n",
      "\n",
      "439 / 1159\n",
      "\n",
      "440 / 1159\n",
      "\n",
      "441 / 1159\n",
      "\n",
      "442 / 1159\n",
      "\n",
      "443 / 1159\n",
      "\n",
      "444 / 1159\n",
      "\n",
      "445 / 1159\n",
      "\n",
      "446 / 1159\n",
      "\n",
      "447 / 1159\n",
      "\n",
      "448 / 1159\n",
      "\n",
      "449 / 1159\n",
      "\n",
      "450 / 1159\n",
      "\n",
      "451 / 1159\n",
      "\n",
      "452 / 1159\n",
      "\n",
      "453 / 1159\n",
      "\n",
      "454 / 1159\n",
      "\n",
      "455 / 1159\n",
      "\n",
      "456 / 1159\n",
      "\n",
      "457 / 1159\n",
      "\n",
      "458 / 1159\n",
      "\n",
      "459 / 1159\n",
      "\n",
      "460 / 1159\n",
      "\n",
      "461 / 1159\n",
      "\n",
      "462 / 1159\n",
      "\n",
      "463 / 1159\n",
      "\n",
      "464 / 1159\n",
      "\n",
      "465 / 1159\n",
      "\n",
      "466 / 1159\n",
      "\n",
      "467 / 1159\n",
      "\n",
      "468 / 1159\n",
      "\n",
      "469 / 1159\n",
      "\n",
      "470 / 1159\n",
      "\n",
      "471 / 1159\n",
      "\n",
      "472 / 1159\n",
      "\n",
      "473 / 1159\n",
      "\n",
      "474 / 1159\n",
      "\n",
      "475 / 1159\n",
      "\n",
      "476 / 1159\n",
      "\n",
      "477 / 1159\n",
      "\n",
      "478 / 1159\n",
      "\n",
      "479 / 1159\n",
      "\n",
      "480 / 1159\n",
      "\n",
      "481 / 1159\n",
      "\n",
      "482 / 1159\n",
      "\n",
      "483 / 1159\n",
      "\n",
      "484 / 1159\n",
      "\n",
      "485 / 1159\n",
      "\n",
      "486 / 1159\n",
      "\n",
      "487 / 1159\n",
      "\n",
      "488 / 1159\n",
      "\n",
      "489 / 1159\n",
      "\n",
      "490 / 1159\n",
      "\n",
      "491 / 1159\n",
      "\n",
      "492 / 1159\n",
      "\n",
      "493 / 1159\n",
      "\n",
      "494 / 1159\n",
      "\n",
      "495 / 1159\n",
      "\n",
      "496 / 1159\n",
      "\n",
      "497 / 1159\n",
      "\n",
      "498 / 1159\n",
      "\n",
      "499 / 1159\n",
      "\n",
      "500 / 1159\n",
      "\n",
      "501 / 1159\n",
      "\n",
      "502 / 1159\n",
      "\n",
      "503 / 1159\n",
      "\n",
      "504 / 1159\n",
      "\n",
      "505 / 1159\n",
      "\n",
      "506 / 1159\n",
      "\n",
      "507 / 1159\n",
      "\n",
      "508 / 1159\n",
      "\n",
      "509 / 1159\n",
      "\n",
      "510 / 1159\n",
      "\n",
      "511 / 1159\n",
      "\n",
      "512 / 1159\n",
      "\n",
      "513 / 1159\n",
      "\n",
      "514 / 1159\n",
      "\n",
      "515 / 1159\n",
      "\n",
      "516 / 1159\n",
      "\n",
      "517 / 1159\n",
      "\n",
      "518 / 1159\n",
      "\n",
      "519 / 1159\n",
      "\n",
      "520 / 1159\n",
      "\n",
      "521 / 1159\n",
      "\n",
      "522 / 1159\n",
      "\n",
      "523 / 1159\n",
      "\n",
      "524 / 1159\n",
      "\n",
      "525 / 1159\n",
      "\n",
      "526 / 1159\n",
      "\n",
      "527 / 1159\n",
      "\n",
      "528 / 1159\n",
      "\n",
      "529 / 1159\n",
      "\n",
      "530 / 1159\n",
      "\n",
      "531 / 1159\n",
      "\n",
      "532 / 1159\n",
      "\n",
      "533 / 1159\n",
      "\n",
      "534 / 1159\n",
      "\n",
      "535 / 1159\n",
      "\n",
      "536 / 1159\n",
      "\n",
      "537 / 1159\n",
      "\n",
      "538 / 1159\n",
      "\n",
      "539 / 1159\n",
      "\n",
      "540 / 1159\n",
      "\n",
      "541 / 1159\n",
      "\n",
      "542 / 1159\n",
      "\n",
      "543 / 1159\n",
      "\n",
      "544 / 1159\n",
      "\n",
      "545 / 1159\n",
      "\n",
      "546 / 1159\n",
      "\n",
      "547 / 1159\n",
      "\n",
      "548 / 1159\n",
      "\n",
      "549 / 1159\n",
      "\n",
      "550 / 1159\n",
      "\n",
      "551 / 1159\n",
      "\n",
      "552 / 1159\n",
      "\n",
      "553 / 1159\n",
      "\n",
      "554 / 1159\n",
      "\n",
      "555 / 1159\n",
      "\n",
      "556 / 1159\n",
      "\n",
      "557 / 1159\n",
      "\n",
      "558 / 1159\n",
      "\n",
      "559 / 1159\n",
      "\n",
      "560 / 1159\n",
      "\n",
      "561 / 1159\n",
      "\n",
      "562 / 1159\n",
      "\n",
      "563 / 1159\n",
      "\n",
      "564 / 1159\n",
      "\n",
      "565 / 1159\n",
      "\n",
      "566 / 1159\n",
      "\n",
      "567 / 1159\n",
      "\n",
      "568 / 1159\n",
      "\n",
      "569 / 1159\n",
      "\n",
      "570 / 1159\n",
      "\n",
      "571 / 1159\n",
      "\n",
      "572 / 1159\n",
      "\n",
      "573 / 1159\n",
      "\n",
      "574 / 1159\n",
      "\n",
      "575 / 1159\n",
      "\n",
      "576 / 1159\n",
      "\n",
      "577 / 1159\n",
      "\n",
      "578 / 1159\n",
      "\n",
      "579 / 1159\n",
      "\n",
      "580 / 1159\n",
      "\n",
      "581 / 1159\n",
      "\n",
      "582 / 1159\n",
      "\n",
      "583 / 1159\n",
      "\n",
      "584 / 1159\n",
      "\n",
      "585 / 1159\n",
      "\n",
      "586 / 1159\n",
      "\n",
      "587 / 1159\n",
      "\n",
      "588 / 1159\n",
      "\n",
      "589 / 1159\n",
      "\n",
      "590 / 1159\n",
      "\n",
      "591 / 1159\n",
      "\n",
      "592 / 1159\n",
      "\n",
      "593 / 1159\n",
      "\n",
      "594 / 1159\n",
      "\n",
      "595 / 1159\n",
      "\n",
      "596 / 1159\n",
      "\n",
      "597 / 1159\n",
      "\n",
      "598 / 1159\n",
      "\n",
      "599 / 1159\n",
      "\n",
      "600 / 1159\n",
      "\n",
      "601 / 1159\n",
      "\n",
      "602 / 1159\n",
      "\n",
      "603 / 1159\n",
      "\n",
      "604 / 1159\n",
      "\n",
      "605 / 1159\n",
      "\n",
      "606 / 1159\n",
      "\n",
      "607 / 1159\n",
      "\n",
      "608 / 1159\n",
      "\n",
      "609 / 1159\n",
      "\n",
      "610 / 1159\n",
      "\n",
      "611 / 1159\n",
      "\n",
      "612 / 1159\n",
      "\n",
      "613 / 1159\n",
      "\n",
      "614 / 1159\n",
      "\n",
      "615 / 1159\n",
      "\n",
      "616 / 1159\n",
      "\n",
      "617 / 1159\n",
      "\n",
      "618 / 1159\n",
      "\n",
      "619 / 1159\n",
      "\n",
      "620 / 1159\n",
      "\n",
      "621 / 1159\n",
      "\n",
      "622 / 1159\n",
      "\n",
      "623 / 1159\n",
      "\n",
      "624 / 1159\n",
      "\n",
      "625 / 1159\n",
      "\n",
      "626 / 1159\n",
      "\n",
      "627 / 1159\n",
      "\n",
      "628 / 1159\n",
      "\n",
      "629 / 1159\n",
      "\n",
      "630 / 1159\n",
      "\n",
      "631 / 1159\n",
      "\n",
      "632 / 1159\n",
      "\n",
      "633 / 1159\n",
      "\n",
      "634 / 1159\n",
      "\n",
      "635 / 1159\n",
      "\n",
      "636 / 1159\n",
      "\n",
      "637 / 1159\n",
      "\n",
      "638 / 1159\n",
      "\n",
      "639 / 1159\n",
      "\n",
      "640 / 1159\n",
      "\n",
      "641 / 1159\n",
      "\n",
      "642 / 1159\n",
      "\n",
      "643 / 1159\n",
      "\n",
      "644 / 1159\n",
      "\n",
      "645 / 1159\n",
      "\n",
      "646 / 1159\n",
      "\n",
      "647 / 1159\n",
      "\n",
      "648 / 1159\n",
      "\n",
      "649 / 1159\n",
      "\n",
      "650 / 1159\n",
      "\n",
      "651 / 1159\n",
      "\n",
      "652 / 1159\n",
      "\n",
      "653 / 1159\n",
      "\n",
      "654 / 1159\n",
      "\n",
      "655 / 1159\n",
      "\n",
      "656 / 1159\n",
      "\n",
      "657 / 1159\n",
      "\n",
      "658 / 1159\n",
      "\n",
      "659 / 1159\n",
      "\n",
      "660 / 1159\n",
      "\n",
      "661 / 1159\n",
      "\n",
      "662 / 1159\n",
      "\n",
      "663 / 1159\n",
      "\n",
      "664 / 1159\n",
      "\n",
      "665 / 1159\n",
      "\n",
      "666 / 1159\n",
      "\n",
      "667 / 1159\n",
      "\n",
      "668 / 1159\n",
      "\n",
      "669 / 1159\n",
      "\n",
      "670 / 1159\n",
      "\n",
      "671 / 1159\n",
      "\n",
      "672 / 1159\n",
      "\n",
      "673 / 1159\n",
      "\n",
      "674 / 1159\n",
      "\n",
      "675 / 1159\n",
      "\n",
      "676 / 1159\n",
      "\n",
      "677 / 1159\n",
      "\n",
      "678 / 1159\n",
      "\n",
      "679 / 1159\n",
      "\n",
      "680 / 1159\n",
      "\n",
      "681 / 1159\n",
      "\n",
      "682 / 1159\n",
      "\n",
      "683 / 1159\n",
      "\n",
      "684 / 1159\n",
      "\n",
      "685 / 1159\n",
      "\n",
      "686 / 1159\n",
      "\n",
      "687 / 1159\n",
      "\n",
      "688 / 1159\n",
      "\n",
      "689 / 1159\n",
      "\n",
      "690 / 1159\n",
      "\n",
      "691 / 1159\n",
      "\n",
      "692 / 1159\n",
      "\n",
      "693 / 1159\n",
      "\n",
      "694 / 1159\n",
      "\n",
      "695 / 1159\n",
      "\n",
      "696 / 1159\n",
      "\n",
      "697 / 1159\n",
      "\n",
      "698 / 1159\n",
      "\n",
      "699 / 1159\n",
      "\n",
      "700 / 1159\n",
      "\n",
      "701 / 1159\n",
      "\n",
      "702 / 1159\n",
      "\n",
      "703 / 1159\n",
      "\n",
      "704 / 1159\n",
      "\n",
      "705 / 1159\n",
      "\n",
      "706 / 1159\n",
      "\n",
      "707 / 1159\n",
      "\n",
      "708 / 1159\n",
      "\n",
      "709 / 1159\n",
      "\n",
      "710 / 1159\n",
      "\n",
      "711 / 1159\n",
      "\n",
      "712 / 1159\n",
      "\n",
      "713 / 1159\n",
      "\n",
      "714 / 1159\n",
      "\n",
      "715 / 1159\n",
      "\n",
      "716 / 1159\n",
      "\n",
      "717 / 1159\n",
      "\n",
      "718 / 1159\n",
      "\n",
      "719 / 1159\n",
      "\n",
      "720 / 1159\n",
      "\n",
      "721 / 1159\n",
      "\n",
      "722 / 1159\n",
      "\n",
      "723 / 1159\n",
      "\n",
      "724 / 1159\n",
      "\n",
      "725 / 1159\n",
      "\n",
      "726 / 1159\n",
      "\n",
      "727 / 1159\n",
      "\n",
      "728 / 1159\n",
      "\n",
      "729 / 1159\n",
      "\n",
      "730 / 1159\n",
      "\n",
      "731 / 1159\n",
      "\n",
      "732 / 1159\n",
      "\n",
      "733 / 1159\n",
      "\n",
      "734 / 1159\n",
      "\n",
      "735 / 1159\n",
      "\n",
      "736 / 1159\n",
      "\n",
      "737 / 1159\n",
      "\n",
      "738 / 1159\n",
      "\n",
      "739 / 1159\n",
      "\n",
      "740 / 1159\n",
      "\n",
      "741 / 1159\n",
      "\n",
      "742 / 1159\n",
      "\n",
      "743 / 1159\n",
      "\n",
      "744 / 1159\n",
      "\n",
      "745 / 1159\n",
      "\n",
      "746 / 1159\n",
      "\n",
      "747 / 1159\n",
      "\n",
      "748 / 1159\n",
      "\n",
      "749 / 1159\n",
      "\n",
      "750 / 1159\n",
      "\n",
      "751 / 1159\n",
      "\n",
      "752 / 1159\n",
      "\n",
      "753 / 1159\n",
      "\n",
      "754 / 1159\n",
      "\n",
      "755 / 1159\n",
      "\n",
      "756 / 1159\n",
      "\n",
      "757 / 1159\n",
      "\n",
      "758 / 1159\n",
      "\n",
      "759 / 1159\n",
      "\n",
      "760 / 1159\n",
      "\n",
      "761 / 1159\n",
      "\n",
      "762 / 1159\n",
      "\n",
      "763 / 1159\n",
      "\n",
      "764 / 1159\n",
      "\n",
      "765 / 1159\n",
      "\n",
      "766 / 1159\n",
      "\n",
      "767 / 1159\n",
      "\n",
      "768 / 1159\n",
      "\n",
      "769 / 1159\n",
      "\n",
      "770 / 1159\n",
      "\n",
      "771 / 1159\n",
      "\n",
      "772 / 1159\n",
      "\n",
      "773 / 1159\n",
      "\n",
      "774 / 1159\n",
      "\n",
      "775 / 1159\n",
      "\n",
      "776 / 1159\n",
      "\n",
      "777 / 1159\n",
      "\n",
      "778 / 1159\n",
      "\n",
      "779 / 1159\n",
      "\n",
      "780 / 1159\n",
      "\n",
      "781 / 1159\n",
      "\n",
      "782 / 1159\n",
      "\n",
      "783 / 1159\n",
      "\n",
      "784 / 1159\n",
      "\n",
      "785 / 1159\n",
      "\n",
      "786 / 1159\n",
      "\n",
      "787 / 1159\n",
      "\n",
      "788 / 1159\n",
      "\n",
      "789 / 1159\n",
      "\n",
      "790 / 1159\n",
      "\n",
      "791 / 1159\n",
      "\n",
      "792 / 1159\n",
      "\n",
      "793 / 1159\n",
      "\n",
      "794 / 1159\n",
      "\n",
      "795 / 1159\n",
      "\n",
      "796 / 1159\n",
      "\n",
      "797 / 1159\n",
      "\n",
      "798 / 1159\n",
      "\n",
      "799 / 1159\n",
      "\n",
      "800 / 1159\n",
      "\n",
      "801 / 1159\n",
      "\n",
      "802 / 1159\n",
      "\n",
      "803 / 1159\n",
      "\n",
      "804 / 1159\n",
      "\n",
      "805 / 1159\n",
      "\n",
      "806 / 1159\n",
      "\n",
      "807 / 1159\n",
      "\n",
      "808 / 1159\n",
      "\n",
      "809 / 1159\n",
      "\n",
      "810 / 1159\n",
      "\n",
      "811 / 1159\n",
      "\n",
      "812 / 1159\n",
      "\n",
      "813 / 1159\n",
      "\n",
      "814 / 1159\n",
      "\n",
      "815 / 1159\n",
      "\n",
      "816 / 1159\n",
      "\n",
      "817 / 1159\n",
      "\n",
      "818 / 1159\n",
      "\n",
      "819 / 1159\n",
      "\n",
      "820 / 1159\n",
      "\n",
      "821 / 1159\n",
      "\n",
      "822 / 1159\n",
      "\n",
      "823 / 1159\n",
      "\n",
      "824 / 1159\n",
      "\n",
      "825 / 1159\n",
      "\n",
      "826 / 1159\n",
      "\n",
      "827 / 1159\n",
      "\n",
      "828 / 1159\n",
      "\n",
      "829 / 1159\n",
      "\n",
      "830 / 1159\n",
      "\n",
      "831 / 1159\n",
      "\n",
      "832 / 1159\n",
      "\n",
      "833 / 1159\n",
      "\n",
      "834 / 1159\n",
      "\n",
      "835 / 1159\n",
      "\n",
      "836 / 1159\n",
      "\n",
      "837 / 1159\n",
      "\n",
      "838 / 1159\n",
      "\n",
      "839 / 1159\n",
      "\n",
      "840 / 1159\n",
      "\n",
      "841 / 1159\n",
      "\n",
      "842 / 1159\n",
      "\n",
      "843 / 1159\n",
      "\n",
      "844 / 1159\n",
      "\n",
      "845 / 1159\n",
      "\n",
      "846 / 1159\n",
      "\n",
      "847 / 1159\n",
      "\n",
      "848 / 1159\n",
      "\n",
      "849 / 1159\n",
      "\n",
      "850 / 1159\n",
      "\n",
      "851 / 1159\n",
      "\n",
      "852 / 1159\n",
      "\n",
      "853 / 1159\n",
      "\n",
      "854 / 1159\n",
      "\n",
      "855 / 1159\n",
      "\n",
      "856 / 1159\n",
      "\n",
      "857 / 1159\n",
      "\n",
      "858 / 1159\n",
      "\n",
      "859 / 1159\n",
      "\n",
      "860 / 1159\n",
      "\n",
      "861 / 1159\n",
      "\n",
      "862 / 1159\n",
      "\n",
      "863 / 1159\n",
      "\n",
      "864 / 1159\n",
      "\n",
      "865 / 1159\n",
      "\n",
      "866 / 1159\n",
      "\n",
      "867 / 1159\n",
      "\n",
      "868 / 1159\n",
      "\n",
      "869 / 1159\n",
      "\n",
      "870 / 1159\n",
      "\n",
      "871 / 1159\n",
      "\n",
      "872 / 1159\n",
      "\n",
      "873 / 1159\n",
      "\n",
      "874 / 1159\n",
      "\n",
      "875 / 1159\n",
      "\n",
      "876 / 1159\n",
      "\n",
      "877 / 1159\n",
      "\n",
      "878 / 1159\n",
      "\n",
      "879 / 1159\n",
      "\n",
      "880 / 1159\n",
      "\n",
      "881 / 1159\n",
      "\n",
      "882 / 1159\n",
      "\n",
      "883 / 1159\n",
      "\n",
      "884 / 1159\n",
      "\n",
      "885 / 1159\n",
      "\n",
      "886 / 1159\n",
      "\n",
      "887 / 1159\n",
      "\n",
      "888 / 1159\n",
      "\n",
      "889 / 1159\n",
      "\n",
      "890 / 1159\n",
      "\n",
      "891 / 1159\n",
      "\n",
      "892 / 1159\n",
      "\n",
      "893 / 1159\n",
      "\n",
      "894 / 1159\n",
      "\n",
      "895 / 1159\n",
      "\n",
      "896 / 1159\n",
      "\n",
      "897 / 1159\n",
      "\n",
      "898 / 1159\n",
      "\n",
      "899 / 1159\n",
      "\n",
      "900 / 1159\n",
      "\n",
      "901 / 1159\n",
      "\n",
      "902 / 1159\n",
      "\n",
      "903 / 1159\n",
      "\n",
      "904 / 1159\n",
      "\n",
      "905 / 1159\n",
      "\n",
      "906 / 1159\n",
      "\n",
      "907 / 1159\n",
      "\n",
      "908 / 1159\n",
      "\n",
      "909 / 1159\n",
      "\n",
      "910 / 1159\n",
      "\n",
      "911 / 1159\n",
      "\n",
      "912 / 1159\n",
      "\n",
      "913 / 1159\n",
      "\n",
      "914 / 1159\n",
      "\n",
      "915 / 1159\n",
      "\n",
      "916 / 1159\n",
      "\n",
      "917 / 1159\n",
      "\n",
      "918 / 1159\n",
      "\n",
      "919 / 1159\n",
      "\n",
      "920 / 1159\n",
      "\n",
      "921 / 1159\n",
      "\n",
      "922 / 1159\n",
      "\n",
      "923 / 1159\n",
      "\n",
      "924 / 1159\n",
      "\n",
      "925 / 1159\n",
      "\n",
      "926 / 1159\n",
      "\n",
      "927 / 1159\n",
      "\n",
      "928 / 1159\n",
      "\n",
      "929 / 1159\n",
      "\n",
      "930 / 1159\n",
      "\n",
      "931 / 1159\n",
      "\n",
      "932 / 1159\n",
      "\n",
      "933 / 1159\n",
      "\n",
      "934 / 1159\n",
      "\n",
      "935 / 1159\n",
      "\n",
      "936 / 1159\n",
      "\n",
      "937 / 1159\n",
      "\n",
      "938 / 1159\n",
      "\n",
      "939 / 1159\n",
      "\n",
      "940 / 1159\n",
      "\n",
      "941 / 1159\n",
      "\n",
      "942 / 1159\n",
      "\n",
      "943 / 1159\n",
      "\n",
      "944 / 1159\n",
      "\n",
      "945 / 1159\n",
      "\n",
      "946 / 1159\n",
      "\n",
      "947 / 1159\n",
      "\n",
      "948 / 1159\n",
      "\n",
      "949 / 1159\n",
      "\n",
      "950 / 1159\n",
      "\n",
      "951 / 1159\n",
      "\n",
      "952 / 1159\n",
      "\n",
      "953 / 1159\n",
      "\n",
      "954 / 1159\n",
      "\n",
      "955 / 1159\n",
      "\n",
      "956 / 1159\n",
      "\n",
      "957 / 1159\n",
      "\n",
      "958 / 1159\n",
      "\n",
      "959 / 1159\n",
      "\n",
      "960 / 1159\n",
      "\n",
      "961 / 1159\n",
      "\n",
      "962 / 1159\n",
      "\n",
      "963 / 1159\n",
      "\n",
      "964 / 1159\n",
      "\n",
      "965 / 1159\n",
      "\n",
      "966 / 1159\n",
      "\n",
      "967 / 1159\n",
      "\n",
      "968 / 1159\n",
      "\n",
      "969 / 1159\n",
      "\n",
      "970 / 1159\n",
      "\n",
      "971 / 1159\n",
      "\n",
      "972 / 1159\n",
      "\n",
      "973 / 1159\n",
      "\n",
      "974 / 1159\n",
      "\n",
      "975 / 1159\n",
      "\n",
      "976 / 1159\n",
      "\n",
      "977 / 1159\n",
      "\n",
      "978 / 1159\n",
      "\n",
      "979 / 1159\n",
      "\n",
      "980 / 1159\n",
      "\n",
      "981 / 1159\n",
      "\n",
      "982 / 1159\n",
      "\n",
      "983 / 1159\n",
      "\n",
      "984 / 1159\n",
      "\n",
      "985 / 1159\n",
      "\n",
      "986 / 1159\n",
      "\n",
      "987 / 1159\n",
      "\n",
      "988 / 1159\n",
      "\n",
      "989 / 1159\n",
      "\n",
      "990 / 1159\n",
      "\n",
      "991 / 1159\n",
      "\n",
      "992 / 1159\n",
      "\n",
      "993 / 1159\n",
      "\n",
      "994 / 1159\n",
      "\n",
      "995 / 1159\n",
      "\n",
      "996 / 1159\n",
      "\n",
      "997 / 1159\n",
      "\n",
      "998 / 1159\n",
      "\n",
      "999 / 1159\n",
      "\n",
      "1000 / 1159\n",
      "\n",
      "1001 / 1159\n",
      "\n",
      "1002 / 1159\n",
      "\n",
      "1003 / 1159\n",
      "\n",
      "1004 / 1159\n",
      "\n",
      "1005 / 1159\n",
      "\n",
      "1006 / 1159\n",
      "\n",
      "1007 / 1159\n",
      "\n",
      "1008 / 1159\n",
      "\n",
      "1009 / 1159\n",
      "\n",
      "1010 / 1159\n",
      "\n",
      "1011 / 1159\n",
      "\n",
      "1012 / 1159\n",
      "\n",
      "1013 / 1159\n",
      "\n",
      "1014 / 1159\n",
      "\n",
      "1015 / 1159\n",
      "\n",
      "1016 / 1159\n",
      "\n",
      "1017 / 1159\n",
      "\n",
      "1018 / 1159\n",
      "\n",
      "1019 / 1159\n",
      "\n",
      "1020 / 1159\n",
      "\n",
      "1021 / 1159\n",
      "\n",
      "1022 / 1159\n",
      "\n",
      "1023 / 1159\n",
      "\n",
      "1024 / 1159\n",
      "\n",
      "1025 / 1159\n",
      "\n",
      "1026 / 1159\n",
      "\n",
      "1027 / 1159\n",
      "\n",
      "1028 / 1159\n",
      "\n",
      "1029 / 1159\n",
      "\n",
      "1030 / 1159\n",
      "\n",
      "1031 / 1159\n",
      "\n",
      "1032 / 1159\n",
      "\n",
      "1033 / 1159\n",
      "\n",
      "1034 / 1159\n",
      "\n",
      "1035 / 1159\n",
      "\n",
      "1036 / 1159\n",
      "\n",
      "1037 / 1159\n",
      "\n",
      "1038 / 1159\n",
      "\n",
      "1039 / 1159\n",
      "\n",
      "1040 / 1159\n",
      "\n",
      "1041 / 1159\n",
      "\n",
      "1042 / 1159\n",
      "\n",
      "1043 / 1159\n",
      "\n",
      "1044 / 1159\n",
      "\n",
      "1045 / 1159\n",
      "\n",
      "1046 / 1159\n",
      "\n",
      "1047 / 1159\n",
      "\n",
      "1048 / 1159\n",
      "\n",
      "1049 / 1159\n",
      "\n",
      "1050 / 1159\n",
      "\n",
      "1051 / 1159\n",
      "\n",
      "1052 / 1159\n",
      "\n",
      "1053 / 1159\n",
      "\n",
      "1054 / 1159\n",
      "\n",
      "1055 / 1159\n",
      "\n",
      "1056 / 1159\n",
      "\n",
      "1057 / 1159\n",
      "\n",
      "1058 / 1159\n",
      "\n",
      "1059 / 1159\n",
      "\n",
      "1060 / 1159\n",
      "\n",
      "1061 / 1159\n",
      "\n",
      "1062 / 1159\n",
      "\n",
      "1063 / 1159\n",
      "\n",
      "1064 / 1159\n",
      "\n",
      "1065 / 1159\n",
      "\n",
      "1066 / 1159\n",
      "\n",
      "1067 / 1159\n",
      "\n",
      "1068 / 1159\n",
      "\n",
      "1069 / 1159\n",
      "\n",
      "1070 / 1159\n",
      "\n",
      "1071 / 1159\n",
      "\n",
      "1072 / 1159\n",
      "\n",
      "1073 / 1159\n",
      "\n",
      "1074 / 1159\n",
      "\n",
      "1075 / 1159\n",
      "\n",
      "1076 / 1159\n",
      "\n",
      "1077 / 1159\n",
      "\n",
      "1078 / 1159\n",
      "\n",
      "1079 / 1159\n",
      "\n",
      "1080 / 1159\n",
      "\n",
      "1081 / 1159\n",
      "\n",
      "1082 / 1159\n",
      "\n",
      "1083 / 1159\n",
      "\n",
      "1084 / 1159\n",
      "\n",
      "1085 / 1159\n",
      "\n",
      "1086 / 1159\n",
      "\n",
      "1087 / 1159\n",
      "\n",
      "1088 / 1159\n",
      "\n",
      "1089 / 1159\n",
      "\n",
      "1090 / 1159\n",
      "\n",
      "1091 / 1159\n",
      "\n",
      "1092 / 1159\n",
      "\n",
      "1093 / 1159\n",
      "\n",
      "1094 / 1159\n",
      "\n",
      "1095 / 1159\n",
      "\n",
      "1096 / 1159\n",
      "\n",
      "1097 / 1159\n",
      "\n",
      "1098 / 1159\n",
      "\n",
      "1099 / 1159\n",
      "\n",
      "1100 / 1159\n",
      "\n",
      "1101 / 1159\n",
      "\n",
      "1102 / 1159\n",
      "\n",
      "1103 / 1159\n",
      "\n",
      "1104 / 1159\n",
      "\n",
      "1105 / 1159\n",
      "\n",
      "1106 / 1159\n",
      "\n",
      "1107 / 1159\n",
      "\n",
      "1108 / 1159\n",
      "\n",
      "1109 / 1159\n",
      "\n",
      "1110 / 1159\n",
      "\n",
      "1111 / 1159\n",
      "\n",
      "1112 / 1159\n",
      "\n",
      "1113 / 1159\n",
      "\n",
      "1114 / 1159\n",
      "\n",
      "1115 / 1159\n",
      "\n",
      "1116 / 1159\n",
      "\n",
      "1117 / 1159\n",
      "\n",
      "1118 / 1159\n",
      "\n",
      "1119 / 1159\n",
      "\n",
      "1120 / 1159\n",
      "\n",
      "1121 / 1159\n",
      "\n",
      "1122 / 1159\n",
      "\n",
      "1123 / 1159\n",
      "\n",
      "1124 / 1159\n",
      "\n",
      "1125 / 1159\n",
      "\n",
      "1126 / 1159\n",
      "\n",
      "1127 / 1159\n",
      "\n",
      "1128 / 1159\n",
      "\n",
      "1129 / 1159\n",
      "\n",
      "1130 / 1159\n",
      "\n",
      "1131 / 1159\n",
      "\n",
      "1132 / 1159\n",
      "\n",
      "1133 / 1159\n",
      "\n",
      "1134 / 1159\n",
      "\n",
      "1135 / 1159\n",
      "\n",
      "1136 / 1159\n",
      "\n",
      "1137 / 1159\n",
      "\n",
      "1138 / 1159\n",
      "\n",
      "1139 / 1159\n",
      "\n",
      "1140 / 1159\n",
      "\n",
      "1141 / 1159\n",
      "\n",
      "1142 / 1159\n",
      "\n",
      "1143 / 1159\n",
      "\n",
      "1144 / 1159\n",
      "\n",
      "1145 / 1159\n",
      "\n",
      "1146 / 1159\n",
      "\n",
      "1147 / 1159\n",
      "\n",
      "1148 / 1159\n",
      "\n",
      "1149 / 1159\n",
      "\n",
      "1150 / 1159\n",
      "\n",
      "1151 / 1159\n",
      "\n",
      "1152 / 1159\n",
      "\n",
      "1153 / 1159\n",
      "\n",
      "1154 / 1159\n",
      "\n",
      "1155 / 1159\n",
      "\n",
      "1156 / 1159\n",
      "\n",
      "1157 / 1159\n",
      "\n",
      "1158 / 1159\n",
      "\n",
      "1159 / 1159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_output = predict(m, src_inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [m.decode(hypos) for hypos in batch_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{'examples/RE-DTI/generate_checkpoint_optimized_2-only_tokens_in_original.pt'}\", \"w\", encoding='utf8') as fw:\n",
    "    for i in range(len(outputs)):\n",
    "        fw.write(outputs[i] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sed -i \"s/@@ //g\" generate_checkpoint_optimized_2-only_tokens_in_original.pt\n",
    "\n",
    "perl ../../mosesdecoder/scripts/tokenizer/detokenizer.perl -l en -a < generate_checkpoint_optimized_2-only_tokens_in_original.pt > generate_checkpoint_optimized_2-only_tokens_in_original.pt.detok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'examples/RE-DTI/generate_checkpoint_optimized_2-only_tokens_in_original.pt.detok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [\n",
    "    '(learned[0-9]+ )+',\n",
    "    'we can conclude that',\n",
    "    'we have that',\n",
    "    'in conclusion,',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(line: str) -> str:\n",
    "    \"\"\"Removes the prefix from the line.\"\"\"\n",
    "    # search for the prefix in the line\n",
    "    for p in prefix:\n",
    "        res = re.search(p, line)\n",
    "        \n",
    "        if res is not None:\n",
    "            # remove the prefix from the line\n",
    "            line = re.split(p, line)[-1].strip()\n",
    "            break\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(line):\n",
    "    \"\"\"Splits the line into sentences by semicolon.\"\"\"\n",
    "    sentences = re.split(r\"; \", line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relis_sentence(sentence):\n",
    "    # match sentences of the form \"the interaction between X and Y is Z\"\n",
    "    # and return the tuple (X, Z, Y)\n",
    "    ans = None\n",
    "    segs = re.match(r\"the interaction between (.*) and (.*) is (.*)\", sentence)\n",
    "    if segs is not None:\n",
    "        segs = segs.groups()\n",
    "        ans = (segs[0].strip(), segs[2].strip(), segs[1].strip())\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(sample, head_idx=0, rel_idx=1, tail_idx=2):\n",
    "    ret = {\n",
    "        \"triple_list_gold\": [], \n",
    "        \"triple_list_pred\": [], \n",
    "        \"new\": [], \n",
    "        \"lack\": [], \n",
    "        \"id\": [0]\n",
    "    }\n",
    "    for item in sample:\n",
    "        ret[\"triple_list_pred\"].append(\n",
    "            {\"subject\": item[head_idx], \n",
    "            \"relation\": item[rel_idx], \n",
    "            \"object\": item[tail_idx]}\n",
    "        )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 1159\n"
     ]
    }
   ],
   "source": [
    "all_lines = []\n",
    "with open(out_file, \"r\", encoding=\"utf8\") as fr:\n",
    "    for line in fr:\n",
    "        e = line.strip()\n",
    "        if len(e) > 0 and e[-1] == \".\": # if the last character of the line is a period, remove it\n",
    "            all_lines.append(e[:-1])\n",
    "        else:\n",
    "            all_lines.append(e)\n",
    "\n",
    "print(f'Number of lines: {len(all_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the interaction between isopsoralen and monoamine oxidase (mao) is inhibitor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = []\n",
    "cnt = 0\n",
    "fail_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed:id:97, line:the interaction between ici 200,355 and neutrophil elastase (is inactivator; ne) is inhibitor and the role of neutrophil elastase in airway hypersecretion is inhibitor\n",
      "Failed:id:221, line:the interaction between (s) -2-carboalkoxy-substituted 3alpha- [bis (4-fluorophenyl) methoxy] tropane and dopamine transporter (dat; is downregulator\n",
      "Failed:id:250, line:the interaction between lisuride and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-ht, tryptamine and 5-h@@\n",
      "Failed:id:251, line:the interaction between rs 57639 and 5-ht & gt; 5-ht & gt; 5-ht4 receptor is agonist\n",
      "Failed:id:310, line:the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor (is agonist; the interaction between rec 15 / 3079 and 5-ht (1a) receptor\n",
      "Failed:id:349, line:the interaction between haloxysterols a and cholinesterase (bche; ec 3.1.1.7) is inhibitor\n",
      "Failed:id:352, line:the interaction between luf5831 and adenosine a1 receptor (is deoxidizer\n",
      "Failed:id:358, line:the interaction between (and) is inhibitor\n",
      "Failed:id:384, line:the interaction between ro 28-2653 and matrix metalloproteinase inhibitor (is inhibitor; mmpi) is a novel inhibitor, and the therapeutic efficacy of is inhibitor\n",
      "Failed:id:590, line:the interaction between ethoxzolamide and carbonic anhydrase (ca; ec 4.2.1.1) is inhibitor; the interaction between ethoxzolamide and carbonic anhydrase (ca; ec 4.2.1.1) is inhibitor\n",
      "Failed:id:825, line:the interaction between atosiban and oxytocin receptor (is ligand; otr) is and the binding is antagonist\n",
      "Failed:id:1076, line:the interaction between icatibant and aminopeptidase n (is unknown\n"
     ]
    }
   ],
   "source": [
    "# i is the index of the line in the input file\n",
    "for i, line in enumerate(all_lines):\n",
    "    # cnt is the number of lines in the input file\n",
    "    cnt += 1\n",
    "    ret = []\n",
    "    # convert the \"& amp\" if it is in line to \"&\"\n",
    "    line = line.replace(\" & amp; \", \"&\")\n",
    "    strip_line = strip_prefix(line)\n",
    "    sentences = split_sentence(strip_line)\n",
    "    for sen in sentences:\n",
    "        ans = convert_relis_sentence(sen)\n",
    "        if ans is not None:\n",
    "            ret.append(ans)\n",
    "    if len(ret) > 0:\n",
    "        hypothesis.append(ret)\n",
    "    else:\n",
    "        hypothesis.append([(\"failed\", \"failed\", \"failed\")])\n",
    "        fail_cnt += 1\n",
    "        print(\"Failed:id:{}, line:{}\".format(i+1, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_formatted = []\n",
    "for i in range(len(hypothesis)):\n",
    "    ret_formatted.append(converter(hypothesis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed = 12, total = 1159\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{out_file}.extracted.json\", \"w\", encoding=\"utf8\") as fw:\n",
    "    for eg in ret_formatted:\n",
    "        print(json.dumps(eg), file=fw)\n",
    "\n",
    "\n",
    "print(f\"failed = {fail_cnt}, total = {cnt}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'examples/RE-DTI/generate_checkpoint_optimized_2-only_tokens_in_original.pt.detok.extracted.json'\n",
    "gold_file = 'data/KD-DTI/raw/test.json'\n",
    "pmids_file = 'data/KD-DTI/raw/relis_test.pmid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(s: str):\n",
    "    s = s.strip()\n",
    "\n",
    "    # normalize roman type id at end of string\n",
    "    num2roman = {\"0\": \"0\", \"1\": \"I\", \"2\": \"II\", \"3\": \"III\", \"4\": \"IV\", \"5\": \"V\", \"6\": \"VI\", \"7\": \"VII\", \"8\": \"VIII\", \"9\": \"IX\"}\n",
    "    if len(s) > 2 and s[-1].isnumeric() and not s[-2].isnumeric() and s[-1] in num2roman:\n",
    "        tmps = list(s)\n",
    "        s = ''.join(tmps[:-1]) + num2roman[tmps[-1]]\n",
    "\n",
    "    # remove useless end string\n",
    "    s = s.replace(\"-type\", '')\n",
    "    \n",
    "    # remove non-alphanumeric characters\n",
    "    return re.sub('[^a-zA-Z0-9]+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace amine-associated receptor 1 (taar1)\n",
      "traceamineassociatedreceptor1taar1\n",
      "\n",
      "trace amine-associated receptor 1\n",
      "traceamineassociatedreceptorI\n",
      "\n",
      "trace amine-associated receptor 21\n",
      "traceamineassociatedreceptor21\n"
     ]
    }
   ],
   "source": [
    "# e.p. for normalize_name\n",
    "\n",
    "print(f\"trace amine-associated receptor 1 (taar1)\\n{normalize_name('trace amine-associated receptor 1 (taar1)')}\\n\\ntrace amine-associated receptor 1\\n{normalize_name('trace amine-associated receptor 1')}\\n\\ntrace amine-associated receptor 21\\n{normalize_name('trace amine-associated receptor 21')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_abbr(tgt_set):\n",
    "    \"\"\" remove abbreviation in the brackets of entity, eg: aaa (bb) -> aaa \"\"\"\n",
    "    def rm(s):\n",
    "        s = s.strip()\n",
    "        if \"(\" in s and s[-1] == ')':  # entity end with a bracketed short cut\n",
    "            return normalize_name(s[:s.rfind(\"(\")].strip())\n",
    "        else:\n",
    "            return normalize_name(s)\n",
    "\n",
    "    tgt_set = list(tgt_set)\n",
    "    if tgt_set and type(tgt_set[0]) in [tuple, list]:  # process triples\n",
    "        return set([(rm(tp[0]), rm(tp[1]), rm(tp[2])) for tp in tgt_set])\n",
    "    else:  # process entities\n",
    "        return set([rm(e) for e in tgt_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abbr(tgt_set):\n",
    "    \"\"\" extract abbreviation in the brackets of entity, eg: aaa (bb) -> bb \"\"\"\n",
    "    def rm(s):\n",
    "        s = s.strip()\n",
    "        if \"(\" in s and s[-1] == ')':\n",
    "            return normalize_name(s[s.rfind(\"(\")+1:-1].strip())\n",
    "        else:\n",
    "            return normalize_name(s)\n",
    "\n",
    "    tgt_set = list(tgt_set)\n",
    "    if tgt_set and type(tgt_set[0]) in [tuple, list]:  # process triples\n",
    "        return set([(rm(tp[0]), rm(tp[1]), rm(tp[2])) for tp in tgt_set])\n",
    "    else:  # process entities\n",
    "        return set([rm(e) for e in tgt_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style acc \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(gold_set) == 0 else 1.0 * tp_num / len(pred_set | gold_set)\n",
    "\n",
    "\n",
    "def precision(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style precision \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(pred_set) == 0 else 1.0 * tp_num / len(pred_set)\n",
    "\n",
    "\n",
    "def recall(pred_set, gold_set):\n",
    "    \"\"\" Multi-label style recall \"\"\"\n",
    "    tp_num = len(pred_set & gold_set)\n",
    "    return int(pred_set == gold_set) if len(gold_set) == 0 else 1.0 * tp_num / len(gold_set)\n",
    "\n",
    "\n",
    "def normed_eval(pred_set, gold_set, metric):\n",
    "    \"\"\" Both body and abbreviation match are considered correct \"\"\"\n",
    "    abbr_pred_set, abbr_gold_set = get_abbr(pred_set), get_abbr(gold_set)\n",
    "    rm_pred_set, rm_gold_set = rm_abbr(pred_set), rm_abbr(gold_set)\n",
    "    return max(metric(abbr_pred_set, abbr_gold_set), metric(rm_pred_set, rm_gold_set))\n",
    "\n",
    "\n",
    "def get_f1(p, r):\n",
    "    return 0 if (p + r) == 0 else (2.0 * p * r / (p + r))\n",
    "\n",
    "\n",
    "def ave(scores):\n",
    "    return 1.0 * sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(preds, pmids, golden):\n",
    "    ret = []\n",
    "    num_pred, num_gold, num_missing = 0, 0, 0\n",
    "    all_f1, p, r, d_acc, t_acc, i_acc = [], [], [], [], [], []\n",
    "    all_pred_triple, all_pred_d, all_pred_t, all_pred_i, all_gold_triple, all_gold_d, all_gold_t, all_gold_i = [], [], [], [], [], [], [], [],\n",
    "\n",
    "    for pred, idx in zip(preds, pmids):\n",
    "        # initialize a set for each entity type (drug, target, interaction) for the prediction and gold standard\n",
    "        gold_d_set, gold_t_set, gold_i_set, gold_set = set(), set(), set(), set()\n",
    "        pred_d_set, pred_t_set, pred_i_set, pred_set = set(), set(), set(), set()\n",
    "        \n",
    "        # if the prediction is not empty and if the model did not fail to extract any triple\n",
    "        if pred[\"triple_list_pred\"] and pred[\"triple_list_pred\"][0][\"subject\"] != 'failed':\n",
    "            # loop over all triples and add the entities to the respective sets\n",
    "            for tp in pred[\"triple_list_pred\"]:\n",
    "                d = tp[\"subject\"].strip().lower().replace(' ', '')\n",
    "                t = tp[\"object\"].strip().lower().replace(' ', '')\n",
    "                i = tp[\"relation\"].strip().lower().replace(' ', '')\n",
    "\n",
    "                pred_d_set.add(d)\n",
    "                pred_t_set.add(t)\n",
    "                pred_i_set.add(i)\n",
    "                pred_set.add((d, t, i))\n",
    "        # if the paper is not in the golden set\n",
    "        if idx not in golden:\n",
    "            # increase the number of missing papers\n",
    "            num_missing += 1\n",
    "            # print a message\n",
    "            print(\"----Missing:\", idx)\n",
    "            # skip this paper\n",
    "            continue\n",
    "        # if there are triples in the golden set\n",
    "        if golden[idx][\"triples\"]:\n",
    "            # loop over all triples and add the entities to the respective sets\n",
    "            for tp in golden[idx][\"triples\"]:\n",
    "                d = tp[\"drug\"].strip().lower().replace(' ', '')\n",
    "                t = tp[\"target\"].strip().lower().replace(' ', '')\n",
    "                i = tp[\"interaction\"].strip().lower().replace(' ', '')\n",
    "                gold_d_set.add(d)\n",
    "                gold_t_set.add(t)\n",
    "                gold_i_set.add(i)\n",
    "                gold_set.add((d, t, i))\n",
    "\n",
    "        # sample level eval\n",
    "        p.append(normed_eval(pred_set, gold_set, metric=precision))\n",
    "        r.append(normed_eval(pred_set, gold_set, metric=recall))\n",
    "        all_f1.append(get_f1(p[-1], r[-1]))\n",
    "        d_acc.append(normed_eval(pred_d_set, gold_d_set, metric=acc))\n",
    "        t_acc.append(normed_eval(pred_t_set, gold_t_set, metric=acc))\n",
    "        i_acc.append(normed_eval(pred_i_set, gold_i_set, metric=acc))\n",
    "\n",
    "        # onto level eval\n",
    "        all_pred_d.extend(pred_d_set)\n",
    "        all_pred_t.extend(pred_t_set)\n",
    "        all_pred_i.extend(pred_i_set)\n",
    "        all_pred_triple.extend(pred_set)\n",
    "        all_gold_d.extend(gold_d_set)\n",
    "        all_gold_t.extend(gold_t_set)\n",
    "        all_gold_i.extend(gold_i_set)\n",
    "        all_gold_triple.extend(gold_set)\n",
    "        \n",
    "        # if len(gold_set) < len(golden[idx][\"triples\"]):\n",
    "            # print(\"Duplicate extists, ori\", golden[idx][\"triples\"], gold_set)\n",
    "\n",
    "        num_pred += len(pred_set)\n",
    "        num_gold += len(gold_set)\n",
    "\n",
    "        ret.append({\n",
    "            \"pmid\": idx,\n",
    "            \"title\": golden[idx][\"title\"] if \"title\" in golden[idx] else None,\n",
    "            \"abstract\": golden[idx][\"abstract\"],\n",
    "            \"d_pred_gold\": [d_acc[-1], list(pred_d_set), list(gold_d_set)],\n",
    "            \"t_pred_gold\": [t_acc[-1], list(pred_t_set), list(gold_t_set)],\n",
    "            \"i_pred_gold\": [i_acc[-1], list(pred_i_set), list(gold_i_set)],\n",
    "            \"all_pred_gold\": [all_f1[-1], list(pred_set), list(gold_set)],\n",
    "        })\n",
    "\n",
    "\n",
    "    print(\"num sample\", len(all_f1), \"missing\", len(preds) - len(all_f1), \"num_gold tp\", num_gold, \"num_pred\", num_pred)\n",
    "\n",
    "    # Note: we adopt multi-label metrics following: http://129.211.169.156/publication/tkde13rev.pdf\n",
    "    print(\"Sample: acc d: {:.4f}\\tt:{:.4f}\\ti: {:.4f}\\ntp p: {:.4f}\\ttp r: {:.4f}\\ttp micro f1: {:.4f}\\ttp macro f1: {:.4f} \".format(\n",
    "        ave(d_acc), ave(t_acc), ave(i_acc), ave(p), ave(r), ave(all_f1), get_f1(ave(p), ave(r))))\n",
    "\n",
    "    # Ontology evaluation_scripts\n",
    "    all_p, all_r = normed_eval(set(all_pred_triple), set(all_gold_triple), metric=precision), normed_eval(set(all_pred_triple), set(all_gold_triple), metric=recall)\n",
    "    d_p, d_r = normed_eval(set(all_pred_d), set(all_gold_d), metric=precision), normed_eval(set(all_pred_d), set(all_gold_d), metric=recall)\n",
    "    t_p, t_r = normed_eval(set(all_pred_t), set(all_gold_t), metric=precision), normed_eval(set(all_pred_t), set(all_gold_t), metric=recall)\n",
    "    i_p, i_r = normed_eval(set(all_pred_i), set(all_gold_i), metric=precision), normed_eval(set(all_pred_i), set(all_gold_i), metric=recall)\n",
    "\n",
    "    print(\"Ontology: f1 d: {:.4f}\\tt:{:.4f}\\ti: {:.4f}\\t \\nall p: {:.4f}\\tall r: {:.4f}\\tonto f1: {:.4f}\".format(\n",
    "        get_f1(d_p, d_r), get_f1(t_p, t_r), get_f1(i_p, i_r), all_p, all_r, get_f1(all_p, all_r)\n",
    "    ))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with open(pred_file) as reader:\n",
    "    for line in reader:\n",
    "        preds.append(json.loads(line))\n",
    "\n",
    "with open(gold_file) as reader:\n",
    "    golden = json.load(reader)\n",
    "\n",
    "with open(pmids_file) as reader:\n",
    "    if '.json' in pmids_file:\n",
    "        pmids = json.load(reader)\n",
    "    else:\n",
    "        pmids = []\n",
    "        for line in reader:\n",
    "            pmids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====File:  generate_checkpoint_optimized_2-only_tokens_in_original.pt.detok.extracted.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====File: \", os.path.basename(pred_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triple_list_gold': [],\n",
       " 'triple_list_pred': [{'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase (mao)'}],\n",
       " 'new': [],\n",
       " 'lack': [],\n",
       " 'id': [0]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sample 1159 missing 0 num_gold tp 1567 num_pred 1262\n",
      "Sample: acc d: 0.6028\tt:0.4931\ti: 0.8513\n",
      "tp p: 0.3136\ttp r: 0.2987\ttp micro f1: 0.3013\ttp macro f1: 0.3060 \n",
      "Ontology: f1 d: 0.6090\tt:0.4003\ti: 0.7727\t \n",
      "all p: 0.2829\tall r: 0.2380\tonto f1: 0.2585\n"
     ]
    }
   ],
   "source": [
    "# new\n",
    "result = do_eval(preds, pmids, golden)\n",
    "\n",
    "last_pos = pred_file.rfind('.json')\n",
    "res_file_name = pred_file[:last_pos] + '.eval_res.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sample 1159 missing 0 num_gold tp 1567 num_pred 2250\n",
      "Sample: acc d: 0.6175\tt:0.6067\ti: 0.8667\n",
      "tp p: 0.4009\ttp r: 0.3980\ttp micro f1: 0.3851\ttp macro f1: 0.3994 \n",
      "Ontology: f1 d: 0.6093\tt:0.5122\ti: 0.7556\t \n",
      "all p: 0.2518\tall r: 0.3735\tonto f1: 0.3008\n"
     ]
    }
   ],
   "source": [
    "result = do_eval(preds, pmids, golden)\n",
    "\n",
    "last_pos = pred_file.rfind('.json')\n",
    "res_file_name = pred_file[:last_pos] + '.eval_res.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(res_file_name, 'w') as writer:\n",
    "    json.dump(result, writer, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the comparation of the outputs in:\n",
    "\n",
    "examples/RE-DTI/generate_checkpoint_avg.pt.detok.extracted.eval_res.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioGPT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'examples/RE-DTI/generate_checkpoint_avg.pt.detok.extracted.json'\n",
    "# gold_file = 'data/KD-DTI/raw/test.json'\n",
    "gold_file = 'data/KD-DTI/raw/train.json'\n",
    "# pmids_file = 'data/KD-DTI/raw/relis_test.pmid'\n",
    "pmids_file = 'data/KD-DTI/raw/relis_train.pmid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "preds = []\n",
    "with open(pred_file) as reader:\n",
    "    for line in reader:\n",
    "        preds.append(json.loads(line))\n",
    "\n",
    "with open(gold_file) as reader:\n",
    "    golden = json.load(reader)\n",
    "\n",
    "with open(pmids_file) as reader:\n",
    "    if '.json' in pmids_file:\n",
    "        pmids = json.load(reader)\n",
    "    else:\n",
    "        pmids = []\n",
    "        for line in reader:\n",
    "            pmids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Beta-subunits co-determine the sensitivity of rat neuronal nicotinic receptors to antagonists.',\n",
       " 'abstract': \"We have investigated the effect of 4 ganglionic cholinergic antagonists (hexamethonium, mecamylamine, pentolinium, trimetaphan) on rat alpha 3 beta 2 and alpha 3 beta 4 neuronal nicotinic acetylcholine receptors (nAChRs) expressed in Xenopus oocytes. Current responses were elicited by fast application of acetylcholine on voltage-clamped oocytes (holding potentialVh = -80mV). Concentration-inhibition curves were used to get estimates of IC50, the antagonist concentration yielding 50% reduction of the peak current. The KB's of the antagonists were calculated using estimates of the apparent KD of acetylcholine. The order of affinity of the antagonists was similar for both receptor subtypes: mecamylamine approximately pentolinium &gt; hexamethonium &gt; trimetaphan. However, alpha 3 beta 4 neuronal nAChRs were 9 to 22 times more sensitive to each of the 4 antagonists than alpha 3 beta 2 receptors. These results further underline the importance of the beta-subunit as co-determinant of the functional properties of neuronal nAChRs.\",\n",
       " 'triples': [{'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit beta-4',\n",
       "   'interaction': 'antagonist'},\n",
       "  {'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit alpha-10',\n",
       "   'interaction': 'antagonist'},\n",
       "  {'drug': 'Pentolinium',\n",
       "   'target': 'Neuronal acetylcholine receptor subunit alpha-3',\n",
       "   'interaction': 'antagonist'}],\n",
       " 'pmid': '7761270'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden[pmids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triple_list_gold': [],\n",
       " 'triple_list_pred': [{'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type b (mao-b)'},\n",
       "  {'subject': 'isopsoralen',\n",
       "   'relation': 'inhibitor',\n",
       "   'object': 'monoamine oxidase type a (mao-a)'}],\n",
       " 'new': [],\n",
       " 'lack': [],\n",
       " 'id': [0]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred_D = 0\n",
    "num_pred_T = 0\n",
    "num_gold_D = 0\n",
    "num_gold_T = 0\n",
    "\n",
    "\n",
    "num_predD_in_original = 0\n",
    "num_predT_in_original = 0\n",
    "# # if there are more than 50% prefix tokens of a span is in the original text, count it\n",
    "# num_predD_over50_in_original = 0\n",
    "# num_predT_over50_in_original = 0\n",
    "# partial in the golden means the model didn't generate the whole span.\n",
    "num_goldD_in_original = 0\n",
    "\n",
    "predT_not_in_original_pmid_predTs = []\n",
    "\n",
    "for id, pred in zip(pmids, preds):\n",
    "    # only use lower() to nomalize the text\n",
    "    pred_Ds = list(set([triple['subject'].lower() for triple in pred['triple_list_pred']]))\n",
    "    pred_Ts = list(set([triple['object'].lower() for triple in pred['triple_list_pred']]))\n",
    "    gold_Ds = list(set([triple['drug'].lower() for triple in golden[id]['triples']]))\n",
    "    gold_Ts = list(set([triple['target'].lower() for triple in golden[id]['triples']]))\n",
    "    text = (golden[id]['title'] + \" \" + golden[id]['abstract']).lower()\n",
    "    \n",
    "    num_pred_D += len(pred_Ds)\n",
    "    num_pred_T += len(pred_Ts)\n",
    "    num_gold_D += len(gold_Ds)\n",
    "    num_gold_T += len(gold_Ts)\n",
    "\n",
    "    predT_not_in_original = []\n",
    "\n",
    "    # count for if pred_D in the original text\n",
    "    for pred_D in pred_Ds:\n",
    "        if pred_D is not None:\n",
    "            if pred_D in text:\n",
    "                num_predD_in_original += 1\n",
    "\n",
    "    # count for if pred_T in the original text\n",
    "    for pred_T in pred_Ts:\n",
    "        if pred_T is not None:\n",
    "            if pred_T in text:\n",
    "                num_predT_in_original += 1\n",
    "            else:\n",
    "                predT_not_in_original.append(pred_T)\n",
    "\n",
    "\n",
    "    # count for if god_D in the original text\n",
    "    for gold_D in gold_Ds:\n",
    "        if gold_D in text:\n",
    "            num_goldD_in_original += 1\n",
    "\n",
    "    # count for if god_T in the original text\n",
    "    for gold_T in gold_Ts:\n",
    "        if gold_T in text:\n",
    "            num_goldT_in_original += 1\n",
    "\n",
    "\n",
    "    if predT_not_in_original:\n",
    "        predT_not_in_original_pmid_predTs.append({\n",
    "            \"id\": id,\n",
    "            \"predT_not_in_original\": predT_not_in_original\n",
    "            })\n",
    "    # pred_D_ids = [tokenizer.encode(pred_d, add_special_tokens=False) for pred_d in pred_Ds]\n",
    "    # pred_T_ids = [tokenizer.encode(pred_t, add_special_tokens=False) for pred_t in pred_Ts]\n",
    "    # gold_D_ids = [tokenizer.encode(gold_d, add_special_tokens=False) for gold_d in gold_Ds]\n",
    "    # gold_T_ids = [tokenizer.encode(gold_t, add_special_tokens=False) for gold_t in gold_Ts]\n",
    "    # text_id = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # # count for if pred_D in the original text\n",
    "    # for pred_d_id in pred_D_ids:\n",
    "    #     length = len(pred_d_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == pred_d_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predD_in_original += 1\n",
    "    #             else:\n",
    "    #                 if j+1 > length/2:\n",
    "    #                     num_predD_over50_in_original += 1\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if pred_T in the original text\n",
    "    # for pred_t_id in pred_T_ids:\n",
    "    #     length = len(pred_t_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == pred_t_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predT_in_original += 1\n",
    "    #             else:\n",
    "    #                 if j+1 > length/2:\n",
    "    #                     print(tokenizer.decode(pred_t_id[:j+1]))\n",
    "    #                     num_predT_over50_in_original += 1\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if gold_T in the original text\n",
    "    # for gold_t_id in gold_T_ids:\n",
    "    #     length = len(gold_t_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == gold_t_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predT_in_golden += 1\n",
    "    #             else:\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break\n",
    "    \n",
    "    # # count for if gold_D in the original text\n",
    "    # for gold_d_id in gold_D_ids:\n",
    "    #     length = len(gold_d_id)\n",
    "    #     for i in range(len(text_id)):\n",
    "    #         for j in range(length):\n",
    "    #             if text_id[i+j+1] == gold_d_id[:j+1]:\n",
    "    #                 if j+1 == length:\n",
    "    #                     num_predD_in_golden += 1\n",
    "    #             else:\n",
    "    #                 break\n",
    "    #         if i + length +1 >= len(text_id):\n",
    "    #             break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(golden) == len(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gold_D = 0\n",
    "num_gold_T = 0\n",
    "\n",
    "\n",
    "num_predD_in_original = 0\n",
    "num_predT_in_original = 0\n",
    "# # if there are more than 50% prefix tokens of a span is in the original text, count it\n",
    "# num_predD_over50_in_original = 0\n",
    "# num_predT_over50_in_original = 0\n",
    "# partial in the golden means the model didn't generate the whole span.\n",
    "num_goldD_in_original = 0\n",
    "num_goldT_in_original = 0\n",
    "\n",
    "for id in pmids:\n",
    "    # only use lower() to nomalize the text\n",
    "    gold_Ds = list(set([triple['drug'].lower() for triple in golden[id]['triples']]))\n",
    "    gold_Ts = list(set([triple['target'].lower() for triple in golden[id]['triples']]))\n",
    "    text = (golden[id]['title'] + \" \" + golden[id]['abstract']).lower()\n",
    "    \n",
    "    num_gold_D += len(gold_Ds)\n",
    "    num_gold_T += len(gold_Ts)\n",
    "\n",
    "    predT_not_in_original = []\n",
    "\n",
    "    # count for if god_D in the original text\n",
    "    for gold_D in gold_Ds:\n",
    "        if gold_D in text:\n",
    "            num_goldD_in_original += 1\n",
    "\n",
    "    # count for if god_T in the original text\n",
    "    for gold_T in gold_Ts:\n",
    "        if gold_T in text:\n",
    "            num_goldT_in_original += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_goldD_in_original:  8369\n",
      "num_goldT_in_original:  1535\n",
      "num_gold_D:  19426\n",
      "num_gold_T:  17179\n"
     ]
    }
   ],
   "source": [
    "# print num_predD_in_original, num_predT_in_original, num_predD_over50_in_original, num_predT_over50_in_original, num_predD_in_golden = 0, num_predT_in_golden with their lables\n",
    "# print(\"num_predD_in_original: \", num_predD_in_original)\n",
    "# print(\"num_predT_in_original: \", num_predT_in_original)\n",
    "print(\"num_goldD_in_original: \", num_goldD_in_original)\n",
    "print(\"num_goldT_in_original: \", num_goldT_in_original)\n",
    "# print(\"num__pred_D: \", num_pred_D)\n",
    "# print(\"num_pred_T: \", num_pred_T)\n",
    "print(\"num_gold_D: \", num_gold_D)\n",
    "print(\"num_gold_T: \", num_gold_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gold_D:\n",
      "8369 / 19426 : 0.4308\n",
      "For gold_T:\n",
      "1535 / 17179 : 0.0894\n"
     ]
    }
   ],
   "source": [
    "# print(f\"For pred_D:\\n{num_predD_in_original} / {num_pred_D} : {round(num_predD_in_original/num_pred_D, 4)}\")\n",
    "# print(f\"For pred_T:\\n{num_predT_in_original} / {num_pred_T} : {round(num_predT_in_original/num_pred_T, 4)}\")\n",
    "print(f\"For gold_D:\\n{num_goldD_in_original} / {num_gold_D} : {round(num_goldD_in_original/num_gold_D, 4)}\")\n",
    "print(f\"For gold_T:\\n{num_goldT_in_original} / {num_gold_T} : {round(num_goldT_in_original/num_gold_T, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('analysis/predT_not_in_original_pmid_predTs.json', 'w') as writer:\n",
    "    json.dump(predT_not_in_original_pmid_predTs, writer, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
